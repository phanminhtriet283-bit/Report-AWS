[
{
	"uri": "http://localhost:1313/Report-AWS/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "AWS Training and Certification Blog\nFrom Statistics Superfan to NFL Analytics Pioneer: Mike Band‚Äôs Journey written by Priya Ponnuswamy on September 4, 2025 in Amazon Machine Learning, Analytics, Artificial Intelligence, AWS Training and Certification, Best Practices | Permalink | Share\nHave you ever wondered how a childhood passion for sports statistics can transform into a groundbreaking analytics career in the NFL (National Football League)? Meet Mike Band, senior manager of research and analytics at NFL Next Gen Stats. His story shows that pursuing your passion and curiosity can lead to extraordinary careers.\nWhether you\u0026rsquo;re a data engineer or an aspiring machine learning (ML) professional, or already an experienced data analyst, this article offers inspiration and guidance to help you begin or advance your career path.\nFrom Data to Touchdowns: It All Adds Up | NFL Next Gen Stats \u0026amp; AWS\nThe Road to the Big Leagues\nAs a child, Band was passionate about football and statistics. He strengthened that passion by earning a Bachelor of Science in sports management at the University of Florida, and then a Master of Science in analytics at the University of Chicago.\nBeyond his academic degrees, Band‚Äôs journey took him from college athletics to the professional arena. He worked as an admissions assistant for the University of Florida football program and as a scouting assistant at the Minnesota Vikings before joining the NFL‚Äôs Next Gen Stats team‚Äîtruly a dream job for fans of football statistics! But this work is more than just analyzing numbers ‚Äî it‚Äôs about revolutionizing how we understand the sport.\nMaking Waves at Next Gen Stats\nIn his role as senior manager of research and analytics, Band has helped develop advanced AI models that are transforming the way football is analyzed. As the NFL‚Äôs proprietary player- and ball-tracking platform, Next Gen Stats is used by league officials, broadcasters, media partners, and all 32 teams ‚Äî including coaches, scouts, and players.\nThrough his work with Amazon Web Services (AWS), Band has pushed the boundaries of what‚Äôs possible in football analytics, helping turn raw data into actionable insights. Remember the tackle probability model you often see during games? Band and his team played a key role in creating it, using AWS ML capabilities to turn tracking data into advanced statistics that contextualize on-field action.\nAdvice for the Next Generation\nBand‚Äôs advice for those pursuing sports analytics? Don‚Äôt wait for someone to give you permission to follow your passion. He emphasizes the importance of being proactive and letting curiosity guide you. ‚ÄúThe best opportunities come to those who create them,‚Äù Band says.\nHere are some guiding principles Band has followed in his career:\nStay hungry for knowledge, even outside of work. Keep up with emerging technologies, especially AI. Don‚Äôt stop at generating insights‚Äîfocus on effectively communicating them. Look for the next wave in your field‚Äîand ride it. Turn passion into expertise through continuous learning. Join the growing ranks of technical professionals building meaningful and rewarding careers. AWS Skill Builder, AWS‚Äôs online learning platform, gives you a starting point to enter this high-performance world:\nStart your journey with a free Skill Builder account. Choose role-based learning paths such as Data Engineer or Machine Learning Engineer. Use digital courses to build a strong knowledge foundation. Develop practical cloud skills through AWS Cloud Quest (game-based learning) and AWS SimuLearn, which offer AI-augmented solution-building exercises. Validate your skills, knowledge, and expertise with AWS Certifications. Success comes not only from having the right degrees or strong technical skills ‚Äî but also from keeping childlike curiosity and an endless drive to learn. Whether you‚Äôre dreaming of a career in sports analytics or any other field, follow Band‚Äôs ‚Äúplaybook‚Äù: stay curious, be proactive, and never stop learning. After all, today‚Äôs passion project may become tomorrow‚Äôs career breakthrough.\nStart your learning journey today!\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.4-api-gateway-integration/5.4.1-create-http-api/",
	"title": "Create HTTP API",
	"tags": [],
	"description": "",
	"content": "Create HTTP API In this step, you will create an HTTP API that will serve as the endpoint for clients to send questions to your Lambda function.\nüîπ Step 1 ‚Äî Go to API Gateway and click Create API Open the Amazon API Gateway Console and select Create API.\nüîπ Step 2 ‚Äî Select Build under HTTP API Choose:\nBuild ‚Üí HTTP API\nüîπ Step 3 ‚Äî Rename the API Set:\nAPI name: bedrock-chatbot-api Click Next, then Create to finish creating the API.\nüéØ Result You have successfully created an HTTP API, and it is now ready to be integrated with your Lambda function in the next step.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.1-create-lambda/",
	"title": "Create Lambda Function",
	"tags": [],
	"description": "",
	"content": "Create Lambda Function In this step, you will create a new AWS Lambda function that will receive requests from the client and send prompts to Amazon Bedrock using the Converse API.\nüîπ Step 1 ‚Äî Open the Lambda creation page Go to the AWS Lambda Console Select Functions Click Create function üîπ Step 2 ‚Äî Set the name and configuration for the Lambda function Configure the function as follows:\nFunction name: bedrock-chatbot-lambda Runtime: Python 3.12 (recommended and stable for this workshop) Architecture: x86_64 or arm64 (both are supported) Permissions ‚Üí Use an existing role Select the role you created in the Prerequisites section, for example:\nlambda-bedrock-role Then click Create function.\nüéØ Expected result You now have an empty Lambda function that:\nIs ready for adding the Converse API invocation code Has an execution role with Bedrock permissions + CloudWatch logging Can be tested directly in the Lambda Console In the next section, you will configure the IAM role further and prepare the source code to call the model.\nContinue to 5.3.2 ‚Äì Add code to call the Converse API.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report ‚ÄúViet Nam Cloud Day 2025‚Äù Event Objectives Inspire and unlock AI + Cloud adoption for Vietnamese businesses Connect the ecosystem: policymakers ‚Äì enterprises ‚Äì startups ‚Äì experts Share hands-on transformation stories and repeatable implementation roadmaps Move from vision to measurable outcomes Promote data-driven decision making and standardized delivery practices Speakers Hung Nguyen Gia ‚Äì Head of Solutions Architecture, AWS Son Do ‚Äì Technical Account Manager, AWS Nguyen Van Hai ‚Äì Director of Software Engineering, Techcombank Phuc Nguyen ‚Äì Solutions Architect, AWS Alex Tran ‚Äì Head of AI, OCB Nguyen Minh Ngan ‚Äì AI Specialist, OCB Nguyen Manh Tuyen ‚Äì Head of Data Applications, LPBank Securities Vinh Nguyen ‚Äì Co-founder \u0026amp; CTO, Ninety Eight Hung Hoang ‚Äì Customer Solutions Manager, AWS Key Highlights Morning Panel: Navigating the GenAI Revolution Build organizational capability to absorb AI: data, processes, people Start small, measure clearly, iterate fast, and scale with control Change management: leadership roles, internal communication, and AI ethics framework Define KPI/OKR for AI use cases to track ROI and tangible value Establish guardrails to mitigate risks: model bias, data leakage, and output misuse Track 1: Gen AI and Data Build a unified ‚Äúdata foundation‚Äù: quality, lineage, governance, and security GenAI on AWS roadmap: from data readiness to model operations AI across the software development lifecycle: from requirements to testing Design guardrails and least-privilege access control Construct knowledge stores with vector stores, feature stores, and data cataloging Model observability: drift, output quality, and inference cost management AI/ML cost governance: model selection, context optimization, caching, and batching Track 2: Migration \u0026amp; Modernization Large-scale migration lessons: insights from financial services Application modernization with AI-powered coding assistants and automated testing Transform VMware infrastructure to AWS: minimize downtime and optimize operations cost Security at scale: embed security end-to-end from build to run Path from monolith ‚Üí microservices, containerization and/or serverless by domain IaC \u0026amp; policy-as-code: standardize configs, cut operational errors, enable auditing Automate CI/CD pipelines, regression testing, and security scans pre-release What I Learned GenAI is the next wave of transformation AI reshapes how businesses build products, serve customers, and operate. Success requires integrating AI into strategy, culture, and core capabilities. Apply product thinking to AI: measure, iterate, and scale. Data is the foundation of every AI initiative Unified, high-quality, well-governed data determines AI effectiveness. Standardize metadata, lineage, sensitivity classification, and role-based access. Treat data as a strategic asset, not just technical input. Security and responsibility enable sustainable AI Security-by-design: encryption, data domain separation, and least privilege. Control model inputs/outputs, filter content, and trace requests and decisions. Transparency, fairness, and regulatory compliance are baseline requirements for modern AI deployments. Applying at Work Launch GenAI POCs with clear objectives (content generation, knowledge search, business assistants) Standardize data catalogs, establish lineage, and role-based access policies Embed security/privacy controls directly into build and run pipelines Adopt an ‚ÄúAI-by-default‚Äù mindset for automatable workflows Set up an AI/ML CoE, implementation playbooks, and security checklists Train teams on prompt engineering, output evaluation, and AI ethics Build a prioritized use case backlog by value vs complexity Event Experience Attending AWS Cloud Day 2025 gave me a clear pathway to bring AI into the organization: start with trustworthy data, set explicit business value metrics, then progressively raise levels of automation. What struck me most was how leading organizations balance experimentation speed with operational discipline. I left with a practical action list and strong motivation to drive change within the team.\nBroader View of Technology and Trends Recognize the pivotal role of data ‚Äì GenAI ‚Äì security in strategy. See AWS‚Äôs priorities in Vietnam and the region around AI, modernization, and security. Understand the picture from strategy to operations, not just isolated technologies. Connect and Learn from Experts Hear cross-industry case studies from banking, tech, and startups. Exchange directly with architects, leaders, and practitioner communities. Expand networks for collaboration and real-world implementation sharing. New Mindset for Application Embed AI into daily workflows instead of scattered pilots. Establish a solid data foundation before ‚Äúburning‚Äù budget on large models. Make security, privacy, and compliance design criteria from the start. Inspired to Innovate Embrace the spirit of ‚ÄúThink Big, Build Smart, Ship Secure‚Äù. Accelerate learning, controlled experimentation, and internal sharing. Commit to practical, measurable digital and AI transformation. Lessons learned AI delivers sustainable value only when tightly connected to strategy, standardized data, and disciplined security. Organizations must be flexible in experimentation yet rigorous in operations to scale without losing control. The clear path: trustworthy data ‚Üí KPI-backed use cases ‚Üí robust guardrails ‚Üí staged automation.\nSome Photos "
},
{
	"uri": "http://localhost:1313/Report-AWS/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report ‚ÄúAWS Cloud Mastery Series #1‚Äù Event Objectives Update the AI/ML landscape in Vietnam and the region Equip attendees with AWS AI/ML foundations, with a focus on SageMaker Demonstrate Generative AI on Amazon Bedrock through illustrative demos Strengthen community connections and share practical implementation stories Speakers Lam Tuan Kiet ‚Äî Sr. DevOps Engineer, FPT Software Danh Hoang Hieu Nghi ‚Äî AI Engineer, Renova Cloud Dinh Le Hoang Anh ‚Äî Cloud Engineer Trainee, First Cloud AI Journey Key Highlights AI/ML Services Landscape on AWS Overview of AWS‚Äôs AI/ML services and common use cases SageMaker as a full lifecycle ML platform: from data ingest to inference Reference workflow: data cleansing, training, evaluation, packaging, and deployment Practical MLOps: automate pipelines, monitor, and govern model lifecycles Demo with SageMaker Studio covering frequent scenarios Generative AI with Amazon Bedrock Foundation Models (Claude, Llama, Titan) and task‚Äëaligned selection criteria Prompt Engineering \u0026amp; Chain‚Äëof‚ÄëThought: structuring prompts to improve answers RAG: combining knowledge retrieval with FMs for accuracy and freshness Bedrock Agents: orchestrate multi‚Äëstep workflows, tools, and enterprise data Guardrails: control content, reduce bias, and mitigate data‚Äëleak risks Demo building a context‚Äëaware GenAI chatbot using internal documents What I Learned Grasping and Applying AWS AI/ML Services End‚Äëto‚Äëend model building on SageMaker: prepare data, train, and deploy Apply MLOps to optimize time/cost and automate key ML stages Translate knowledge into action through SageMaker Studio demos Applying GenAI with Amazon Bedrock Choose the right FM (Claude, Llama, Titan) based on business needs Use Prompt Engineering and RAG to improve quality and reliability Build chatbots with guardrails and coordinate tasks via Bedrock Agents Applying to Work Automate and optimize data‚Äëto‚Äëmodel workflows\nUse SageMaker + MLOps to shorten development, training, and deployment Build internal Generative AI applications\nCombine Bedrock, RAG, and Prompt Engineering for customer support bots, employee assistants, and document‚Äëdriven Q\u0026amp;A Boost productivity and decision‚Äëmaking with AI\nLeverage FMs for faster analysis, report generation, NLP, and support for business/tech teams Event Experience Attending ‚ÄúAWS Cloud Mastery Series #1‚Äù offered a clear, hands‚Äëon, and inspiring learning journey: we updated knowledge in AI/ML and GenAI, watched real demos, engaged with experts, and explored how to bring these capabilities into day‚Äëto‚Äëday work. Highlights:\nHands‚Äëon Content, Easy to Implement Clear delivery with examples and live demos made concepts easy to grasp Materials/guides helped map learning directly to concrete use cases Energetic Learning, Broad Connections Direct interaction with speakers/experts through open Q\u0026amp;A Effective networking expanded ties across the AI/ML community Lessons learned AI/ML and GenAI are increasingly accessible via platforms like SageMaker and Bedrock, accelerating experimentation and delivery Prompt Engineering and RAG are pivotal, determining output quality and reliability Real value comes when technology is designed around business goals, not isolated from context "
},
{
	"uri": "http://localhost:1313/Report-AWS/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Phan Minh Triet\nPhone Number: 0947720616\nEmail: phanminhtriet283@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction to Amazon Bedrock Amazon Bedrock is a fully managed platform that provides access to a wide range of powerful large language models (LLMs). It enables applications to interact with AI models through simple API calls without the need to deploy or manage machine learning infrastructure. Bedrock supports a variety of foundation models such as Claude, Llama, and Titan, making it suitable for tasks like text generation, question-answering, content summarization, and other AI-driven use cases. In a serverless architecture, Bedrock can be integrated directly with AWS Lambda to build lightweight, scalable, and easily maintained AI services. Workshop Overview In this workshop, you will build a simple question-and-answer service using Amazon Bedrock. When a user submits a query, a Lambda function will construct a prompt, send it to a Bedrock model, and return the generated response.\nTo support this workflow, the workshop makes use of three core components:\nLambda function ‚Äì acts as the processing layer, receiving input and invoking Bedrock‚Äôs API. Amazon Bedrock Runtime ‚Äì performs the inference based on the selected model. API Gateway (optional) ‚Äì exposes an HTTP endpoint for external clients or users to send their questions. The high-level workflow of the system is as follows:\nA user sends a question through API Gateway. API Gateway forwards the request to the Lambda function. Lambda invokes Amazon Bedrock with the constructed prompt. Bedrock returns the generated model output. Lambda formats and returns the response to the client. The diagram below illustrates the overall architecture used in this workshop:\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives Connect and get acquainted with members of the First Cloud Journey (FCJ) team. Understand basic AWS services, how to create and manage costs with an AWS account. Learn how to use the AWS Console and AWS CLI to interact with and manage services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Get to know FCJ members - Read and note internship regulations and rules 08/09/2025 08/09/2025 3 - Learn about AWS and its basic service types + Compute (EC2) + Storage (S3) + Networking (VPC) + Database (RDS) 09/09/2025 09/09/2025 cloudjourney.awsstudygroup.com 4 - Create an AWS Free Tier account - Learn AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Manage identity and access permissions + Install and configure AWS CLI + Use AWS CLI with basic operations 10/09/2025 10/09/2025 cloudjourney.awsstudygroup.com 5 - Learn how to effectively manage costs using AWS Budgets + Cost Budget + Usage Budget + Reservation (RI) Budget + Savings Plans Budget - Practice: + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Savings Plans Budget + Clean up unused resources 11/09/2025 11/09/2025 cloudjourney.awsstudygroup.com 6 - Learn about AWS Support service - AWS Support Plans: + Basic, Developer, Business, and Enterprise - Types of support requests: + Account and Billing support + Service Limit Increase + Technical Support - Practice: + Select Basic Support plan + Create a support case 12/09/2025 12/09/2025 cloudjourney.awsstudygroup.com Week 1 Achievements Understood what AWS is and the main service groups:\nCompute: Provides computing resources for applications such as virtual machines and containers. Storage: Used for storing, backing up, and recovering data. Networking: Manages network infrastructure, security, and connectivity between AWS resources. Database: Offers both relational and non-relational database management services. Successfully created and configured an AWS Free Tier account.\nLearned to create and manage Groups and Users in IAM.\nUnderstood how to log in using IAM and that users within the same group share the same permissions.\nBecame familiar with the AWS Management Console and how to navigate, access, and use services from the web interface.\nInstalled and configured AWS CLI on the local machine, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nChecking account and configuration information Listing available regions Creating and deleting S3 Buckets Using Amazon SNS Creating IAM Groups, Users, and adding users to groups Creating and deleting Access Keys Creating and configuring a basic VPS Running and terminating EC2 Instances Learned how to manage and monitor costs in AWS through:\nCreating and configuring Budget types (Cost, Usage, RI, Savings Plan). Cleaning up unused resources for efficient cost management. Understood the AWS Support Plans and learned how to create support requests via the AWS Support Center:\nBasic: Free, provides support for account and billing issues through the Help Center. Developer: $29/month, includes basic architectural guidance and unlimited technical support from the root account. Business: $100/month, commonly used by small and medium businesses with features such as use-case-based guidance, access to AWS Support API, and unlimited support requests from all IAM users. Enterprise: $15,000/month, for large-scale enterprises with strict security and reliability standards, offering advanced architectural, infrastructure, strategic, and cost optimization support, with priority case handling. Became comfortable using both AWS Console and AWS CLI for basic operations.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand the concept of Amazon Virtual Private Cloud (VPC) and its importance in AWS architecture. Learn how to design, deploy, and manage a virtual private network on AWS. Learn how to set up a AWS Site-to-Site VPN connection between On-Premise and AWS Cloud environments. Master network security best practices following the AWS Well-Architected Framework. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Overview of Amazon VPC - Study AWS network architecture and the role of VPC in the cloud environment - Understand the concepts: Subnet, Route Table, Internet Gateway, NAT Gateway, VPC Peering 15/09/2025 15/09/2025 cloudjourney.awsstudygroup.com 3 - Hands-on: + Create a VPC with a standard network structure + Create Public and Private Subnets + Configure Route Table and Internet Gateway for Internet access 16/09/2025 16/09/2025 cloudjourney.awsstudygroup.com 4 - Learn about Network Security on AWS + Security Groups + Network ACLs + Compare and practice configuring access rules - Hands-on: + Create Security Groups and Network ACLs for VPC + Test access control using EC2 instances 17/09/2025 17/09/2025 cloudjourney.awsstudygroup.com 5 - Introduction to AWS Site-to-Site VPN - Study the connection model between On-premise and AWS Cloud - Hands-on: + Create Virtual Private Gateway (VGW) and Customer Gateway (CGW) + Establish VPN connection between both environments + Verify connection status and routing configuration 18/09/2025 18/09/2025 cloudjourney.awsstudygroup.com 6 - Summarize and review Week 2 learning outcomes - Advanced Practice: + Design VPC based on AWS Well-Architected Framework + Automate infrastructure deployment using Infrastructure as Code (IaC) templates (CloudFormation or Terraform) + Clean up resources after completing the workshop 19/09/2025 19/09/2025 cloudjourney.awsstudygroup.com Week 2 Achievements: Clearly understood what Amazon VPC is and the importance of network isolation in AWS Cloud.\nMastered the basic structure of a VPC, including:\nSubnets: Divide network segments for different resources. Route Tables: Define routing paths for network traffic within VPC. Internet Gateway: Enables public subnet Internet access. NAT Gateway: Allows private subnet instances to securely connect to the Internet. VPC Peering: Connects two different VPCs for resource sharing. Successfully deployed a VPC with both public and private subnets, and verified connectivity through EC2 instances.\nUnderstood and configured Security Groups and Network ACLs, differentiating their scopes and use cases.\nLearned the process of setting up a Site-to-Site VPN between on-premise systems and AWS:\nCreated and configured Virtual Private Gateway (VGW) and Customer Gateway (CGW). Established VPN connections using routing and tunnel configuration parameters. Verified VPN connection via AWS CLI and AWS Management Console. Mastered advanced network security practices following the AWS Well-Architected Framework, including:\nNetwork segmentation. Access control at both subnet and instance levels. Data encryption during transmission over VPN. Gained hands-on experience with Infrastructure as Code (IaC) to automate the creation of VPCs, Subnets, Route Tables, and Security Groups.\nCompleted the workshop by:\nDesigning and deploying a complete VPC model. Successfully establishing a Site-to-Site VPN connection. Cleaning up all AWS resources after completion. Summary of Knowledge Gained: Amazon VPC: Gained in-depth understanding of virtual networking in AWS. Network Security: Learned to configure and manage secure access with Security Groups and Network ACLs. VPN Connection: Understood how to set up and maintain secure Site-to-Site VPN connections. AWS CLI \u0026amp; Console: Practiced deploying, testing, and cleaning up resources using both tools. IaC (Infrastructure as Code): Learned how to automate AWS infrastructure deployment efficiently. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand the concept and functionality of Amazon EC2 (Elastic Compute Cloud). Learn how to launch, connect, configure, and manage EC2 instances on both Windows and Linux. Practice deploying a sample web application (AWS User Management) on EC2 instances. Learn how to monitor, secure, and clean up EC2 resources efficiently. Tasks to be carried out this week: No. Task Start Date Completion Date Reference 1 - Overview of Amazon EC2 and its key concepts: + Instance, AMI, Key Pair, Elastic IP, Security Group, Volume + EC2 pricing models (On-Demand, Spot, Reserved, Savings Plan) 22/09/2025 22/09/2025 cloudjourney.awsstudygroup.com 2 - Hands-on: Create and configure a Windows EC2 instance + Choose AMI and instance type + Configure key pair and security group + Connect using Remote Desktop (RDP) + Explore Windows Server 2022 environment 23/09/2025 23/09/2025 cloudjourney.awsstudygroup.com 3 - Hands-on: Create and configure a Linux EC2 instance + Launch Amazon Linux 2 + Connect via SSH using key pair + Explore the Linux environment and essential commands + Update system packages 24/09/2025 24/09/2025 cloudjourney.awsstudygroup.com 4 - Deploy sample app ‚ÄúAWS User Management‚Äù + Install Node.js, npm, and dependencies on both Linux and Windows instances + Deploy CRUD web app (User Management System) + Test Create, Read, Update, Delete, and Search functions + Share app across network using Security Groups 25/09/2025 25/09/2025 cloudjourney.awsstudygroup.com 5 - Learn EC2 monitoring and management tools: + Amazon CloudWatch (for metrics and logs) + AWS Systems Manager + EC2 Instance Connect - Clean up unused instances, Elastic IPs, and security groups 26/09/2025 26/09/2025 cloudjourney.awsstudygroup.com Week 3 Achievements: Understood the core concepts of Amazon EC2, including:\nAMI (Amazon Machine Image): Provides the OS and app template for launching instances. Instance Type: Defines computing capacity (CPU, RAM, storage). Key Pair: Used for secure login (SSH/RDP). Elastic IP: A static IP for accessing instances over the Internet. Security Group: Acts as a virtual firewall controlling inbound/outbound traffic. Successfully created and configured Windows and Linux EC2 instances:\nConnected via RDP (Windows) and SSH (Linux). Managed instances through AWS Console and CLI. Configured network rules to allow web traffic (port 80, 443, 22, 3389). Deployed the AWS User Management web application on both platforms:\nInstalled Node.js, npm, and app dependencies. Deployed a fully functional CRUD application (Add, Edit, Delete, Search users). Shared the application with other users using public IP or Elastic IP. Learned to monitor EC2 instances using:\nAmazon CloudWatch (view CPU, memory, and network usage). AWS Systems Manager for automation and instance control. EC2 Instance Connect for secure browser-based access. Practiced resource management and cost optimization:\nStopped and terminated instances when not in use. Released unused Elastic IPs. Deleted unneeded Security Groups and Key Pairs. Summary of Knowledge Gained: Amazon EC2: Deep understanding of cloud-based compute instances. Windows \u0026amp; Linux Management: Hands-on experience configuring, connecting, and securing both OS environments. Application Deployment: Deployed Node.js CRUD app on EC2 instances. Cloud Monitoring: Used CloudWatch and Systems Manager to observe performance. Cost Efficiency: Learned to clean up and optimize EC2 resources effectively. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives Understand how to grant application permissions to access AWS services. Learn the difference between Access Key/Secret Access Key and IAM Role. Know how to create and attach IAM Roles to EC2 instances for secure access to AWS services. Become familiar with AWS Cloud9 IDE, a cloud-based development environment. Practice writing, running, and debugging code directly in Cloud9 with AWS CLI integration. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Overview of AWS Identity and Access Management (IAM) - Understand how AWS controls access to resources - Review key concepts: User, Group, Policy, Role 2025-09-22 2025-09-22 cloudjourney.awsstudygroup.com 3 - Practice granting permissions via Access Key and Secret Access Key - Test application accessing AWS services with credentials - Analyze the security risks of embedding credentials in source code 2025-09-23 2025-09-23 cloudjourney.awsstudygroup.com 4 - Introduce and create IAM Role for EC2 Instances - Attach IAM Role and test application access to S3/DynamoDB - Practice revoking permissions and re-testing application 2025-09-24 2025-09-24 cloudjourney.awsstudygroup.com 5 - Introduction to AWS Cloud9 IDE - Create a Cloud9 environment and configure workspace - Explore features: terminal, file explorer, debugger, syntax highlighting 2025-09-25 2025-09-25 cloudjourney.awsstudygroup.com 6 - Advanced Practice: + Write AWS CLI scripts within Cloud9 + Deploy Node.js CRUD app (AWS User Management) in Cloud9 + Clean up AWS resources after completion 2025-09-26 2025-09-26 cloudjourney.awsstudygroup.com Week 4 Achievements: Fully understood the difference between Access Key/Secret Key and IAM Role, and why IAM Roles are more secure. Learned how to create and configure IAM Roles with proper permissions (e.g., read/write access to S3). Successfully attached IAM Roles to EC2 instances and accessed AWS services securely. Became familiar with AWS Cloud9 IDE: Created and configured development environments. Connected to EC2 instance. Executed AWS CLI commands and tested sample code directly in the IDE. Built and deployed a simple Node.js application on Cloud9 to interact with AWS S3 and DynamoDB. Practiced resource cleanup after workshops to avoid unnecessary billing. Summary of Knowledge Gained: IAM Role: Secure and scalable permission model for EC2. Access Key/Secret Key: Recognized risks of storing static credentials in code. AWS Cloud9: Browser-based IDE supporting multiple languages and AWS integration. AWS CLI \u0026amp; SDK: Tools for interacting programmatically with AWS services. Practical Experience: Configured, tested, and cleaned up AWS resources effectively. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Gain a deep understanding of Amazon S3 ‚Äî object storage model, key features, and use cases. Practice hosting a static website on S3: enable the feature, configure public access, test, and optimize performance. Learn how to secure buckets (Block Public Access, IAM policy, Bucket Policy) while allowing public access for specific objects when necessary. Perform advanced management operations: Versioning, S3 Transfer Acceleration, S3 Batch / S3 Replication (cross-region replication), and object migration. Understand how to clean up resources to avoid unwanted costs and follow S3 best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 Environment setup \u0026amp; basic theory + Overview of S3: object, bucket, key, region, storage class (STANDARD, IA, GLACIER). + Durability \u0026amp; availability (11 nines). + Use cases (static website, backup, data lake). 2025-10-06 2025-10-06 cloudjourney.awsstudygroup.com 3 Create an S3 bucket \u0026amp; enable Static Website Hosting + Create a bucket (naming rules, region selection). + Upload index.html / error.html files. + Enable static website hosting and test the endpoint. 2025-10-07 2025-10-07 cloudjourney.awsstudygroup.com 4 Configure Block Public Access \u0026amp; Bucket Policy + Understand Block Public Access at account and bucket levels. + Configure Bucket Policy to allow public access only for index.html. + Test browser access and validate security. 2025-10-08 2025-10-08 cloudjourney.awsstudygroup.com 4 Optimize performance \u0026amp; advanced security + Compare S3 + CloudFront vs S3 Transfer Acceleration vs AWS Amplify Hosting. + Enable S3 Transfer Acceleration and test with curl from multiple regions. + (Optional) Create a CloudFront distribution for HTTPS + CDN. + Monitor via CloudWatch and track costs. 2025-10-09 2025-10-09 cloudjourney.awsstudygroup.com 6 Versioning, Lifecycle \u0026amp; Replication + Resource cleanup + Enable bucket versioning, test overwrite and restore old versions. + Set lifecycle rules to transition objects to IA/Glacier. + Configure Cross-Region Replication (CRR), create IAM replication role. + Move/copy/sync objects across buckets or regions. + Clean up resources: delete test objects, disable acceleration, remove CloudFront (if any), delete test bucket. 2025-10-10 2025-10-10 cloudjourney.awsstudygroup.com Week 4 Achievements: Configuration and basic operations:\nSet up and configured AWS CLI to work with Amazon S3 (Access Key, Secret Key, default Region). Verified connection and configuration using: aws configure list aws s3 ls to list existing buckets. aws s3api list-buckets for detailed information. Created and deleted buckets via CLI: aws s3 mb s3://bucket-name aws s3 rb s3://bucket-name --force Static Website Hosting Practice:\nCreated a new bucket, uploaded index.html and error.html. Enabled Static Website Hosting and successfully accessed the website through the public endpoint. Verified 403/404 errors and ensured error.html was correctly displayed. Security and Access Management:\nEnabled and disabled Block Public Access at both account and bucket levels to understand behavior. Configured Bucket Policy to allow only index.html to be publicly accessible. Tested public/private object access for permission validation. Data Management and Performance Optimization:\nEnabled Versioning on the bucket, uploaded multiple object versions, and restored older ones. Configured Lifecycle Rules to automatically move infrequently accessed data to IA or Glacier. Enabled S3 Transfer Acceleration and measured upload speed differences across regions using curl. Advanced Operations and Data Movement:\nConfigured Cross-Region Replication (CRR) to copy objects to a different region, understood IAM Role requirements (Replication Role). Practiced CLI operations: aws s3 cp, aws s3 mv, aws s3 sync for moving and syncing data between buckets. Verified successful replication in the destination bucket. Cleanup \u0026amp; Cost Optimization:\nDeleted test objects, disabled Transfer Acceleration and CloudFront (if used). Deleted unused test buckets to prevent additional charges. Compiled best practices for cost and security: Avoid public buckets entirely. Use Versioning + Lifecycle Rules for recovery and cost reduction. Use S3 Storage Lens and CloudWatch Budgets to monitor and control costs. Notes: Never make the entire bucket public; use Bucket Policy to expose only specific objects. Enable Block Public Access at the account level by default. Use Versioning + Lifecycle Rules to prevent data loss and reduce cost. Use CloudFront or Amplify Hosting for HTTPS, CDN, and modern static site deployment. Monitor costs using CloudWatch and S3 Storage Lens. Delete carefully when Versioning is enabled‚Äîremove all versions and delete markers before deleting a bucket. Summary of Knowledge Gained: Operate Amazon S3 at a production level: hosting, security, cost optimization, versioning, and replication. Choose the right tool for static website hosting: Amplify (recommended), CloudFront + S3, or S3 only for simplicity. Perform large-scale data management: object transfer, cross-region replication, and automation with Lifecycle Rules. Clean up resources properly to avoid unexpected costs. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 6 Objectives: Understand the architecture of Amazon RDS and supported engines (MySQL, PostgreSQL, MariaDB, SQL Server, etc.). Create, configure, and manage an RDS Instance at the infrastructure level. Set up parameter groups, security groups, and subnet groups for RDS. Connect to RDS from EC2, Cloud9, and from a local machine. Practice backup ‚Äì restore ‚Äì snapshot ‚Äì automated backup ‚Äì failover. Become familiar with Performance Insights, Monitoring, and Enhanced Logging. Learn how to optimize cost, scale storage/compute, and clean up unused resources. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Amazon RDS Overview + Supported engines + Multi-AZ \u0026amp; Read Replica architecture + Backup/Snapshot lifecycle 13/10/2025 13/10/2025 cloudjourney.awsstudygroup.com 3 Create an RDS Instance + Create a Subnet Group + Configure Security Group (allow EC2/Cloud9) + Choose engine, instance size, storage 14/10/2025 14/10/2025 cloudjourney.awsstudygroup.com 4 Connect \u0026amp; Operate on RDS + Connect from Cloud9/EC2 + Create database \u0026amp; table + CRUD using MySQL Client or PostgreSQL Client 15/10/2025 15/10/2025 cloudjourney.awsstudygroup.com 5 Backup ‚Äì Restore ‚Äì Snapshot ‚Äì Monitoring + Create manual snapshot + Restore from snapshot + Monitor CPU, connections + Use Performance Insights 16/10/2025 16/10/2025 cloudjourney.awsstudygroup.com 6 Scaling ‚Äì Cost Optimization ‚Äì Resource Cleanup + Modify instance class + Increase storage + Configure proper automated backup retention + Delete snapshots \u0026amp; RDS instances 17/10/2025 17/10/2025 cloudjourney.awsstudygroup.com Achievements in Week 6: 1. Solid understanding of Amazon RDS architecture: Differentiated between Single-AZ, Multi-AZ, and Read Replica. Understood how automated backups work (retention 1‚Äì35 days). Understood that snapshots are manual backups and not auto-deleted. 2. Successfully created and configured a full RDS Instance: Created a DB Subnet Group with 2 subnets across 2 AZs.\nCreated a Security Group allowing connections from EC2/Cloud9 (port 3306 or 5432).\nConfigured the instance with:\nEngine (MySQL/PostgreSQL) Instance class (db.t3.micro) Storage (20GB gp3) Backup retention Public/Private endpoint options 3. Successfully connected from EC2 / Cloud9 / local machine: Installed MySQL/PostgreSQL client.\nConnected using the command:\nmysql -h endpoint.amazonaws.com -u admin -p Created database, table, and performed CRUD operations:\nCREATE DATABASE demo; CREATE TABLE users (...); INSERT, SELECT, UPDATE, DELETE 4. Mastered Backup ‚Äì Restore ‚Äì Snapshot flow: Created manual snapshots. Restored a new RDS instance from snapshots. Observed backup windows. Verified daily automated backups. 5. Monitoring \u0026amp; Performance: Monitored CPU, RAM, and active connections from the Monitoring dashboard. Used Performance Insights to identify heavy queries. Checked Slow Query Log (if enabled). 6. Scaling \u0026amp; Cost Optimization: Modified instance class from t3.micro ‚Üí t3.small. Increased storage from 20GB ‚Üí 30GB. Reduced backup retention to 3 days for cost savings. Removed unnecessary snapshots. 7. Cleaned up resources to avoid extra charges: Deleted the RDS instance. Deleted subnet group. Deleted security group. Deleted old snapshots. Summary of Knowledge Gained: Understood how Amazon RDS works and common database engines. Learned how to create, configure, and connect to RDS from Cloud9 and EC2. Gained knowledge of backup, snapshot, and restore workflows. Used Performance Insights for performance monitoring. Understood how to scale and optimize RDS cost. Cleaned up resources properly to avoid unnecessary fees. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 7 Objectives: Amazon Route 53 ‚Äì DNS \u0026amp; Domain Management\nUnderstand DNS architecture and Route 53‚Äôs role in AWS. Create Public Hosted Zones and manage A/AAAA/CNAME records. Integrate Route 53 with CloudFront and API Gateway. Learn Simple, Weighted, Latency, and Failover routing. AWS WAF ‚Äì Web Application Firewall\nUnderstand Layer 7 protection mechanisms. Create Web ACL and enable AWS Managed Rules. Configure IP blocking and rate limiting. Attach WAF to CloudFront to secure API and website traffic. Amazon CloudFront ‚Äì CDN \u0026amp; Edge Security\nCreate CDN distribution for S3 and API Gateway origins. Configure cache behaviors, TTL settings, and HTTPS. Use Origin Access Control for securing S3 buckets. Enable access logs for monitoring. AWS CloudWatch ‚Äì Monitoring \u0026amp; Observability\nCreate CloudWatch Dashboard for Edge Layer. Analyze WAF \u0026amp; CloudFront logs using Logs Insights. Build alarms for abnormal traffic spikes. Understand metrics such as request count, cache hit ratio, blocked requests. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Route 53 ‚Äì Domain \u0026amp; DNS Setup ‚Ä¢ Create Hosted Zone. ‚Ä¢ Add A/AAAA alias to CloudFront. ‚Ä¢ Verify DNS propagation. 20/10/2025 20/10/2025 cloudjourney.awsstudygroup.com 3 CloudFront ‚Äì Distribution Setup ‚Ä¢ Create distribution for S3/API. ‚Ä¢ Enable HTTPS and OAC. ‚Ä¢ Configure cache behavior. ‚Ä¢ Enable access logs. 21/10/2025 21/10/2025 cloudjourney.awsstudygroup.com 4 AWS WAF ‚Äì Web ACL Configuration ‚Ä¢ Create Web ACL. ‚Ä¢ Enable Managed Rules. ‚Ä¢ Add IP block \u0026amp; rate limit rules. ‚Ä¢ Attach to CloudFront. 22/10/2025 22/10/2025 cloudjourney.awsstudygroup.com 5 CloudWatch ‚Äì Logs \u0026amp; Metrics Analysis ‚Ä¢ Create Log Groups. ‚Ä¢ Query logs via Logs Insights. ‚Ä¢ Create Metric Filter for blocked requests. ‚Ä¢ Build alarms. 23/10/2025 23/10/2025 cloudjourney.awsstudygroup.com 6 Security Hardening ‚Ä¢ Enforce HTTPS redirection. ‚Ä¢ Optimize cache TTL. ‚Ä¢ Analyze attack patterns in logs. ‚Ä¢ Enable AWS Shield Standard. 24/10/2025 24/10/2025 cloudjourney.awsstudygroup.com 7 Resource Cleanup ‚Ä¢ Remove test Web ACL. ‚Ä¢ Delete unused Log Groups. ‚Ä¢ Disable Distribution. ‚Ä¢ Review Billing for WAF/CF logs. 25/10/2025 25/10/2025 cloudjourney.awsstudygroup.com Achievements in Week 7: 1. Strong understanding of Route 53 DNS architecture Configured public DNS and domain routing successfully. Understood traffic flow: DNS ‚Üí CloudFront ‚Üí API. 2. Successfully deployed CloudFront CDN Improved global latency. Configured cache behavior and encryption. Protected S3 using OAC. 3. Built a secure Layer 7 protection system using AWS WAF Blocked malicious traffic (SQLi, XSS, bots). Implemented rate limiting. Analyzed WAF logs for threat detection. 4. Built observability with CloudWatch Created Edge Monitoring Dashboard. Queried logs using Logs Insights. Set alarms for suspicious activity. Summary of Knowledge Gained: Understanding of AWS Edge Services (Route 53, CloudFront, WAF). Ability to secure and accelerate applications using CDN. Hands-on experience in threat detection \u0026amp; log analysis. Ability to build a front-door security system following AWS best practices. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 8 Objectives: Amazon API Gateway ‚Äì API Layer\nUnderstand REST API architecture and components. Build REST API with multiple resources and methods. Configure Integration Requests/Responses with Lambda. Enable CORS following best practices. Configure Custom Domain Name with ACM SSL. Enable execution logging and access logging. Test API using Postman and frontend clients. Amazon Cognito ‚Äì Authentication \u0026amp; Authorization\nLearn Cognito User Pool vs Identity Pool. Create User Pool, App Client, and Hosted UI domain. Configure password policy and email verification. Create Identity Pool for AWS credentials. Integrate Cognito with API Gateway using JWT Authorizer. Test end-to-end login ‚Üí token ‚Üí protected API. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 API Gateway ‚Äì Architecture \u0026amp; REST API Setup ‚Ä¢ Study REST API. ‚Ä¢ Create resources /auth, /users, /items. ‚Ä¢ Configure CRUD methods. 27/10/2025 27/10/2025 cloudjourney.awsstudygroup.com 3 Lambda Integration ‚Ä¢ Create Lambda handlers. ‚Ä¢ Configure IAM roles. ‚Ä¢ Enable Proxy Integration. ‚Ä¢ Test API with Postman. 28/10/2025 28/10/2025 cloudjourney.awsstudygroup.com 4 Cognito User Pool \u0026amp; Identity Pool Setup ‚Ä¢ Create User Pool. ‚Ä¢ Configure password/email policies. ‚Ä¢ Create App Client \u0026amp; Hosted UI. ‚Ä¢ Test signup/login. 29/10/2025 29/10/2025 cloudjourney.awsstudygroup.com 5 JWT Authorization with Cognito ‚Ä¢ Create JWT Authorizer. ‚Ä¢ Attach to /users/* route. ‚Ä¢ Test Access \u0026amp; ID Tokens. ‚Ä¢ Configure 401/403 responses. 30/10/2025 30/10/2025 cloudjourney.awsstudygroup.com 6 Custom Domain + Logging Setup ‚Ä¢ Create custom API domain with ACM SSL. ‚Ä¢ Map domain to API stage. ‚Ä¢ Enable execution \u0026amp; access logs. ‚Ä¢ Analyze logs in CloudWatch Insights. 31/10/2025 31/10/2025 cloudjourney.awsstudygroup.com 7 End-to-End Testing \u0026amp; Cleanup ‚Ä¢ Test full login-to-API flow. ‚Ä¢ Remove unused Lambda/API test resources. ‚Ä¢ Delete test users. ‚Ä¢ Review Billing. 01/11/2025 01/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 8: 1. Built a complete REST API backend with API Gateway Configured multiple routes and methods. Implemented CORS and Lambda Proxy Integration. Successfully tested using Postman. 2. Implemented full authentication system using Cognito Created User Pool and Identity Pool. Enabled secure signup \u0026amp; login flows. Understood JWT token structure and validation. 3. Secured API using JWT Authorizer Protected sensitive routes (/users/*). Tested access with valid, expired, and invalid tokens. 4. Set up monitoring and logging for API Gateway Enabled CloudWatch Logs for debugging. Queried logs using Logs Insights. Investigated 4XX/5XX errors. Summary of Knowledge Gained: Strong understanding of API Gateway operation. Deep knowledge of Cognito authentication flows. Ability to secure APIs using JWT-based authorization. Experience integrating API Gateway + Lambda + Cognito. Familiarity with CloudWatch monitoring and API logging. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 9 Objectives: AWS Lambda ‚Äì Serverless Compute\nUnderstand Lambda execution lifecycle (init, invoke, freeze). Build backend Lambda functions. Integrate Lambda with API Gateway using Lambda Proxy. Configure IAM roles for RDS, S3, and Bedrock access. Use AWS Secrets Manager for secure credential storage. Monitor Lambda via CloudWatch Logs and Metrics. Lambda in VPC \u0026amp; VPC Endpoints\nDeploy Lambda inside private subnets for secure backend operations. Configure Security Groups for Lambda ‚Üí RDS communication. Create VPC Endpoints: S3 Gateway Endpoint for internal S3 access Bedrock Interface Endpoint for private AI API calls Test Lambda connectivity to both RDS and Bedrock. Performance \u0026amp; Cost Optimization\nLearn cold start behaviors and mitigation strategies. Tune memory \u0026amp; timeout settings for best performance. Reduce NAT Gateway traffic using VPC Endpoints. Apply database connection pooling to reduce overhead. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Lambda ‚Äì Overview \u0026amp; Function Creation ‚Ä¢ Study lifecycle. ‚Ä¢ Create first function. ‚Ä¢ Configure IAM execution role. ‚Ä¢ Run test. 03/11/2025 03/11/2025 cloudjourney.awsstudygroup.com 3 Lambda Integration with API Gateway ‚Ä¢ Attach Lambda to REST API. ‚Ä¢ Enable Proxy Integration. ‚Ä¢ Enable CORS. ‚Ä¢ Test with Postman. 04/11/2025 04/11/2025 cloudjourney.awsstudygroup.com 4 Lambda in VPC ‚Äì Private Deployment ‚Ä¢ Attach Lambda to private subnets. ‚Ä¢ Configure SG rules. ‚Ä¢ Fix timeout issues. ‚Ä¢ Validate DB queries. 05/11/2025 05/11/2025 cloudjourney.awsstudygroup.com 5 VPC Endpoints Setup ‚Ä¢ Create S3 Gateway Endpoint. ‚Ä¢ Create Bedrock Interface Endpoint. ‚Ä¢ Test inference from Lambda. 06/11/2025 06/11/2025 cloudjourney.awsstudygroup.com 6 Secrets Manager Integration ‚Ä¢ Store DB credentials. ‚Ä¢ Retrieve via SDK. ‚Ä¢ Apply IAM policies. ‚Ä¢ Test secure DB connection. 07/11/2025 07/11/2025 cloudjourney.awsstudygroup.com 7 Performance \u0026amp; Cost Optimization ‚Ä¢ Tune memory/timeouts. ‚Ä¢ Implement pooling. ‚Ä¢ Analyze NAT vs Endpoint cost. ‚Ä¢ Review Billing. 08/11/2025 08/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 9: 1. Built a functional serverless backend with AWS Lambda Multiple Lambda functions handling API operations. Understood cold starts and how to optimize them. IAM roles configured securely for resource access. 2. Lambda successfully deployed inside VPC Stable connection to RDS from private subnets. Resolved networking issues (SG, routing, NACL). Verified smooth query execution. 3. Implemented VPC Endpoints for secure \u0026amp; cost-efficient networking S3 Gateway Endpoint removed NAT dependency. Bedrock Endpoint enabled secure AI inference. Overall networking cost reduced significantly. 4. Improved security with Secrets Manager No credentials stored in environment variables. Lambda fetches secrets securely using AWS SDK. 5. Performance \u0026amp; cost improvements Better performance through memory tuning. Reduced DB overload via connection pooling. Lower NAT Gateway costs using VPC Endpoint. Summary of Knowledge Gained: Deep understanding of Lambda serverless architecture. Working knowledge of VPC networking for private workloads. Practical usage of VPC Endpoints (S3, Bedrock). Strong experience using Secrets Manager securely. Ability to optimize Lambda performance \u0026amp; cost for production systems. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Understanding and Configuring Amazon VPC\nWeek 3: Deploying and Managing Amazon EC2 Instances on Windows\nWeek 4: Implementing IAM Roles and Working with AWS Cloud9\nWeek 5: Hosting and Managing Static Websites with Amazon S3\nWeek 6: Working with Amazon RDS ‚Äì Creating, Configuring, Connecting \u0026amp; Operating Databases on AWS\nWeek 7: Route 53, AWS WAF \u0026amp; CloudFront ‚Äì Configure edge security and content delivery; monitor the system with CloudWatch\nWeek 8: API Gateway \u0026amp; Cognito ‚Äì Build the API layer and implement user authentication\nWeek 9: AWS Lambda \u0026amp; VPC Endpoints ‚Äì Deploy serverless compute and connect Lambda to RDS inside a private subnet\nWeek 10: Amazon RDS \u0026amp; S3/Bedrock Endpoints ‚Äì Optimize the database and integrate Lambda access to S3 and Bedrock via VPC Endpoints\nWeek 11: Serverless Front-end Deployment ‚Äì Host front-end on S3 + CloudFront and integrate with API Gateway; implement Cognito authentication\nWeek 12: AI Agent with Amazon Bedrock ‚Äì Build AI-powered APIs, orchestrate workflows using Lambda/Step Functions, and send notifications with SES\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.2-call-bedrock-converse/",
	"title": "Add Code to Call the Converse API",
	"tags": [],
	"description": "",
	"content": "Add Code to Call the Converse API In this step, you will add Python code to your Lambda Function to send user questions to Amazon Bedrock using the Converse API, and return the model‚Äôs response to the client.\nThe Lambda function will perform the following tasks:\nReceive a question from the client or a test event Construct a request following the Converse API format Send the prompt to Amazon Bedrock Runtime Receive the model-generated answer Return the response in JSON format üîπ Step 1 ‚Äî Open the Lambda Function source file Open the Lambda function you created Scroll to the Code source section Open the file lambda_function.py üîπ Step 2 ‚Äî Replace the entire file content with the code below import json import boto3 # Create a client to call Bedrock Runtime bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;) # Choose a model that supports the Converse API MODEL_ID = \u0026#34;anthropic.claude-3-sonnet-20240229\u0026#34; def lambda_handler(event, context): # Receive data from API Gateway or test event body = json.loads(event.get(\u0026#34;body\u0026#34;, \u0026#34;{}\u0026#34;)) if isinstance(event.get(\u0026#34;body\u0026#34;), str) else event question = body.get(\u0026#34;question\u0026#34;, \u0026#34;Hello! What would you like to ask?\u0026#34;) # Send request to Bedrock using the Converse API response = bedrock.converse( modelId=MODEL_ID, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;text\u0026#34;: question} ] } ] ) # Extract the model\u0026#39;s response answer = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;][0][\u0026#34;text\u0026#34;] # Return the result to the client return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;answer\u0026#34;: answer}) } üîπ Important note about MODEL_ID The MODEL_ID value inside lambda_function.py can be changed to any model you want to use.\nHowever, the model must support the Converse API.\nTo find the correct Model ID:\nOpen Amazon Bedrock Console ‚Üí Model catalog Select the model you want to use In the model information panel, look for Model ID Example:\nCopy this Model ID and update the variable:\nMODEL_ID = \u0026#34;your-selected-model-id\u0026#34; If the model does not support the Converse API, Lambda will throw an error when calling bedrock.converse().\nüîπ Step 3 ‚Äî Deploy the Lambda Function After updating the source code:\nClick Deploy Your Lambda Function is now ready to call Amazon Bedrock üéØ Expected Outcome At the end of this section, your Lambda Function will be able to:\nReceive questions from a client Send prompts to Bedrock using the Converse API Receive model-generated responses Return the result as JSON You have now completed the core logic of your AI Q\u0026amp;A service.\nContinue to 5.3.3 ‚Äì Test the Lambda Function.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "AWS for SAP\nAgentic AI Assistant for SAP with AWS Generative AI Written by Sourav Sadhu on September 4, 2025 in Amazon Bedrock, Amazon Bedrock Agents, Amazon Bedrock Knowledge Bases, SAP on AWS, Technical How-to Permalink Share\nIntroduction\nSAP systems are the backbone of many enterprises, managing critical business processes and generating large volumes of high-value data. These critical business processes and data often extend beyond the core enterprise SAP systems, requiring customers to interact with external systems. As organizations look to leverage this data for deeper insights and improved decision-making, there is a growing need to transform how SAP customers interact with their data and systems.\nThe natural language processing (NLP) capabilities of generative AI provide SAP users with a powerful tool to interact with complex ERP systems using natural language questions, eliminating the need for deep technical knowledge or complex SQL queries. This democratizes data access across the organization, enabling business users to ask questions, generate reports, and gather real-time insights through conversational interfaces.\nIntegrating generative AI with SAP systems enables organizations to bridge the gap between structured ERP data and unstructured information from SAP and non-SAP sources, providing a more holistic view of their business context. This integration can lead to more accurate forecasting, personalized customer experiences, and data-driven decisions that span the entire enterprise ecosystem.\nAWS and SAP empower customers at every stage of their generative AI adoption journey with a rich portfolio of advanced generative AI services, robust infrastructure, and implementation resources. These services integrate with SAP systems and complement the broader cloud service ecosystems of both AWS and SAP.\nIn this blog post (Part 1 of a 2-part series), I will describe and demonstrate how you can leverage Amazon Bedrock and other AWS services to derive insights from SAP and non-SAP data sources using human natural language through a unified interface exposed via MS Teams, Slack, and a Streamlit UI.\nIn Part 2 of this blog series, I will describe and demonstrate how you can leverage SAP BTP services [SAP Build Apps, SAP Generative AI Hub] to derive insights from SAP and non-SAP systems using natural language via a unified interface using SAP Build Apps as the front-end UI.\nOverview\nI will start by developing the business logic required to extract data from the SAP system. I will create two AWS Lambda functions to execute the business logic, supported by various AWS services including Bedrock Knowledge Bases and AWS Secrets Manager. I will then focus on creating the business logic to process data from an additional non-SAP data source by deploying another Lambda function that is designed specifically to extract data from Amazon DynamoDB, where logistics information is stored. To enhance the system‚Äôs capabilities, I will set up a knowledge base that will act as a third data source, supporting general user queries. Next, I will deploy an Amazon Bedrock Agent responsible for orchestrating the flow between the different data sources based on the user‚Äôs query. In the final stage, I will build a user interface using Streamlit, while also providing an alternative integration option with MS Teams and Slack to increase accessibility.\nFigure 1. High-level architecture\nImplementation guide\nI have structured the solution into 5 steps. Let‚Äôs walk through each step:\nStep 1 ‚Äì Create business logic to fetch data from the SAP system\nStep 2 ‚Äì Create business logic to fetch data from the non-SAP system\nStep 3 ‚Äì Create Bedrock knowledge bases for general queries\nStep 4 ‚Äì Create Bedrock Agents to orchestrate between different data sources\nStep 5 ‚Äì Build user interfaces with Microsoft Teams, Slack, and Streamlit\nPrerequisites\nAn AWS account with appropriate IAM permissions to work with Amazon S3, AWS Lambda, Amazon Bedrock Agents, Amazon Bedrock Knowledge Bases, Amazon Bedrock LLM (Claude), AWS Secrets Manager, and Amazon DynamoDB. If you are new to these services, you should review them before proceeding.\nAn SAP system that supports SAP OData services as the data source for SAP Sales Orders. I have used the standard OData service SAP Sales Order Service: API_SALES_ORDER_SRV and the Entity Set A_SalesOrder for this demo. However, you can use any OData service depending on your use case. I have exposed the OData service over the internet, but depending on where your SAP system is hosted, you may choose not to expose it over the internet. We recommend setting up a private connection for better performance and security. For more information, see How to enable OData services in SAP S/4HANA and Connecting to RISE from your AWS account. A Slack account for Slack integration and an MS Teams account for MS Teams integration [optional]. Step 1 ‚Äì Create business logic to fetch data from the SAP system\nI. I will start by creating a secret in AWS Secrets Manager to store the credentials and connection details of the S4 system.\nChoose the secret type Other type of secret and then add the following details in the Key/Value pairs. Set the secret values specific to your environment.\nSecret key Secret value S4_host_details https://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt; S4_username xxxx S4_password xxxx For more information, see Create a secret in AWS Secrets Manager.\nII. As the second step, I will create two Bedrock knowledge bases to supplement and support SAP data when required.\nOdata-Schema-knowledgebase: I will use this knowledge base to provide schema details to the LLM, so that the model has sufficient knowledge of which attributes to use when constructing the OData URL based on the user‚Äôs query. SAP-external-knowledgebase: I will use this knowledge base to provide additional details for non-SAP data. I considered the following inputs when creating the two knowledge bases, with all other settings kept at their default values.\nProvide knowledge base details Knowledge base name: Choose a name for each knowledge base with a user-friendly description. I used Odata-Schema-knowledgebase and SAP-external-knowledgebase. Knowledge base description: Provide a description that uniquely identifies your knowledge base. IAM permissions: Choose Create and use a new service role. Data source configuration Data source: Choose Amazon S3. Data source name: Choose a name for each data source. S3 URI: Create two S3 buckets, one for each knowledge base. Upload the file Sales_Order_Schema.json for Odata-Schema-knowledgebase and Shipping_Policy.pdf for SAP-external-knowledgebase from the GitHub repository to the corresponding S3 buckets and provide the S3 URIs. Storage and data handling configuration Embeddings model: Amazon Titan Text Embeddings V2. Vector database: For Vector creation method, choose Create a new vector store quickly, and use Amazon OpenSearch Serverless as the vector store. For more information, see Create a knowledge base by connecting to a data source in Amazon Bedrock Knowledge Bases.\nThis is my final view\nIII. Now, I will create two Lambda functions to extract data from the SAP system based on the user‚Äôs query.\nSAP-Odata-URL-Generation: This Lambda function executes the business logic to generate an OData URL based on the user‚Äôs query using the LLM, supplemented by schema details from the knowledge base and host details from AWS Secrets Manager. SAP-Sales-Order-Query: This Lambda function executes the core business logic to retrieve data from the SAP system. It uses the OData URL provided by the SAP-Odata-URL-Generation function and securely accesses the system credentials stored in AWS Secrets Manager. The function then processes the retrieved data, leveraging the LLM through Bedrock, and finally presents structured information back to the Bedrock Agent for further use. I considered the following inputs when creating the Lambda functions, keeping all other settings at their default values.\nChoose Author from scratch. Function name: Choose a name for each function with a user-friendly description. I selected SAP-Odata-URL-Generation and SAP-Sales-Order-Query. Runtime: Python 3.13. Architecture: x86_64. Code: Copy the code from SAP-Odata-URL-Generation.py for the SAP-Odata-URL-Generation function and SAP-Sales-Order-Query.py for the SAP-Sales-Order-Query function from the GitHub repository. Note: Adjust the code with deployment-specific values such as kb_id, SecretId, and so on. Configuration: Memory: 1024 MB, Timeout: 15 minutes. Layers: Add the requests-layer.zip layer from the GitHub repository to include the requests module in the Lambda function. Permissions: The following permissions need to be configured for the Lambda functions. SAP-Odata-URL-Generation: Execution role ‚Äì In addition to the basic Lambda execution role, create a new IAM policy with the following actions: bedrock:InvokeModel, bedrock-agent-runtime:Retrieve, secretsmanager:GetSecretValue. Resource-based policy ‚Äì lambda:InvokeFunction permission for the ARN of the Bedrock Agent that we will create in Step 4. SAP-Sales-Order-Query: Execution role ‚Äì In addition to the basic Lambda execution role, create a new IAM policy with the following actions: bedrock:InvokeModel, secretsmanager:GetSecretValue. Resource-based policy ‚Äì lambda:InvokeFunction permission for the ARN of the Bedrock Agent that we will create in Step 4. For more information, see Building Lambda functions with Python and Working with Lambda layers for Python functions.\nStep 2 ‚Äì Create business logic to fetch data from the non-SAP system\nI. I will start by creating a DynamoDB table with the following inputs:\nTable name: logistics Partition key: order_id Use the Items.json file from the GitHub repository to create items in the DynamoDB table. For more information, see Creating Amazon DynamoDB table items from a JSON file.\nII. Now, I will create a Lambda function to extract data from the DynamoDB table based on the user‚Äôs query.\nChoose Author from scratch. Function name: Choose a name for the function with a user-friendly description. I chose Logistics-System. Runtime: Python 3.13. Architecture: x86_64. Configuration: Memory: 1024 MB, Timeout: 15 minutes. Code: Copy the code from Logistics-System.py from the GitHub repository. Permissions: Add the following permissions to the Lambda function. Execution role ‚Äì In addition to the basic Lambda execution role, create a new IAM policy with the following actions: dynamodb:Query, dynamodb:DescribeTable. Resource-based policy ‚Äì lambda:InvokeFunction permission for the ARN of the Bedrock Agent that we will create in Step 4. Step 3 ‚Äì Create a knowledge base for general queries\nNow, I will create a third knowledge base. This knowledge base will act as a general information repository. Users can access this resource to learn about different topics, from organizational information to topic-specific expertise, depending on their needs.\nGeneral-information-knowledgebase: In this demo, I will use this knowledge base to provide guidance on SAP business processes.\nI considered the following inputs when creating the knowledge base, keeping the rest of the settings at their default values.\nProvide knowledge base details Knowledge base name: Choose a name for the knowledge base. I used General-information-knowledgebase with a user-friendly description. IAM permissions: Choose Create and use a new service role. Data source configuration Data source: Choose Amazon S3. Data source name: Choose any name for the data source. S3 URI: Create an S3 bucket, upload the file ‚ÄúHow to create SAP Sales Order pdf‚Äù from the GitHub repository, and provide the corresponding S3 URI. Storage and data handling configuration Embeddings model: Amazon Titan Text Embeddings V2. Vector database: For Vector creation method, choose Create a new vector store quickly and use Amazon OpenSearch Serverless as the vector store. This is my final view\nStep 4 ‚Äì Create a Bedrock Agent\nIn this step, I will create a Bedrock Agent to help orchestrate between the different data sources that we have created in the previous steps in order to answer user queries.\nI considered the following inputs when creating the Bedrock Agent, keeping all other settings at their default values.\nAgent details: Agent name: Choose a name and a user-friendly description for the agent. I named it Business-Query-System. Agent resource role: Choose Create and use a new service role. Choose model: Select Claude 3 Sonnet v1. You may choose another LLM, but you will need to adjust the prompts accordingly to achieve the desired responses. Instructions for the Agent: Provide detailed, step-by-step instructions clearly describing what you want the Agent to do. You are an AI assistant that helps users query SAP sales data directly from the SAP system and shipping details from the logistics system. You also help users with general business process queries using the company knowledge bases.\nAdditional settings: Enable User input.\nAction groups: Action groups define the tasks the agent should help the user perform.\nAction group: SAP-Sales-Order ‚Äì I will use this action group to handle any query related to SAP sales orders. Action group name: Choose a name and provide a user-friendly description. I used SAP-Sales-Order. Action group type: Defined by function details. Invocation method: Choose Use an existing Lambda function and select the Lambda function created in Step 1, SAP-Sales-Order-Query. Action function name 1: SalesOrder and provide a description for the function. Action group: Logistics-System ‚Äì I will use this action group to handle any query related to logistics information for sales orders. Action group name: Choose a name and provide a user-friendly description. I used Logistics-System. Action group type: Defined by API schema. Invocation method: Choose Use an existing Lambda function and select the Lambda function created in Step 2, Logistics-System. Action group schema: Choose an existing API schema. S3 URL: Create an S3 bucket, upload the logistics.json file from the GitHub repository to the S3 bucket, and provide the S3 URL. Memory: Choose Enabled with a memory time of 2 days and a maximum of 20 recent sessions.\nKnowledge bases: Add the previously created knowledge bases.\nSelect knowledge base: SAP-external-knowledgebase. Knowledge base instructions for the Agent: Use this knowledge base when you need information external to the SAP system and combine it with SAP system data to complete your response.\nSelect knowledge base: General-information-knowledgebase. Knowledge base instructions for the Agent: Use this knowledge base to answer general business questions from the user that are not directly available from the SAP system.\nOrchestration configuration ‚Äì Default orchestration. Bedrock Agents provide default prompt templates, but these can be customized to meet specific requirements. I will customize the following prompt templates to fit our use case.\nPre-processing: Choose Override default pre-processing template. Add the following section to the prompt template: Category F: Questions that can be answered or supported by the agent calling our function-based tools using the provided functions and arguments from the conversation history or relevant arguments gathered using the askuser function AND also require external data from the knowledge bases to complete the response. Combine data from the SAP system or the non-SAP Logistics system and the external knowledge base to prepare the final answer. Orchestration: Choose Override default orchestration template. Add the following text to the corresponding sections in the prompt template.\n$knowledge_base_guideline$\nIf any data such as the delivery date is not updated in the Logistics system, then check the knowledge base named SAP-external-knowledgebase to find the estimated delivery time based on the shipping category. Then, take that duration and add it to the ‚ÄúOrder Received‚Äù date and share the estimated delivery date with the user.\nIf the SAP system returns an error because the requested data is not available, check the knowledge base named SAP-external-knowledgebase for the explanation of the ERROR CODE. Respond to the user only with the explanation of the error code.\n$tools_guidelines$ [This section does not exist by default; we need to create it]\nCall the SAP-Sales-Order tool only for any questions related to sales orders.\nCall the Logistics-System tool only for any shipping details for sales orders.\nDO NOT call both tools SAP-Sales-Order and Logistics-System unless the user explicitly asks for both pieces of information.\n$multiple_tools_guidelines$ [This section does not exist by default; we need to create it]\nIf the user‚Äôs question requires calling more than one tool, call the tools one by one. Collect the responses from both tools and then combine them before replying to the user. For example, if the user asks for both Sales Order and Logistics information, first fetch the Sales Order details using the Sales Order tool. Then, fetch the logistics details using the Logistics tool. Finally, combine both responses into one when responding to the user.\nAfter entering all the details, I will choose Save, and then choose Prepare to update the agent with the latest changes. To navigate to the agent dashboard, select Save and exit.\nFinally, create an Alias to have a specific snapshot or version of the agent that will be used by applications.\nChoose Create alias, provide an alias name with a user-friendly description.\nChoose Create new version and link it with this alias for a version with default On-demand throughput.\nFor more information, see Create and configure an agent manually.\nThis is my final view\nNow, I need to adjust the IAM roles for the Lambda functions so that the Bedrock Agent can invoke them.\nFollow the steps in Using resource-based policies for Lambda and attach the following resource-based policy to the Lambda function to allow Amazon Bedrock to access the Lambda function for the agent‚Äôs action groups, replacing the ${values} as needed.\nJSON\n{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;AccessLambdaFunction\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;Service\u0026quot;: \u0026quot;bedrock.amazonaws.com\u0026quot;\r},\r\u0026quot;Action\u0026quot;: \u0026quot;lambda:InvokeFunction\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:lambda:${region}:${account-id}:function:function-name\u0026quot;,\r\u0026quot;Condition\u0026quot;: {\r\u0026quot;StringEquals\u0026quot;: {\r\u0026quot;AWS:SourceAccount\u0026quot;: \u0026quot;${account-id}\u0026quot;\r},\r\u0026quot;ArnLike\u0026quot;: {\r\u0026quot;AWS:SourceArn\u0026quot;: \u0026quot;arn:aws:bedrock:${region}:${account-id}:agent/${agent-id}\u0026quot;\r}\r}\r}\r]\r}\rThis is my policy\n{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Id\u0026quot;: \u0026quot;default\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;bedrock-agent-sales\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;Service\u0026quot;: \u0026quot;bedrock.amazonaws.com\u0026quot;\r},\r\u0026quot;Action\u0026quot;: \u0026quot;lambda:InvokeFunction\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:lambda:us-east-1:1234567xxxx:function:SAP-Sales-Order-Query\u0026quot;,\r\u0026quot;Condition\u0026quot;: {\r\u0026quot;StringEquals\u0026quot;: {\r\u0026quot;AWS:SourceAccount\u0026quot;: \u0026quot;1234567xxxx\u0026quot;\r},\r\u0026quot;AWS:SourceArn\u0026quot;: {\r\u0026quot;arn:aws:bedrock:us-east-1: 1234567xxxx:agent/VX5FAWE3OO\u0026quot;\r}\r}\r}\r]\r}\rStep 5 ‚Äì Build user interfaces with Microsoft Teams, Slack, and Streamlit\nThis step involves developing user interfaces that allow users to interact with the Bedrock Agent.\nMicrosoft Teams ‚Äì This integration requires access to MS Teams (with the appropriate permissions) and Amazon Q Developer in the chat application. Amazon Q Developer in the chat application (formerly AWS Chatbot) allows you to interact with Bedrock GenAI Agents within Microsoft Teams. Step 1: Configure app access\nMicrosoft Teams must be installed and approved by your organization‚Äôs administrator.\nStep 2: Configure the Teams channel\nCreate a standard MS Teams channel or use an existing channel and add Amazon Q Developer to the channel. [Note: A standard channel is required because Microsoft Teams currently does not support Amazon Q Developer in private channels.]\nIn Microsoft Teams, find and select your team name, then choose Manage team. Select Apps, then choose Add app. Enter Amazon Q Developer in the search bar to find the app. Select the bot. Choose Add to a team and complete the prompts. Step 3: Configure Amazon Q Developer for the Teams client\nThis step grants Amazon Q Developer in the chat application access to your MS Teams channel.\nOpen Amazon Q Developer in chat from the AWS console. Under Chat client configuration, choose Microsoft Teams, copy and paste the Microsoft Teams channel URL that we created in the previous step, and then choose Configure. [You will be redirected to a Teams authorization page to request permission for Amazon Q Developer to access the information.] On the Microsoft Teams authorization page, choose Accept. On the left side, you will now see your Teams channel listed under Microsoft Teams. Next, I will link the Teams channel with my configuration.\nOn the Teams details page in the Amazon Q Developer console, choose Create new channel configuration. I used the following inputs for my configuration, keeping the remaining options at their default values.\nConfiguration details: Configuration name: Choose a name for your configuration. I named mine aws-sap-demos-team. Microsoft Teams channel: Channel URL: Copy and paste the Microsoft Teams channel URL that we created in Step 2. Permissions: Role settings: Choose Channel role. Channel role: Choose Create an IAM role from a template. Role name: Choose any name. I used awschatbot-sap-genai-teams-role. Policy template: Amazon Q Developer access. Channel guardrail policies [Policy names]: AWSLambdaBasicExecutionRole, AmazonQDeveloperAccess. You can adjust IAM policies as required, but following the principle of least privilege is always recommended. Step 4: Connect the agent to the chat channel\nConnecting the Amazon Q Developer Bedrock Agent requires the IAM action bedrock:InvokeAgent.\nAdd the following policy to the IAM role awschatbot-sap-genai-teams-role created in the previous step.\nJSON\n{\r\u0026quot;Sid\u0026quot;: \u0026quot;AllowInvokeBedrockAgent\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;bedrock:InvokeAgent\u0026quot;,\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:bedrock:aws-region:\u0026lt;AWS Account ID\u0026gt;:agent-alias/\u0026lt;Bedrock Agent ID\u0026gt;/\u0026lt;Agent Alias ID\u0026gt;/\u0026quot;\r]\r}\rTo add the Bedrock Agent to your chat channel, enter the following. Choose any connection name you like.\n@Amazon Q connector add connector_name arn:aws:bedrock:aws-region:AWSAccountID:agent/AgentID AliasID\rMy entry looks like this:\n@Amazon Q connector add order_assistant arn:aws:bedrock:us-east-1:xxxxxxx:agent/VX5FAWE3OO VG92WRF1JI\rFor more information, see Tutorial: Get started with Microsoft Teams\nThe Teams interface looks like this\nSlack ‚Äì This integration requires access to Slack (with the appropriate permissions) and Amazon Q Developer in the chat application. Amazon Q Developer in the chat application (formerly AWS Chatbot) allows you to interact with Bedrock GenAI Agents in Slack. Step 1: Configure app access\nThe workspace administrator must approve the use of the Amazon Q Developer app in your Slack workspace.\nStep 2: Configure the Slack channel\nCreate a Slack channel or use an existing channel and add Amazon Q Developer to the Slack channel.\nIn your Slack channel, type /invite @Amazon Q and choose Invite Them.\nStep 3: Configure Amazon Q Developer for the Slack client\nThis step grants Amazon Q Developer in the chat application access to your Slack workspace.\nOpen Amazon Q Developer in chat from the AWS console. Under Chat client configuration, choose Slack, then select Configure. [You will be redirected to a Slack authorization page to request permission for Amazon Q Developer to access information.] Select the Slack workspace that you want to use with Amazon Q Developer and choose Allow. On the left side, you will now see your Slack workspace listed under Slack. Next, I will link a channel with my configuration.\nOn the Workspace details page in the Amazon Q Developer console, choose Create new channel configuration. I used the following inputs for my configuration, with the rest kept at their default values.\nConfiguration details: Configuration name: Choose a name for your configuration. I named it sap-genai-slack-chatbot. Amazon internal options: Account classification: Choose Non-production. Slack channel: Channel ID: Provide the channel ID of the Slack channel that you configured in Step 2. Permissions: Role settings: Choose Channel role. Channel role: Choose Create an IAM role from a template. Role name: Choose any name. I used aws-sap-genai-chatbot-role. Policy template: Amazon Q Developer access. Channel guardrail policies [Policy names]: AWSLambdaBasicExecutionRole, AmazonQDeveloperAccess. You can adjust IAM policies according to your needs, but following the principle of least privilege is recommended. Step 4: Connect the agent to the chat channel\nConnecting the Amazon Q Developer Bedrock Agent requires the IAM action bedrock:InvokeAgent.\nAdd the following policy to the IAM role awschatbot-sap-genai-teams-role created earlier.\nJSON\n{\r\u0026quot;Sid\u0026quot;: \u0026quot;AllowInvokeBedrockAgent\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;bedrock:InvokeAgent\u0026quot;,\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:bedrock:aws-region:\u0026lt;AWS Account ID\u0026gt;:agent-alias/\u0026lt;Bedrock Agent ID\u0026gt;/\u0026lt;Agent Alias ID\u0026gt;/\u0026quot;\r]\r}\rTo add the Bedrock Agent to your Slack channel, enter the following. Choose any connection name you like.\n@Amazon Q connector add connector_name arn:aws:bedrock:aws-region:AWSAccountID:agent/AgentID AliasID.\rMy entry looks like this:\n@Amazon Q connector add order_assistant arn:aws:bedrock:us-east-1:xxxxxxx:agent/VX5FAWE3OO VG92WRF1JI\rFor more information, see Tutorial: Get started with Slack\nThe Slack interface looks like this\nStreamlit ‚Äì Streamlit is an open-source Python framework commonly used to build interactive web applications from Python scripts. I followed the steps below to host the application on an Amazon EC2 instance. Launch an EC2 instance; I used an Amazon Linux t2.micro instance. Set up the EC2 instance with the required security group allowing HTTP/HTTPS traffic (ports 80/443/8501 or any other port you choose to use). Prepare the environment as follows: Install required packages\nsudo apt update\rsudo apt-get install python3-venv\rSet up a virtual environment\nmkdir streamlit-demo\rcd streamlit-demo\rpython3 -m venv venv\rsource venv/bin/activate\rInstall Streamlit\npip install streamlit\nCreate a file named streamlit-app.py using a text editor such as vi/vim/nano and copy the code from streamlit-app.py in the GitHub repository.\nRun the Streamlit application with the following command:\nnohup streamlit run streamlit-app.py \u0026amp;\nStreamlit assigns an available port starting at 8501, incrementing by 1. If you want Streamlit to use a specific port, you can use:\nstreamlit run streamlit-app.py \u0026ndash;server.port XXXX\nAfter running the above commands, I can see the URL to open the Streamlit application in my browser, as shown below.\nThe Streamlit application looks like this\nCost\nRunning Large Language Models (LLMs) requires significant infrastructure, development, and maintenance costs. However, AWS services such as Amazon Bedrock can significantly reduce these costs through simplified infrastructure management, optimized development workflows, flexible pricing models, and different cost-optimization options to access LLMs of your choice.\nAWS Service ‚Äì US East (N. Virginia) Usage Estimated cost [running for one hour] Bedrock ‚Äì Foundation LLM inference [Claude 3.5 Sonnet] 100K Input tokens, 200K Output tokens $3.3 Bedrock ‚Äì Embeddings model inference [Amazon Titan Text Embeddings v2] 100 documents, average 500 words per doc $0.10 OpenSearch Compute Unit (OCU) ‚Äì Indexing 2 OCU [minimum 2 OCU] $0.48 OpenSearch Compute Unit (OCU) ‚Äì Search and Query 2 OCU [minimum 2 OCU] $0.48 OpenSearch Managed Storage 10GB $0.24 EC2 Instance [Streamlit application] t2.micro $0.0116 Lambda, Secrets Manager, DynamoDB $0.2 Estimated cost to use the application for one hour ‚Äì $4.8116 For more information, see Amazon Bedrock pricing, Amazon OpenSearch Service Pricing, Amazon EC2 On-Demand Pricing, AWS Lambda pricing, AWS Secrets Manager pricing, Amazon DynamoDB pricing\nConclusion\nThis blog demonstrates how to build an intelligent virtual assistant that interacts seamlessly with both SAP and non-SAP systems using AWS services, with a focus on Amazon Bedrock. The solution integrates SAP systems for sales order data, non-SAP systems for logistics information, and knowledge bases for additional details, all of which are accessible through multiple user interfaces including Streamlit, Microsoft Teams, and Slack. By leveraging a suite of AWS services such as Lambda, Bedrock, Secrets Manager, and DynamoDB, the implementation enables natural language interaction with complex enterprise systems, providing unified access to diverse data sources while maintaining strong security. The serverless architecture and pay-as-you-go pricing model make this solution accessible and cost-effective for organizations looking to enhance data accessibility through conversational AI interfaces. The blog offers a detailed, step-by-step guide for deploying this solution, paving the way for enterprises to harness generative AI in their SAP and non-SAP environments.\nExperience firsthand how this solution can transform the way you interact with enterprise data by deploying it in your environment. Accelerate your digital transformation by exploring our comprehensive suite of machine learning services, including Amazon AgentCore (Preview) for deploying and operating secure AI agents at scale, Amazon Forecast for predictive analytics, Amazon Textract for intelligent document processing, Amazon Translate for language translation, and Amazon Comprehend for natural language processing. These services integrate seamlessly with SAP to meet diverse business needs and unlock new possibilities for your organization.\nVisit the AWS for SAP page to learn why thousands of customers trust AWS to migrate and innovate with SAP.\nTAGS: #saponaws, RISE with SAP, S/4HANA, SAP applications, SAP Beyond, SAP transformation\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.4-api-gateway-integration/5.4.2-integrate-lambda/",
	"title": "Integrate API with Lambda",
	"tags": [],
	"description": "",
	"content": "Integrate API with Lambda In this step, you will configure the HTTP API so that incoming requests from the client are forwarded to your Lambda function.\nAPI Gateway will serve as the HTTP interface for your Bedrock-powered chatbot.\nüîπ Step 1 ‚Äî Open the API you created Go to API Gateway Console and select the API:\nbedrock-chatbot-api\nüîπ Step 2 ‚Äî Create a route Go to the Routes section Click Create Configure the route: Method: POST Resource path: /chat Click Create to add the route.\nüîπ Step 3 ‚Äî Add Integration with Lambda Click the newly created /chat route In the Integration section, choose Attach integration Select Create and attach an integration, then configure: Integration type: Lambda function Region: The AWS region you are using Lambda function: lambda-bedrock-function (the Lambda you created earlier) Click Create.\nüéØ Expected Result You have now:\nCreated the /chat route Attached the route to your Lambda function Your API now has an endpoint ready for testing "
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.2-prerequisite/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Preparation Steps Before Starting Before building your AI API using Lambda + Bedrock, you need to configure several AWS resources and permissions.\n1. Choose an AWS Region that Supports Bedrock Amazon Bedrock is available in several AWS regions.\nFor this workshop, you may choose:\nus-east-1 (N. Virginia) ‚Äì commonly used in AWS documentation ap-southeast-1 (Singapore) ‚Äì also supports Bedrock and works normally Just make sure the model you want to use (Claude, Llama, Mistral‚Ä¶) is available in that region.\n2. Create an IAM Role for Lambda Lambda needs an IAM Role to call Bedrock models and write logs to CloudWatch.\nIn this section, you will first create a custom policy, then create a role and attach the policy.\nüîπ Step 1 ‚Äî Create a New Policy Open IAM Console ‚Üí Policies ‚Üí Create policy In the ‚ÄúSpecify permissions‚Äù page, select the JSON tab.\nDelete all existing content and replace it with the following: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34;, \u0026#34;bedrock:InvokeModelWithResponseStream\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next, give the policy a name (e.g., lambda-bedrock) and click Create policy. üîπ Step 2 ‚Äî Create the IAM Role and Attach the Policy Go to IAM Console ‚Üí Roles ‚Üí Create role Select: Trusted entity type: AWS service Use case: Lambda In ‚ÄúAdd permissions‚Äù, search for the policy you created earlier (lambda-bedrock) and select it. Name your role, for example: lambda-bedrock-role Then click Create role.\nYour Lambda execution role is now ready with the minimum permissions required to call Amazon Bedrock and write CloudWatch logs.\n3. Verify the Model Using Bedrock Playground Before writing Lambda code to call the Converse API, test the model in the AWS Console.\nüîπ Step 1 ‚Äî Open the Model Catalog Open Amazon Bedrock ‚Üí Model catalog üîπ Step 2 ‚Äî Choose a Model and Open Playground Pick a model such as Claude 3.5 Sonnet, Llama 3.1, or Mistral 24.07 Click Open in playground üîπ Step 3 ‚Äî Send a Test Prompt Enter a simple question to confirm the model is responding correctly in your selected region.\nüîπ Step 4 ‚Äî (Optional) Check if the Model Supports the Converse API If you want to confirm whether the selected model supports Converse API, follow these steps:\nReturn to the model page in the catalog Scroll down to Code examples AWS will open an example code page. If the model supports Converse API, the example will include:\nbedrock.converse(...) Like this:\n4. Recommended (Optional) Knowledge You may find it helpful to know the following:\nBasics of AWS Lambda How to create an HTTP API with API Gateway How to view logs in CloudWatch This workshop is beginner-friendly and does not require deep AWS experience.\nIn the next step, you will create a Lambda function and write your first code snippet to call the Converse API in Amazon Bedrock.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "FitAI Challenge An application that helps users lose weight through exercise challenges, integrated with AI for tracking and evaluation 1. Executive Summary FitAI Challenge is a website developed for Vietnamese users, aiming to promote fitness and exercise culture through sports challenges that incorporate gamification and artificial intelligence (AI). The website uses an AI Camera to recognize and count exercise movements such as push-ups, squats, planks, and jumping jacks, while also analyzing posture to provide accurate evaluations. Users can participate in individual challenges to earn FitPoints upon completing tasks, which can be redeemed for vouchers, gifts, or discounts from partner merchants. FitAI Challenge targets students, young adults, and working professionals ‚Äî individuals who need motivation to maintain regular workout habits amid their busy lives.\n2. Problem Statement What‚Äôs the Problem? In Vietnam, most existing fitness applications primarily focus on basic guidance or step counting, and there is currently no platform that combines AI-based motion recognition, gamification, and an online fitness challenge community. Users often lack motivation to exercise consistently and do not have tools that can accurately evaluate their workout performance. In addition, gyms and sports brands also lack creative engagement channels to connect with young and active customer groups.\nThe Solution FitAI Challenge uses an AI Camera to recognize, count, and evaluate the accuracy of workout movements through Computer Vision. All user workout data is stored and processed via AWS Cloud using a serverless architecture: AWS Lambda: processes AI data and backend requests. AWS S3: stores videos, images, and temporary results. The website is developed using React Native with a friendly and intuitive interface. Users can: Participate in individual, group, or nationwide challenges. Earn FitPoints upon completing exercises. Redeem FitPoints for vouchers or gifts from partners (Shopee, Grab, CGV, etc.). Track leaderboards and share achievements on social media.\nBenefits and Return on Investment For users: Create daily workout motivation through challenge and reward mechanisms. Receive transparent performance evaluations supported by AI. Connect with the fitness community through leaderboards and sharing feeds. For partner businesses: A branding channel associated with a healthy lifestyle. Access to a young, dynamic, and health-conscious customer base. For the development team: Establish a unique ‚ÄúFitness + Gamification + E-commerce‚Äù business model in Vietnam. Serverless cloud architecture helps reduce operating costs and allows easy scalability. The MVP can be developed within the first 3 months with low infrastructure costs (estimated at 0.80 USD/month on AWS).\n3. Solution Architecture FitAI Challenge is an intelligent sports training platform that applies an AWS Serverless architecture combined with an AI/ML pipeline. The system‚Äôs goal is to record workout data, analyze performance, and generate AI-powered feedback to provide personalized coaching for users. Data from the web application is sent to Amazon API Gateway, processed by AWS Lambda (Java), and stored in Amazon S3 along with the Docker Database.\nAWS Services Used Service Role Amazon Route 53 Manages domain names and routes traffic to CloudFront. AWS WAF Protects frontend and API layers from DDoS and OWASP attacks. Amazon CloudFront Delivers static content (web app built from Java web, HTML, CSS, JS). Amazon API Gateway Receives requests from the frontend and forwards them to Lambda functions. AWS Lambda (Java) Handles business logic (registration, login, data upload, scoring, AI pipeline). AWS Step Functions \u0026amp; SQS Coordinates workflows between Lambda and SageMaker/Bedrock. Amazon Cognito Authenticates users, manages login sessions, and controls access permissions. Amazon S3 Stores raw data, videos, images, and analysis results. Docker Runs the Java Spring Boot API backend and hosts the database (PostgreSQL or MongoDB). Amazon SageMaker Runs inference for computer vision/pose estimation models. Amazon Bedrock Generates natural language feedback, training suggestions, and summary reports. Amazon SES Sends authentication emails and user result notifications. Amazon CloudWatch Monitors logs, Lambda performance, costs, and system efficiency. IAM Manages access permissions and security across services. AWS CodePipeline / CodeBuild / CodeDeploy CI/CD pipeline for automating Java backend and Lambda deployment. Component Design Frontend Layer: The web app displays the user interface and connects to the API Gateway. The content is built and deployed on S3 + CloudFront. Users access the system through Route 53 ‚Üí WAF ‚Üí CloudFront ‚Üí API Gateway. Application Layer: The API Gateway receives requests from the frontend. Lambda (Java) executes business functions: AuthLambda: handles user login and authentication. UploadLambda: receives workout data, images, or videos. AIPipelineLambda: triggers the AI workflow (SageMaker + Bedrock). SaveResultLambda: stores training results and AI feedback.\n4. Technical Implementation Implementation Phases\nPhase Description Achieved Outcome 1. AWS Infrastructure Setup Deploy VPC, Private Subnets, Route 53, WAF, S3, API Gateway, Lambda, Cognito, RDS MySQL, VPC Endpoints (S3, Bedrock). Secure, isolated, and ready-to-use base infrastructure. 2. CI/CD Pipeline Set up CodeCommit + CodeBuild + CodeDeploy + CodePipeline for Java backend and Lambda; build \u0026amp; deploy web to S3/CloudFront. Automated deployment for backend and frontend. 3. Build Lambda Functions (Java) Create Lambdas for Auth, Upload, AI Pipeline, Save Result; connect to RDS MySQL and S3. Completed serverless backend. 4. AI Pipeline Integrate SQS, Step Functions, and Bedrock; build workflow: receive job ‚Üí analyze data ‚Üí score ‚Üí generate AI feedback. Smooth AI operation with automated user feedback. 5. Web App Deployment Build web app ‚Üí Deploy to S3 + CloudFront ‚Üí configure domain in Route 53. Stable online user interface. 6. Monitoring \u0026amp; Cost Optimization Use CloudWatch + Cost Explorer to track logs, performance, and costs; tune Lambda, RDS, and CloudFront configurations. Stable system with low cost and tight monitoring. 5. Timeline \u0026amp; Milestones Before internship (Month 0):\nDesign detailed architecture based on the new diagram.\nExperiment with a simple AI pipeline using Bedrock (text feedback).\nInternship (Months 1‚Äì3):\nMonth 1:\nSet up infrastructure: VPC, Subnets, RDS MySQL, Cognito, API Gateway, Lambda, S3, CloudFront.\nConfigure CI/CD (CodeCommit, CodeBuild, CodeDeploy, CodePipeline).\nMonth 2:\nDevelop Java backend and Lambda functions.\nBuild the AI pipeline with SQS, Step Functions, and Bedrock.\nMonth 3:\nIntegrate frontend with backend.\nRun performance tests, pilot with 10‚Äì20 users, prepare the final demo.\nPost-deployment:\nContinue optimizing the AI model and add deeper gamification over the next year.\n6. Budget Estimation You can view costs on the AWS Pricing Calculator, or download the attached budget estimate.\nInfrastructure Costs (MVP estimate)\nAWS Services: Amazon API Gateway: 0.38 USD / month (‚âà300 requests/month, 1 KB/request). Amazon Bedrock: 0.32 USD / month (1 req/min, 350 input tokens, 70 output tokens). Amazon CloudFront: 1.20 USD / month (5 GB transfer, 500,000 HTTPS requests). Amazon CloudWatch: 1.85 USD / month (5 metrics, 0.5 GB logs). Amazon Cognito: 0.00 USD / month (‚â§100 MAU). Amazon Route 53: 0.51 USD / month (1 hosted zone). Amazon S3: 0.04 USD / month (1 GB storage, 1,000 PUT/POST/LIST, 20,000 GET). Amazon SES: 0.30 USD / month (3,000 emails from EC2/Lambda). Amazon Simple Queue Service (SQS): ‚âà0.00 USD / month (0.005 million requests/month). AWS Lambda: 0.00 USD / month (‚âà300,000 requests/month, 512 MB ephemeral storage). AWS Step Functions: 0.00 USD / month (500 workflows, 5 state transitions/workflow). AWS Web Application Firewall (WAF): 6.12 USD / month (1 Web ACL, 1 rule). Amazon RDS MySQL (auto-stop mode): 0‚Äì3 USD / month (depending on runtime). Total: around 10.7 ‚Äì 12 USD / month depending on RDS usage; equivalent to 128 ‚Äì 144 USD / 12 months.\n7. Risk Assessment Risk Matrix\nTechnical: AI misidentifies movements or fails on image/video processing; misconfigured VPC/Endpoints cause service disruption. User: Users don‚Äôt maintain workout habits; low return rate. Market \u0026amp; Partners: Hard to expand reward partners and co-branding; partner policy changes. Cost: Unexpected cost increases when user volume spikes (CloudFront, Bedrock). Mitigation Strategies\nOptimize AI models with continuous training; monitor quality via logs; add basic validation before scoring. Deepen gamification (streak chains, friend groups, seasonal challenges, appealing rewards). Prepare clear value propositions; diversify partner types (sports, healthy food, entertainment). Set AWS Budgets + Alarms per service (CloudFront, Bedrock, Lambda, RDS). Contingency Plans\nIf AI fails ‚Üí use fallback logic (simple time/rep-based scoring) and clearly notify users. If user volume drops ‚Üí launch community challenges, pair with social media campaigns. If commercial partners withdraw ‚Üí maintain internal FitPoints with small rewards (in-app badges/vouchers) while finding new partners. 8. Expected Outcomes Technical Improvements:\nComplete the AI motion recognition system with \u0026gt;90% accuracy. Stable app that supports up to 10,000 concurrent active users on serverless architecture. Optimize architecture to keep infra cost around 10‚Äì12 USD/month in the early stage. Long-term Value:\nBuild a sustainable Vietnamese fitness community connected via online challenges. Become the pioneering ‚ÄúAI + Fitness + Gamification‚Äù platform in Vietnam. Establish a workout data foundation to expand into health analytics, personalized training programs, and future AI projects. "
},
{
	"uri": "http://localhost:1313/Report-AWS/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Networking \u0026amp; Content Delivery Amazon CloudFront Now Supports IPv6 Origins for End-to-End IPv6 Delivery\nWritten by Sagar Desarda and Ravi Avula on 04 SEP 2025 in Best Practices, Networking \u0026amp; Content Delivery, Technical How-To Permalink Share\nIPv6 adoption continues to accelerate worldwide as organizations move beyond the limitations of IPv4 address space. At Amazon Web Services (AWS), we have long supported IPv6 from end users to the Amazon CloudFront network, helping end users reduce latency, improve performance, and expand reach across modern mobile networks. Today, we are excited to take another step forward. Starting today, CloudFront now supports IPv6 connectivity from edge to origin‚Äîenabling a truly end-to-end IPv6 delivery path. This allows end users to use CloudFront as a dual-stack IPv6 and IPv4 Internet gateway for their web applications to provide content acceleration.\nWhy is this important?\nIPv6 is the foundational transport protocol for most modern mobile networks and an increasing share of broadband traffic. Enabling IPv6 to the origin allows you to maintain consistency of protocol across the delivery chain, reduce operational costs from dual-stack complexity, and achieve more deterministic, observable, and efficient traffic flows. For CloudFront end users, these benefits translate directly into faster page loads, more stable streaming, and an architecture that continues to operate reliably as IPv4 resources deplete.\nBenefits of IPv6 for CloudFront-backed applications\nCloudFront now supports origins over IPv6, enabling end-to-end IPv6 connectivity from end users to your origin servers. This unlocks a wide range of technical and operational advantages over traditional IPv4-based delivery.\nEliminates NAT costs and improves performance\nIPv4 networks rely heavily on Network Address Translation (NAT), especially carrier-grade NAT used by ISPs and mobile operators. These NAT layers introduce delays during connection establishment, limit port availability, and can cause packet drops. IPv6 removes the need for NAT, enabling direct end-to-end connectivity between clients, CloudFront, and origin servers. This results in lower latency, faster page loads, and better user experience‚Äîespecially in mobile-first markets with high IPv6 adoption.\nMore efficient packet processing\nIPv6 introduces simplified headers and fixed-length extension headers for optional control information. This makes packet parsing and forwarding more efficient for routers, firewalls, load balancers, and CloudFront nodes. IPv6 reduces per-packet processing overhead and eliminates ambiguity in forwarding or deep packet inspection systems. Unlike IPv4, which allows fragmentation along the routing path, IPv6 delegates all fragmentation responsibility to the source server. This architectural constraint improves transmission performance by reducing retransmissions and maintaining optimal segment sizes throughout the transport path. As a result, IPv6 enables more stable and efficient TCP connections, especially over long-haul or high-latency paths between CloudFront and origins.\nPredictable path control and congestion handling\nIPv6 enforces end-to-end Path MTU Discovery (PMTUD), fully delegating fragmentation responsibility to the source. This improves transmission predictability and minimizes risks of packet drops or fragmentation due to MTU mismatches. IPv6 improves TCP stability and throughput‚Äîespecially across long-haul or high-latency paths between CloudFront and non-AWS origins‚Äîby reducing retransmissions and maintaining optimal segment sizes end-to-end.\nFor AWS origins, similar benefits are achieved today thanks to AWS backbone features like jumbo frame support. Enabling jumbo frames between AWS edge locations and application endpoints in AWS Regions allows CloudFront to send and receive larger payloads per packet. Jumbo frames reduce the total time required to transmit data between end users and your application.\nHigher connection scalability\nUnder IPv4, NAT reduces the number of available source ports per origin IP address, limiting the number of concurrent connections a CloudFront node can establish with an origin. This constraint can become problematic in high-traffic environments where thousands of concurrent requests must be handled efficiently. This is especially beneficial when using protocols like HTTP/2, where multiplexing multiple streams over a single connection and reusing connections is critical for maximizing performance and minimizing latency.\nGetting Started Beginning today, you can configure origins associated with your CloudFront distribution to use IPv6. The new feature allows you to choose between IPv4 (default), IPv6, or Dualstack (IPv4 and IPv6). For existing origins, CloudFront continues to use IPv4. When using Dualstack, CloudFront automatically selects between IPv4 and IPv6 to ensure even load distribution across both.\nYou can use the CloudFront console or CloudFront API to create or update your distribution to configure IPv6 connectivity with your origin. In this post, we guide you through creating an IPv6-enabled origin and explore best practices for safely enabling IPv6 on existing origins. Before starting, ensure that your origin supports IPv6 or dual-stack connectivity. This may be a custom origin or an AWS service that supports IPv6, such as Elastic Load Balancers, Amazon API Gateway, or AWS Lambda Function URLs.\nCreating a new CloudFront distribution with an IPv6 origin In the CloudFront console, choose the option to create a CloudFront distribution.\nStep 1: Begin Enter the distribution name and any other optional parameters before selecting Next to proceed to Step 2.\nStep 2: Specify the origin In Step 2, select the origin type and enter origin information. To configure IPv6, choose custom origin settings in the Settings panel.\nSelect IPv6 or Dualstack for the Origin IP Address Type and choose Next.\nStep 3: Enable security You may choose to enable AWS WAF to protect your application and select Next.\nStep 4: Review and create Review and select Create distribution to create the distribution.\nAdding a new IPv6 origin to an existing CloudFront distribution To add a new IPv6 origin to an existing CloudFront distribution, open the Distribution settings by selecting the distribution and choosing the Origins tab to Create origin.\nExpand the Additional settings panel and choose the IPv6 or Dualstack option for the Origin IP Address Type to enable IPv6 connectivity. When you create the origin, add or update cache behaviors to point to your new origin.\nEnabling IPv6 for an existing origin You can use CloudFront continuous deployment to safely roll out changes to origin settings. Continuous deployment allows you to test changes safely using a deployment policy to route requests to a staging distribution, validate the changes, and then promote them. For details, refer to the CloudFront documentation.\nValidating IPv6 connectivity to the origin Use metrics or application logs to validate IPv6 traffic at the origin. In this example, we used an Application Load Balancer (ALB) as the origin and validated using the IPv6 Request Count metric.\nConclusion As IPv6 adoption continues to increase across mobile and global networks, enabling end-to-end IPv6‚Äîfrom end users to Amazon CloudFront to your origin‚Äîunlocks performance and architectural advantages that IPv4 cannot match. It eliminates NAT costs, improves routing visibility and flow control, and streamlines packet processing through fixed headers and reliable Path MTU Discovery. While CloudFront optimizes for both IPv4 and IPv6, the benefits of IPv6 are most pronounced in the first and last mile of content delivery. Adopting end-to-end IPv6 lays the foundation for scalable, high-performance, future-ready content delivery.\nEnabling full IPv6 support on Amazon CloudFront is no longer optional‚Äîit is a fundamental step toward unlocking lower latency, higher resiliency, and future scalability. If you haven‚Äôt already, enable IPv6 support in your CloudFront distribution today.\nSagar Desarda\nSagar Desarda is the Head of Technical Account Management (TAM) and Business Development (BD) for Data, Analytics, and GenAI ISVs. His teams collaborate with customers to optimize their AWS architectures, ensure smooth operation of business-critical applications, accelerate adoption, and drive go-to-market success across North America. Sagar is also the AMER lead for the Edge Services Specialist group, where he drives new business growth, oversees technical engagements, and authors customer-facing publications.\nRavi Avula\nRavi is a Senior Solutions Architect at AWS focused on Enterprise Architecture. He has 20 years of software engineering experience and has held multiple leadership roles in software engineering and software architecture within the payments industry.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.3-create-lambda-and-bedrock-call/",
	"title": "Create Lambda and Call Bedrock",
	"tags": [],
	"description": "",
	"content": "Create Lambda and Call Bedrock In this section, you will create a Lambda function and configure it to call an Amazon Bedrock model using the Converse API.\nThe overall architecture was introduced earlier, and this step focuses on implementing the components needed for Lambda to send a prompt to Bedrock and receive a response.\nIn the following steps, you will:\nCreate a new Lambda function Assign the Execution Role you prepared in the Prerequisites section Write the initial code to call a model using the Converse API After completing this section, your Lambda function will be able to interact directly with Bedrock and handle AI Question‚ÄìAnswer requests.\nContents Create Lambda Function Assign IAM Role to Lambda Add Code to Call Bedrock via Converse API "
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.3-test-lambda/",
	"title": "Test Lambda Function",
	"tags": [],
	"description": "",
	"content": "Test Lambda Function After creating the Lambda Function and adding the code to call Amazon Bedrock using the Converse API, the next step is to test the function to ensure everything works correctly.\nIn this section, you will:\nCreate a test event in the Lambda Console Send a sample question to the Bedrock model Review the response and check logs if any errors occur üîπ Step 1 ‚Äî Open Lambda and Create a Test Event Go to AWS Lambda Console Select the function: bedrock-chatbot-lambda Click the Test button Choose Create new event if this is your first test Enter an event name, for example: Event name: test-bedrock-converse Scroll down to the JSON editor and replace the entire content with:\n{ \u0026#34;question\u0026#34;: \u0026#34;What is Amazon Bedrock?\u0026#34; } Example illustration:\nClick Save to store the test event.\nüîπ Step 2 ‚Äî Run the Test and Review the Output Select the event you just created Click Test If everything is working correctly, Lambda should return a response similar to:\n{ \u0026#34;answer\u0026#34;: \u0026#34;Amazon Bedrock is a fully managed service...\u0026#34; } The \u0026quot;answer\u0026quot; field contains the model‚Äôs generated response.\nüîπ Step 3 ‚Äî Check CloudWatch Logs if Errors Occur If the Lambda function throws an error or behaves unexpectedly:\nOpen the Monitor tab Click View logs in CloudWatch Open the most recent log stream to inspect the execution details Common issues and fixes:\n‚ùå AccessDeniedException Cause: Lambda Execution Role lacks bedrock:InvokeModel Fix: Re-check the IAM Role created in the Prerequisites section ‚ùå Timeout Cause: Default Lambda timeout (3 seconds) is too short Fix: Go to Configuration ‚Üí General and increase timeout to 10‚Äì20 seconds ‚ùå KeyError or missing question field Cause: Payload is missing the \u0026quot;question\u0026quot; key Fix: Ensure the test event JSON is formatted like: { \u0026#34;question\u0026#34;: \u0026#34;Your question here\u0026#34; } üéØ Expected Outcome After completing this step, you will:\nConfirm that Lambda successfully invokes Bedrock Receive responses from the model via the Converse API Verify that your IAM Role and MODEL_ID are correctly configured Be ready to move on to building the API endpoint "
},
{
	"uri": "http://localhost:1313/Report-AWS/3-blogstranslated/",
	"title": "Translated Blog Posts",
	"tags": [],
	"description": "",
	"content": "Here is the list and brief introduction of the AWS blog posts translated by the team:\nBlog 1 ‚Äì Journey from Statistics Enthusiast to NFL Analytics Pioneer: Mike Band This blog recounts the journey of Mike Band, who turned his childhood passion for sports statistics into a groundbreaking data analytics career at NFL Next Gen Stats. The article describes his development through education and practical experience, before contributing to building advanced AI models that helped change how the NFL analyzes and approaches game data. Alongside his technical achievements, Band also shares valuable advice for the next generation of engineers and analysts‚Äîstressing curiosity, a proactive spirit, and continuous learning as keys to long-term success.\nBlog 2 ‚Äì Agentic AI Assistant for SAP with AWS Generative AI This blog, Part 1 of a two-part series, details how to build a smart AI assistant (Agentic AI) using Amazon Bedrock to seamlessly interact with SAP systems. The article illustrates how to integrate Generative AI to allow users to query order data (from SAP) and logistics information (from DynamoDB) using natural language through common interfaces like Microsoft Teams, Slack, and Streamlit. This solution democratizes access to enterprise data without requiring deep technical knowledge of SQL or SAP structures.\nBlog 3 ‚Äì Amazon CloudFront Supports IPv6 Origins for End-to-End IPv6 Delivery This blog announces that Amazon CloudFront now supports IPv6 connectivity from the edge to the origin, enabling comprehensive end-to-end IPv6 delivery. The article explains crucial technical benefits such as eliminating NAT overhead, enhancing packet processing efficiency, and improving scalability for modern mobile networks. The authors also provide detailed instructions on how to configure a CloudFront distribution to enable the IPv6 or Dual-stack feature for the origin.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.4-api-gateway-integration/",
	"title": "Create API Gateway",
	"tags": [],
	"description": "",
	"content": "Create API Gateway In this section, you will create an HTTP API on Amazon API Gateway so that clients can send questions to your Lambda function.\nAPI Gateway will act as the HTTP endpoint intermediary between the client and your AI Q\u0026amp;A service.\nContents 5.4.1 ‚Äì Create HTTP API 5.4.2 ‚Äì Integrate API with Lambda After completing this section, you will have a public (or private, depending on configuration) HTTP endpoint that allows clients such as Postman, cURL, or web frontends to send questions to Lambda and receive responses from Bedrock.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Viet Nam Cloud Day 2025\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nPreview: A flagship day diving into GenAI strategy, unified data platforms, enterprise‚Äëscale migration/modernization, and security‚Äëby‚Äëdesign. Highlights included AI across the SDLC, zero‚Äëtrust practices, AI agents for productivity, and practical stories from banks and startups.\nEvent 2 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nPreview: A practical workshop on AWS AI/ML (SageMaker) and GenAI on Bedrock. Walked through the ML lifecycle, MLOps automation, Prompt Engineering, RAG, Bedrock Agents, Guardrails, and live demos building a context‚Äëaware chatbot.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.5-testing-and-logs/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": "Test the API with Thunder Client (VS Code) After integrating API Gateway with Lambda, you have an HTTP endpoint ready for testing.\nIn this step, you‚Äôll use Thunder Client ‚Äî a popular VS Code extension ‚Äî to send requests and inspect responses.\nüîπ Step 1 ‚Äî Get the Invoke URL from API Gateway In AWS API Gateway:\nOpen the API Gateway service. Select the API you created, e.g., bedrock-chatbot-api. In the left menu, choose Deploy ‚Üí Stages. Click the $default stage. In Stage details, copy the Invoke URL. Copy the Invoke URL, for example:\nhttps://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com Next, append the route path you configured, e.g., /chat.\nüëâ The full endpoint will be:\nhttps://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com/chat üîπ Step 2 ‚Äî Install and open Thunder Client in VS Code Open VS Code. Go to the Extensions tab. Search for Thunder Client and click Install. After installation, the Thunder Client icon appears in the sidebar. Click the icon and choose New Request. Select the POST method. Paste the endpoint into the URL box: https://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com/chat üîπ Step 3 ‚Äî Send JSON body and check the response Choose the Body ‚Üí JSON tab. Enter: { \u0026#34;question\u0026#34;: \u0026#34;Amazon Bedrock l√† g√¨?\u0026#34; } Click Send to issue the request.\nIf everything is wired correctly, you should get a response like:\n{ \u0026#34;answer\u0026#34;: \u0026#34;Amazon Bedrock is a fully managed service...\u0026#34; } This confirms:\nAPI Gateway received the request successfully Lambda ran correctly and called Bedrock The system returned the expected result üîß Troubleshooting 403 / AccessDeniedException ‚Üí Check the Lambda IAM role 500 Internal Error ‚Üí Inspect CloudWatch Logs Missing \u0026lsquo;question\u0026rsquo; field ‚Üí Validate the JSON body Timeout ‚Üí Increase Lambda timeout to 10‚Äì20 seconds ‚úî Conclusion You‚Äôve successfully tested the end‚Äëto‚Äëend pipeline:\nClient ‚Üí API Gateway ‚Üí Lambda ‚Üí Bedrock ‚Üí AI Response\nYou‚Äôve completed the testing portion of the workshop.\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Build a Simple AI API with AWS Lambda + Bedrock + API Gateway Overview Amazon Bedrock is a fully managed service that provides access to leading large language models (LLMs) such as Claude, Llama, Mistral, and Titan.\nYou can integrate AI features into your applications using simple API calls without managing infrastructure or hosting models yourself.\nIn this workshop, you will build a simple AI Q\u0026amp;A API using:\nAWS Lambda ‚Äì handles incoming requests and calls Bedrock Amazon Bedrock Runtime ‚Äì sends prompts and receives model responses Amazon API Gateway ‚Äì exposes an HTTP endpoint for client requests A key aspect of this workshop is the use of the Converse API ‚Äî a unified interface for Bedrock models that support the Converse capability (e.g., Claude 3, Claude 3.5, Llama 3.1, Mistral 24.07‚Ä¶).\nWith the Converse API:\nYou can switch models by simply changing the modelId in Lambda There is no need to rewrite conversation-handling logic You can easily test or compare multiple models with the same API flow Note: Only models that support Converse API can be used with the code in this workshop.\nWorkshop Content Introduction Prerequisites Lambda Calls Bedrock Create API Gateway Testing Cleanup "
},
{
	"uri": "http://localhost:1313/Report-AWS/5-workshop/5.6-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Cleanup Resources After completing the workshop, you should delete the AWS resources you no longer need to avoid unnecessary costs.\nBelow is a list of the services you created and how to remove them.\nüîπ 1. Delete the API Gateway Open the API Gateway Console Select the API you created, e.g., bedrock-chatbot-api Choose Actions ‚Üí Delete Confirm deletion This prevents any further requests from reaching Lambda and avoids API Gateway charges.\nüîπ 2. Delete the Lambda Function Open the Lambda Console Select the function bedrock-chatbot-lambda Choose Actions ‚Üí Delete function Confirm deletion üîπ 3. Delete the IAM Role and Policy Delete the Policy: Open IAM Console ‚Üí Policies Search for lambda-bedrock Click Delete Delete the Role: Open IAM Console ‚Üí Roles Search for lambda-bedrock-role Click Delete ‚ö†Ô∏è Note: You can only delete the role after deleting the Lambda function that uses it.\nüîπ 4. Check CloudWatch Log Groups (Optional) Lambda logs remain in CloudWatch and may accumulate storage charges over time.\nOpen the CloudWatch Console Select Logs ‚Üí Log groups Find your Lambda log group (e.g., /aws/lambda/bedrock-chatbot-lambda) Choose Actions ‚Üí Delete log group üîπ 5. Review Other Resources (If Applicable) Depending on how you expanded the workshop, you may have created additional resources such as:\nS3 buckets Step Functions KMS keys VPC / Security Groups If they are no longer needed, delete them to prevent charges.\nüéâ All Done! You have now cleaned up all resources created during this workshop.\nYour AWS account will no longer incur costs from the lab environment.\nThank you for participating in the workshop!\n"
},
{
	"uri": "http://localhost:1313/Report-AWS/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd. from August 12, 2025 to December 9, 2025, I had the opportunity to learn, practice, and apply the knowledge I had acquired at university in a real working environment.\nI participated in the FitAI Challenge project, through which I developed various skills such as self-directed research, requirement analysis, report writing, team communication, problem-solving, and applying new technologies.\nIn terms of work ethic, I consistently strived to complete assigned tasks, adhere to company regulations, and actively collaborate with colleagues to improve work efficiency.\nTo provide an objective reflection of my internship process, I have evaluated myself based on the criteria below:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, ability to apply knowledge, tool proficiency, work quality ‚úÖ ‚òê ‚òê 2 Learning ability Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Willingness to explore and take on tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adherence to schedules, regulations, and work processes ‚òê ‚úÖ ‚òê 6 Willingness to improve Openness to feedback and commitment to self-improvement ‚úÖ ‚òê ‚òê 7 Communication Ability to present ideas clearly and report work effectively ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and contributing to team activities ‚úÖ ‚òê ‚òê 9 Professional conduct Respect for colleagues, partners, and the working environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Ability to identify issues, propose solutions, and think creatively ‚òê ‚úÖ ‚òê 11 Contribution to the project/team Work efficiency, improvement ideas, and recognition from the team ‚úÖ ‚òê ‚òê 12 Overall performance General evaluation of the entire internship process ‚òê ‚úÖ ‚òê Needs Improvement Although I have followed company regulations and workflows well, there is still room to improve discipline**, especially in time management, task planning, and maintaining consistent progress. Continued development of analytical and problem-solving skills is needed, including exploring multiple solution approaches and identifying the root causes of issues to determine the most suitable resolution. Communication skills can be further improved by presenting problems more clearly, defining goals thoroughly from the beginning, and proactively discussing challenges to avoid delays. Strengthening the ability to organize and systematize knowledge would support more effective learning and faster adaptation to new technologies. "
},
{
	"uri": "http://localhost:1313/Report-AWS/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment was new, professional, and provided me with opportunities to observe real-world workflows. Being involved in an actual project helped me better understand how a team operates in a corporate setting. However, as a student without prior cloud background, the initial adaptation phase was quite challenging.\n2. Support from Mentor / Admin Team\nThe mentor was supportive and explained concepts clearly whenever I encountered difficulties. However, since I was completely new to cloud technologies, I would have benefited from additional hands-on guidance, especially in the early stages, to reduce confusion and provide clearer direction.\n3. Alignment Between Tasks and Academic Background\nMy major is Artificial Intelligence, so I had little exposure to cloud computing before joining the program. Some parts of the workload and technical content felt advanced compared to my academic foundation. Adjusting the curriculum to better match each student‚Äôs background would improve the overall learning experience.\n4. Learning Opportunities \u0026amp; Skill Development\nI gained new knowledge and had the opportunity to participate in a real project. However, for interns who are just starting, having more direct instructions or clearer initial orientation would make the learning process smoother and more efficient.\n5. Culture \u0026amp; Team Spirit\nThe team demonstrated strong collaboration and was supportive throughout the program. This positive working culture helped me integrate quickly, even though I was new to cloud technologies.\n6. Policies / Benefits for Interns\nThe program provided adequate learning resources and created favorable conditions for skill development. However, communication channels could be organized more clearly to ensure important updates are not missed.\nAdditional Questions ‚Ä¢ What I was most satisfied with:\nExperiencing a new working environment and being able to contribute to a real project from the beginning.\n‚Ä¢ What the company should improve for future interns:\nProvide more structured and accessible learning content, especially for students unfamiliar with cloud technologies.\n‚Ä¢ Would I recommend this internship to friends:\nYes ‚Äî if the opportunity aligns with their field of study and professional goals.\nSuggestions \u0026amp; Expectations Increase the number of hands-on guidance sessions to help new interns better understand their tasks and reduce confusion during the onboarding phase. Structure communication channels more clearly, such as: One group for general discussions and Q\u0026amp;A One dedicated group for important announcements from mentors Adapt the training content to better fit each academic major, allowing interns to learn and apply knowledge more effectively. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 10 Objectives: Amazon RDS ‚Äì Advanced Operations\nReview RDS architecture: Multi-AZ, Read Replica, backup lifecycle. Understand Parameter Groups and Option Groups. Optimize Lambda ‚Üí RDS connections. Analyze database performance with Performance Insights. Perform backup, restore, manual snapshot management. Amazon S3 ‚Äì Backend Storage Integration\nUnderstand buckets, objects, permissions. Configure secure Bucket Policies \u0026amp; IAM roles. Upload/download files via Lambda. Implement Lifecycle rules for cost optimization. Amazon Bedrock ‚Äì VPC Interface Endpoint\nCreate Interface Endpoint inside private subnets. Configure SG and routing for private access. Invoke Bedrock models (Claude, Titan‚Ä¶) from Lambda. Lambda Integration ‚Äì Full Backend Workflow\nLambda performs database queries on RDS. Lambda stores and reads data from S3. Lambda invokes Bedrock for AI processing. Monitor execution using CloudWatch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 RDS Advanced Operations ‚Ä¢ Review Parameter/Option Groups. ‚Ä¢ Analyze Performance Insights. ‚Ä¢ Create snapshots \u0026amp; restore. 10/11/2025 10/11/2025 cloudjourney.awsstudygroup.com 3 Lambda ‚Üí RDS Integration ‚Ä¢ Configure IAM Role. ‚Ä¢ Connect Lambda to RDS in private subnet. ‚Ä¢ Implement connection pooling. ‚Ä¢ Debug timeout issues. 11/11/2025 11/11/2025 cloudjourney.awsstudygroup.com 4 S3 Integration ‚Ä¢ Create bucket. ‚Ä¢ Configure policies. ‚Ä¢ Upload/download from Lambda. ‚Ä¢ Apply Lifecycle rules. 12/11/2025 12/11/2025 cloudjourney.awsstudygroup.com 5 Create Bedrock Interface Endpoint ‚Ä¢ Deploy endpoint in private subnet. ‚Ä¢ Configure security groups. ‚Ä¢ Test Bedrock inference from Lambda. 13/11/2025 13/11/2025 cloudjourney.awsstudygroup.com 6 AI-Driven Lambda API ‚Ä¢ Query RDS. ‚Ä¢ Store output in S3. ‚Ä¢ Invoke Bedrock model. ‚Ä¢ Monitor logs in CloudWatch. 14/11/2025 14/11/2025 cloudjourney.awsstudygroup.com 7 Cleanup \u0026amp; Cost Optimization ‚Ä¢ Delete snapshots and test DB. ‚Ä¢ Clean S3 bucket. ‚Ä¢ Review Endpoint/RDS/S3 costs. 15/11/2025 15/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 10: 1. Mastered advanced RDS operations Understood Parameter Groups and Option Groups. Applied performance troubleshooting via Performance Insights. Managed backup, restore, and snapshots efficiently. 2. Successfully integrated S3 with backend workflows Secure bucket configuration using IAM best practices. Lambda can upload, read, and manage data objects. Lifecycle rules deployed to reduce storage cost. 3. Built a secure Bedrock Endpoint for private AI inference Lambda can call Bedrock models inside VPC without Internet. Understood use cases for private model inference. 4. Developed AI-enhanced backend API Combined RDS + S3 + Bedrock in a single Lambda workflow. Processed structured + unstructured data. Built CloudWatch monitoring for debugging and optimization. Summary of Knowledge Gained: Strong knowledge of advanced RDS features and optimization. Hands-on skills integrating Lambda with RDS and S3. Experience deploying Bedrock Endpoint for private AI workloads. Ability to build multi-service backend workflows using AWS. Understanding of cost optimization across RDS, S3, Lambda, and VPC networking. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 11 Objectives: Part 1 ‚Äì Front-end Serverless Deployment (S3 + CloudFront)\nUnderstand static website hosting architecture on AWS. Create an S3 bucket to store the frontend build. Configure CloudFront as a global CDN. Enable HTTPS using ACM certificates. Implement Origin Access Control (OAC) for secure S3 access. Block all public access and apply secure bucket policies. Part 2 ‚Äì Integrating Cognito Authentication into the Frontend\nReuse User Pool \u0026amp; App Client configured in Week 8. Build login/signup UI or use Hosted UI. Retrieve ID/Access Tokens after authentication. Store tokens securely (localStorage/sessionStorage). Implement logout and token expiration handling. Part 3 ‚Äì Calling API Gateway from the Frontend\nSend Authorization header: Bearer \u0026lt;JWT\u0026gt;. Test protected API routes using Cognito Authorizer. Handle 401 (Unauthorized) and 403 (Forbidden). Render API responses on the UI. Part 4 ‚Äì Monitoring \u0026amp; Debugging\nEnable and review CloudFront Access Logs. Check API Gateway logs via CloudWatch. Debug Lambda flow end-to-end. Clean up unused resources to reduce costs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Deploy Frontend to S3 ‚Ä¢ Create bucket. ‚Ä¢ Upload build files. ‚Ä¢ Configure index/error pages. 17/11/2025 17/11/2025 cloudjourney.awsstudygroup.com 3 Create CloudFront Distribution ‚Ä¢ Add S3 origin. ‚Ä¢ Configure HTTPS with ACM. ‚Ä¢ Enable OAC + update bucket policy. 18/11/2025 18/11/2025 cloudjourney.awsstudygroup.com 4 Integrate Cognito into Frontend ‚Ä¢ Retrieve JWT tokens from Week 8 User Pool. ‚Ä¢ Store tokens securely. ‚Ä¢ Build login/logout UI. 19/11/2025 19/11/2025 cloudjourney.awsstudygroup.com 5 Frontend ‚Üí API Gateway Integration ‚Ä¢ Send Authorization header. ‚Ä¢ Test protected endpoints. ‚Ä¢ Handle 401/403. 20/11/2025 20/11/2025 cloudjourney.awsstudygroup.com 6 Monitoring \u0026amp; Debugging ‚Ä¢ Review CloudFront logs. ‚Ä¢ Debug API Gateway. ‚Ä¢ Check Lambda logs. 21/11/2025 21/11/2025 cloudjourney.awsstudygroup.com 7 Cleanup Resources ‚Ä¢ Delete test CloudFront. ‚Ä¢ Delete test S3 bucket. ‚Ä¢ Review Billing. 22/11/2025 22/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 11: 1. Fully deployed serverless frontend Frontend hosted on S3 + distributed globally via CloudFront. HTTPS enabled and S3 protected using OAC. Fast and secure static web delivery. 2. Cognito Authentication integrated successfully Reused Week 8 User Pool. JWT retrieval and storage working correctly. Login/logout features implemented with token handling. 3. Secure API Gateway calls from the frontend JWT-based Authorization header validated by API Gateway. Smooth flow from Frontend ‚Üí API ‚Üí Lambda backend. Clear error handling for authentication failures. 4. Effective debugging and monitoring CloudFront logs used to trace user requests. API Gateway + Lambda logs analyzed through CloudWatch. Reduced cost by cleaning unused test resources. Summary of Knowledge Gained: Hosting serverless frontend via S3 + CloudFront. Practical Cognito Authentication integration without Amplify. Making secure API calls with JWT tokens. Debugging across CloudFront, API Gateway, and Lambda. Understanding full serverless stack behavior on AWS. "
},
{
	"uri": "http://localhost:1313/Report-AWS/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 12 Objectives: Part 1 ‚Äì Building AI API using Amazon Bedrock\nUnderstand Bedrock architecture, pricing, throttling. Invoke Claude/Titan models from Lambda via Interface Endpoint. Apply prompt engineering for better model performance. Create API Gateway endpoint for external AI calls. Handle errors such as timeouts and throttling. Part 2 ‚Äì Building automated workflows using Step Functions\nDesign multi-step workflows:\nLambda ‚Üí Bedrock ‚Üí Data Storage ‚Üí SES Email. Use Task, Choice, Wait, Retry, and Catch states. Connect Lambda tasks to create an AI pipeline. Log execution history and debug errors. Part 3 ‚Äì Email notifications using Amazon SES\nVerify email identities. Build Lambda function to send emails via SES. Integrate SES into Step Functions workflow. Handle bounce and complaint notifications. Part 4 ‚Äì Workflow Monitoring\nMonitor Bedrock and Lambda logs with CloudWatch. Review Step Functions execution history. Track workflow metric success/failure. Optimize Bedrock cost (max tokens, batching, etc.). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Bedrock model invocation ‚Ä¢ Call Bedrock from Lambda. ‚Ä¢ Improve prompt. ‚Ä¢ Debug endpoint issues. 24/11/2025 24/11/2025 cloudjourney.awsstudygroup.com 3 AI API Development ‚Ä¢ Create /ai/generate endpoint. ‚Ä¢ Return model response. ‚Ä¢ Validate input. 25/11/2025 25/11/2025 cloudjourney.awsstudygroup.com 4 Step Functions Workflow ‚Ä¢ Build State Machine. ‚Ä¢ Lambda ‚Üí Bedrock ‚Üí Store Output. ‚Ä¢ Add error handling. 26/11/2025 26/11/2025 cloudjourney.awsstudygroup.com 5 SES Integration ‚Ä¢ Verify sender identity. ‚Ä¢ Lambda send email. ‚Ä¢ Add SES to workflow. 27/11/2025 27/11/2025 cloudjourney.awsstudygroup.com 6 Monitoring \u0026amp; Optimization ‚Ä¢ Review logs. ‚Ä¢ Analyze Step Functions map. ‚Ä¢ Optimize cost. 28/11/2025 28/11/2025 cloudjourney.awsstudygroup.com 7 Cleanup \u0026amp; Final Review ‚Ä¢ Remove test resources. ‚Ä¢ Review billing. ‚Ä¢ Validate full workflow. 29/11/2025 29/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 12: 1. Successfully built a Bedrock-powered AI API Lambda in private subnet calls Bedrock models. Stable API responses via API Gateway. Effective prompt engineering implemented. 2. Completed AI workflow using Step Functions Multi-step automation pipeline works end-to-end. Error handling \u0026amp; retries implemented properly. Clean execution flow: Input ‚Üí AI ‚Üí Save ‚Üí Notify. 3. SES email notifications integrated AI summary sent automatically after workflow. Email templates working. Improved automation experience. 4. Full monitoring and debugging implemented CloudWatch logs for Lambda, Bedrock, API Gateway. Detailed Step Functions Execution Map. Cost and performance optimization. Summary of Knowledge Gained: Practical usage of Amazon Bedrock for AI inference. Building serverless AI APIs using Lambda + API Gateway. Workflow orchestration using Step Functions. Email automation with Amazon SES. Complete understanding of production-level AI agent architecture. "
},
{
	"uri": "http://localhost:1313/Report-AWS/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/Report-AWS/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]