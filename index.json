[
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Building Resilience by Design: Developing Effective Recovery Strategies Against Ransomware Ransomware incidents have become a priority topic in leadership discussions across modern organizations. Data shows a clear trend: the number of ransomware events has more than doubled since the beginning of the pandemic, with the financial services sector being one of the most heavily targeted. At AWS, our cross-industry collaboration with global financial institutions, regulators, operational bodies, and ecosystem partners has resulted in a certified architecture for the Cloud Hosted Data Vault (CHDV)‚Äîcommonly referred to simply as a vault.\nA vault is a critical component in strengthening operational resilience against large-scale cyber incidents. It acts as a fully isolated final line of defense, protecting the most mission-critical digital assets of an organization. It allows the business to recover even when traditional mechanisms such as High Availability (HA), Business Continuity (BC), Disaster Recovery (DR), or backups can no longer guarantee recovery. Integrating a vault into your existing resilience practices requires careful planning across multiple dimensions. In this article, we explore the key considerations for planning a vault, how it enhances existing HA, BC/DR, and backup strategies, and the people, process, and operational elements required for an effective cyber vaulting solution.\n1. What Should Go Into the Vault? Every business unit owner might instinctively say that ‚Äúeverything, everywhere‚Äù needs protection. In reality, that‚Äôs not true ‚Äî at least not immediately. During a large-scale cyber incident, it is easy to fall into the mindset that every dataset, application, and service is mission-critical and must be vaulted. While it is tempting to assume that anything running in the production environment is essential, attempting to vault everything without an initial assessment can lead to:\nmassive vault sizes excessive costs recovery delays During a real cyber crisis, you must ask:\nWhich core IT functions and operational services are necessary to restart the business within the next 12, 24, or 48 hours ‚Äî and which ones are not?\nThese core services are called the Minimum Viable Business (MVB) ‚Äî the minimum foundation required for business survival. They include not only customer-facing services but also dependencies across second-, third-, and fourth-party ecosystems.\nFigure 1: What belongs in the vault? ‚Äì Focus on core IT functions and essential operational services.\nIdentifying the MVB cannot be done in isolation. A vault is not just a technical solution ‚Äî it requires collaboration across:\nIT Security Legal Business unit owners Application owners Senior leadership 2. What Will Recovery Look Like? Modern cyberattacks are intentionally designed to make system or service recovery as difficult‚Äîor nearly impossible‚Äîas possible. They aim to break traditional recovery mechanisms such as HA, BC, DR, and Backup.\nBecause attackers want maximum disruption with minimal recoverability, organizations must prepare for several critical elements:\nDecision Time Objective (DTO) How long does it take to decide to initiate recovery from the vault?\nIn a cyber incident, automated recovery workflows may no longer be trustworthy because adversaries may have tampered with them. Therefore, human decision-making becomes essential.\nCyber-Recovery Time Objective (C-RTO) \u0026amp; Cyber-Recovery Point Objective (C-RPO) Recovery timelines must be recalibrated.\nProcesses that normally take seconds may take days during a cyber event.\nMinimum Acceptable Service Offering (MASO) MASO asks:\n‚ÄúWhat is the minimum level of service we can still offer customers and third-party partners during recovery?‚Äù\nThis may involve:\nlimited functionality of the primary service a temporary fallback system alternative access mechanisms Figure 2: What recovery looks like ‚Äì What normally takes seconds may take days during a cyber crisis.\n3. How Will the Vault Be Segmented? Whether you use a single enterprise-wide vault or multiple vaults per service, the goal is always to balance:\nmanageability practicality security Core vault-design principles include:\n‚úî Simplicity The vault must be easy to understand and operate.\nComplexity delays recovery.\n‚úî Independence Each vault should function autonomously, enabling parallel recovery.\n‚úî Service mapping Determine where recovery begins and the optimal restoration sequence.\n‚úî Roles \u0026amp; responsibilities Clearly define ownership for:\napplications infrastructure recovery workflows ‚úî Maintenance A vault is not static: applications and business needs evolve, and so must the vault.\nFigure 3: How the vault is segmented ‚Äì Balancing manageability, content, and recovery workflows.\nAWS provides the flexibility to experiment, fail fast, and iterate without building up technical debt. What seems perfect on paper may fail in testing ‚Äî and those lessons shape the next vault design.\n4. How Should the Vault Be Managed? A vault must be isolated from the operational plane so that any expanding cyber incident in production cannot cross into the vault. Without this separation, the vault itself might be compromised.\nTraditional air-gapped solutions rely on physical isolation (e.g., unplugging cables, removing media), creating a literal gap.\nIn the cloud, equivalent controls are implemented using:\nAccess zones: ephemeral environments with limited access windows Strong MFA: hardware security tokens for vault operators Zero Trust: authenticate every step; trust nothing by default IAM controls: enforce strict least-privilege boundaries Change management: access only via formal approval workflows Figure 4: How the vault is managed ‚Äì Intentional isolation while preserving observability.\n5. How Should You Plan for Logistics and Service Providers? Physical considerations are often overlooked when planning for cyber recovery. When focusing heavily on core applications and infrastructure, organizations frequently neglect external dependencies that are critical to restoration.\nKey examples include:\n‚Ä¢ Internal and external network access Even minor connectivity disruptions can halt recovery.\n‚Ä¢ Software repositories Despite automation, organizations should maintain physical media (e.g., secure USB drives) for emergency distribution.\n‚Ä¢ Supply chain resilience Replace destroyed hardware through pre-planned logistics:\nsourcing transport storage deployment ‚Ä¢ Physical logistics Where will teams work during a large-scale recovery?\nIs there backup equipment and workspace available?\nFigure 5: Logistics and service providers ‚Äì Extending planning to external dependencies.\nSophisticated ransomware attacks often create multiple minor failures simultaneously. Individually, these failures may seem insignificant ‚Äî but together, they can create cascading disruptions that dramatically slow recovery.\n6. Who Is Responsible for the Vaulting Process? People are ultimately responsible for managing and operating the cyber vaulting process. They develop best practices and serve as the first responders during recovery. Choosing the right team is crucial ‚Äî and not always simple.\nVaulting principles span protection, planning, and operations.\nTherefore, effective vault governance requires collaboration across multiple stakeholders, not just a single technical group.\n‚Ä¢ Business stakeholders They ‚Äî not the backup team ‚Äî should determine recovery prioritization.\nThis is based on their knowledge of:\nsystem dependencies regulatory obligations business impact Clear communication between technical and business teams is essential for an effective cyber recovery strategy.\nFigure 6: People and processes ‚Äì Balancing both is fundamental for good practices.\nAs a result of cross-functional collaboration, organizations form detailed recovery plans that must be followed. However, this can inadvertently create recovery scenarios that, while aligned with policy, are not operationally realistic.\nOverly complex procedures can:\nslow recovery cause misalignment encourage operators to take shortcuts Although these behaviors may not immediately compromise security, the worst time to discover gaps in your vaulting process is during an actual cyber incident.\nFor effective recovery, organizations must embed good practices into daily operations. This requires a balance between:\nrobust cyber recovery requirements practical and sustainable operational workflows 7. Why Is Executive Sponsorship Necessary? Any additional operational burden increases cost, effort, and complexity. Naturally, executives ‚Äî especially CFOs ‚Äî question why a solution that does not generate revenue should consume time and budget.\nA vault is not part of daily business operations.\nTherefore, its value and purpose must be clearly understood and supported at the highest levels of the organization.\nThis requires:\n‚≠ê Top-down executive sponsorship Without leadership support, vault initiatives often stagnate, lack funding, or fail to gain organizational adoption.\nFigure 7: Executive sponsorship ‚Äì A top-down approach aligns the entire organization.\nGetting Started with AWS AWS provides multiple layers of defense against ransomware.\n‚úî Storage \u0026amp; Backup AWS Backup ‚Äî centralized management, immutable backups, logical air-gapping Amazon S3 ‚Äî versioning + Object Lock Amazon FSx for NetApp ONTAP ‚Äî tamper-proof snapshots ‚úî Threat Detection Amazon GuardDuty ‚Äî suspicious activity detection AWS Security Hub ‚Äî unified security visibility Amazon Macie ‚Äî sensitive data discovery AWS WAF \u0026amp; AWS Shield ‚Äî web attack and DDoS mitigation AWS Network Firewall ‚Äî network-layer protection Partner solutions like Elastio integrate with AWS Backup to verify data integrity in near real-time, enabling ‚Äúclean recoveries‚Äù with minimal downtime.\n‚úî Identity Protection \u0026amp; Compliance AWS IAM ‚Äî least privilege access AWS Organizations ‚Äî centralized policy governance AWS Config \u0026amp; AWS CloudTrail ‚Äî configuration auditing + forensic analysis Conclusion Cyber incidents ‚Äî especially ransomware ‚Äî continue to rise.\nOrganizations must shift from defending against random failures to preparing for intentional, targeted disruptions.\nThis requires:\na clear recovery strategy detailed planning for worst-case scenarios frequent testing and validation coordinated action across the entire organization Cyber resilience goes far beyond the IT department.\nIt demands commitment from:\noperations finance business leadership engineers front-line staff True resilience becomes possible only when it becomes a shared responsibility across the entire organization, from the CEO to system administrators.\nAbout the Authors Tom Tasker Storage Solution Architect ‚Äì Global Financial Services, AWS\nTom works with major global financial organizations, regulators, and partners to design secure, resilient storage architectures on AWS.\nDanny Johnston Head of Global Financial Services Storage Business Development, AWS\nDanny specializes in enterprise storage strategy and collaborates with customers and regulatory bodies to drive modernization and digital transformation initiatives.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.4-api-gateway-integration/5.4.1-create-http-api/",
	"title": "Create HTTP API",
	"tags": [],
	"description": "",
	"content": "Create HTTP API In this step, you will create an HTTP API that will serve as the endpoint for clients to send questions to your Lambda function.\nüîπ Step 1 ‚Äî Go to API Gateway and click Create API Open the Amazon API Gateway Console and select Create API.\nüîπ Step 2 ‚Äî Select Build under HTTP API Choose:\nBuild ‚Üí HTTP API\nüîπ Step 3 ‚Äî Rename the API Set:\nAPI name: bedrock-chatbot-api Click Next, then Create to finish creating the API.\nüéØ Result You have successfully created an HTTP API, and it is now ready to be integrated with your Lambda function in the next step.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.1-create-lambda/",
	"title": "Create Lambda Function",
	"tags": [],
	"description": "",
	"content": "Create Lambda Function In this step, you will create a new AWS Lambda function that will receive requests from the client and send prompts to Amazon Bedrock using the Converse API.\nüîπ Step 1 ‚Äî Open the Lambda creation page Go to the AWS Lambda Console Select Functions Click Create function üîπ Step 2 ‚Äî Set the name and configuration for the Lambda function Configure the function as follows:\nFunction name: bedrock-chatbot-lambda Runtime: Python 3.12 (recommended and stable for this workshop) Architecture: x86_64 or arm64 (both are supported) Permissions ‚Üí Use an existing role Select the role you created in the Prerequisites section, for example:\nlambda-bedrock-role Then click Create function.\nüéØ Expected result You now have an empty Lambda function that:\nIs ready for adding the Converse API invocation code Has an execution role with Bedrock permissions + CloudWatch logging Can be tested directly in the Lambda Console In the next section, you will configure the IAM role further and prepare the source code to call the model.\nContinue to 5.3.2 ‚Äì Add code to call the Converse API.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report ‚ÄúViet Nam Cloud Day 2025‚Äù Event Objectives Inspire and unlock AI + Cloud adoption for Vietnamese businesses Connect the ecosystem: policymakers ‚Äì enterprises ‚Äì startups ‚Äì experts Share hands-on transformation stories and repeatable implementation roadmaps Move from vision to measurable outcomes Promote data-driven decision making and standardized delivery practices Speakers Hung Nguyen Gia ‚Äì Head of Solutions Architecture, AWS Son Do ‚Äì Technical Account Manager, AWS Nguyen Van Hai ‚Äì Director of Software Engineering, Techcombank Phuc Nguyen ‚Äì Solutions Architect, AWS Alex Tran ‚Äì Head of AI, OCB Nguyen Minh Ngan ‚Äì AI Specialist, OCB Nguyen Manh Tuyen ‚Äì Head of Data Applications, LPBank Securities Vinh Nguyen ‚Äì Co-founder \u0026amp; CTO, Ninety Eight Hung Hoang ‚Äì Customer Solutions Manager, AWS Key Highlights Morning Panel: Navigating the GenAI Revolution Build organizational capability to absorb AI: data, processes, people Start small, measure clearly, iterate fast, and scale with control Change management: leadership roles, internal communication, and AI ethics framework Define KPI/OKR for AI use cases to track ROI and tangible value Establish guardrails to mitigate risks: model bias, data leakage, and output misuse Track 1: Gen AI and Data Build a unified ‚Äúdata foundation‚Äù: quality, lineage, governance, and security GenAI on AWS roadmap: from data readiness to model operations AI across the software development lifecycle: from requirements to testing Design guardrails and least-privilege access control Construct knowledge stores with vector stores, feature stores, and data cataloging Model observability: drift, output quality, and inference cost management AI/ML cost governance: model selection, context optimization, caching, and batching Track 2: Migration \u0026amp; Modernization Large-scale migration lessons: insights from financial services Application modernization with AI-powered coding assistants and automated testing Transform VMware infrastructure to AWS: minimize downtime and optimize operations cost Security at scale: embed security end-to-end from build to run Path from monolith ‚Üí microservices, containerization and/or serverless by domain IaC \u0026amp; policy-as-code: standardize configs, cut operational errors, enable auditing Automate CI/CD pipelines, regression testing, and security scans pre-release What I Learned GenAI is the next wave of transformation AI reshapes how businesses build products, serve customers, and operate. Success requires integrating AI into strategy, culture, and core capabilities. Apply product thinking to AI: measure, iterate, and scale. Data is the foundation of every AI initiative Unified, high-quality, well-governed data determines AI effectiveness. Standardize metadata, lineage, sensitivity classification, and role-based access. Treat data as a strategic asset, not just technical input. Security and responsibility enable sustainable AI Security-by-design: encryption, data domain separation, and least privilege. Control model inputs/outputs, filter content, and trace requests and decisions. Transparency, fairness, and regulatory compliance are baseline requirements for modern AI deployments. Applying at Work Launch GenAI POCs with clear objectives (content generation, knowledge search, business assistants) Standardize data catalogs, establish lineage, and role-based access policies Embed security/privacy controls directly into build and run pipelines Adopt an ‚ÄúAI-by-default‚Äù mindset for automatable workflows Set up an AI/ML CoE, implementation playbooks, and security checklists Train teams on prompt engineering, output evaluation, and AI ethics Build a prioritized use case backlog by value vs complexity Event Experience Attending AWS Cloud Day 2025 gave me a clear pathway to bring AI into the organization: start with trustworthy data, set explicit business value metrics, then progressively raise levels of automation. What struck me most was how leading organizations balance experimentation speed with operational discipline. I left with a practical action list and strong motivation to drive change within the team.\nBroader View of Technology and Trends Recognize the pivotal role of data ‚Äì GenAI ‚Äì security in strategy. See AWS‚Äôs priorities in Vietnam and the region around AI, modernization, and security. Understand the picture from strategy to operations, not just isolated technologies. Connect and Learn from Experts Hear cross-industry case studies from banking, tech, and startups. Exchange directly with architects, leaders, and practitioner communities. Expand networks for collaboration and real-world implementation sharing. New Mindset for Application Embed AI into daily workflows instead of scattered pilots. Establish a solid data foundation before ‚Äúburning‚Äù budget on large models. Make security, privacy, and compliance design criteria from the start. Inspired to Innovate Embrace the spirit of ‚ÄúThink Big, Build Smart, Ship Secure‚Äù. Accelerate learning, controlled experimentation, and internal sharing. Commit to practical, measurable digital and AI transformation. Lessons learned AI delivers sustainable value only when tightly connected to strategy, standardized data, and disciplined security. Organizations must be flexible in experimentation yet rigorous in operations to scale without losing control. The clear path: trustworthy data ‚Üí KPI-backed use cases ‚Üí robust guardrails ‚Üí staged automation.\nSome Photos "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report ‚ÄúAWS Cloud Mastery Series #1‚Äù Event Objectives Update the AI/ML landscape in Vietnam and the region Equip attendees with AWS AI/ML foundations, with a focus on SageMaker Demonstrate Generative AI on Amazon Bedrock through illustrative demos Strengthen community connections and share practical implementation stories Speakers Lam Tuan Kiet ‚Äî Sr. DevOps Engineer, FPT Software Danh Hoang Hieu Nghi ‚Äî AI Engineer, Renova Cloud Dinh Le Hoang Anh ‚Äî Cloud Engineer Trainee, First Cloud AI Journey Key Highlights AI/ML Services Landscape on AWS Overview of AWS‚Äôs AI/ML services and common use cases SageMaker as a full lifecycle ML platform: from data ingest to inference Reference workflow: data cleansing, training, evaluation, packaging, and deployment Practical MLOps: automate pipelines, monitor, and govern model lifecycles Demo with SageMaker Studio covering frequent scenarios Generative AI with Amazon Bedrock Foundation Models (Claude, Llama, Titan) and task‚Äëaligned selection criteria Prompt Engineering \u0026amp; Chain‚Äëof‚ÄëThought: structuring prompts to improve answers RAG: combining knowledge retrieval with FMs for accuracy and freshness Bedrock Agents: orchestrate multi‚Äëstep workflows, tools, and enterprise data Guardrails: control content, reduce bias, and mitigate data‚Äëleak risks Demo building a context‚Äëaware GenAI chatbot using internal documents What I Learned Grasping and Applying AWS AI/ML Services End‚Äëto‚Äëend model building on SageMaker: prepare data, train, and deploy Apply MLOps to optimize time/cost and automate key ML stages Translate knowledge into action through SageMaker Studio demos Applying GenAI with Amazon Bedrock Choose the right FM (Claude, Llama, Titan) based on business needs Use Prompt Engineering and RAG to improve quality and reliability Build chatbots with guardrails and coordinate tasks via Bedrock Agents Applying to Work Automate and optimize data‚Äëto‚Äëmodel workflows\nUse SageMaker + MLOps to shorten development, training, and deployment Build internal Generative AI applications\nCombine Bedrock, RAG, and Prompt Engineering for customer support bots, employee assistants, and document‚Äëdriven Q\u0026amp;A Boost productivity and decision‚Äëmaking with AI\nLeverage FMs for faster analysis, report generation, NLP, and support for business/tech teams Event Experience Attending ‚ÄúAWS Cloud Mastery Series #1‚Äù offered a clear, hands‚Äëon, and inspiring learning journey: we updated knowledge in AI/ML and GenAI, watched real demos, engaged with experts, and explored how to bring these capabilities into day‚Äëto‚Äëday work. Highlights:\nHands‚Äëon Content, Easy to Implement Clear delivery with examples and live demos made concepts easy to grasp Materials/guides helped map learning directly to concrete use cases Energetic Learning, Broad Connections Direct interaction with speakers/experts through open Q\u0026amp;A Effective networking expanded ties across the AI/ML community Lessons learned AI/ML and GenAI are increasingly accessible via platforms like SageMaker and Bedrock, accelerating experimentation and delivery Prompt Engineering and RAG are pivotal, determining output quality and reliability Real value comes when technology is designed around business goals, not isolated from context "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Phan Minh Triet\nPhone Number: 0947720616\nEmail: phanminhtriet283@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction to Amazon Bedrock Amazon Bedrock is a fully managed platform that provides access to a wide range of powerful large language models (LLMs). It enables applications to interact with AI models through simple API calls without the need to deploy or manage machine learning infrastructure. Bedrock supports a variety of foundation models such as Claude, Llama, and Titan, making it suitable for tasks like text generation, question-answering, content summarization, and other AI-driven use cases. In a serverless architecture, Bedrock can be integrated directly with AWS Lambda to build lightweight, scalable, and easily maintained AI services. Workshop Overview In this workshop, you will build a simple question-and-answer service using Amazon Bedrock. When a user submits a query, a Lambda function will construct a prompt, send it to a Bedrock model, and return the generated response.\nTo support this workflow, the workshop makes use of three core components:\nLambda function ‚Äì acts as the processing layer, receiving input and invoking Bedrock‚Äôs API. Amazon Bedrock Runtime ‚Äì performs the inference based on the selected model. API Gateway (optional) ‚Äì exposes an HTTP endpoint for external clients or users to send their questions. The high-level workflow of the system is as follows:\nA user sends a question through API Gateway. API Gateway forwards the request to the Lambda function. Lambda invokes Amazon Bedrock with the constructed prompt. Bedrock returns the generated model output. Lambda formats and returns the response to the client. The diagram below illustrates the overall architecture used in this workshop:\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives Connect and get acquainted with members of the First Cloud Journey (FCJ) team. Understand basic AWS services, how to create and manage costs with an AWS account. Learn how to use the AWS Console and AWS CLI to interact with and manage services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Get to know FCJ members - Read and note internship regulations and rules 08/09/2025 08/09/2025 3 - Learn about AWS and its basic service types + Compute (EC2) + Storage (S3) + Networking (VPC) + Database (RDS) 09/09/2025 09/09/2025 cloudjourney.awsstudygroup.com 4 - Create an AWS Free Tier account - Learn AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Manage identity and access permissions + Install and configure AWS CLI + Use AWS CLI with basic operations 10/09/2025 10/09/2025 cloudjourney.awsstudygroup.com 5 - Learn how to effectively manage costs using AWS Budgets + Cost Budget + Usage Budget + Reservation (RI) Budget + Savings Plans Budget - Practice: + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Savings Plans Budget + Clean up unused resources 11/09/2025 11/09/2025 cloudjourney.awsstudygroup.com 6 - Learn about AWS Support service - AWS Support Plans: + Basic, Developer, Business, and Enterprise - Types of support requests: + Account and Billing support + Service Limit Increase + Technical Support - Practice: + Select Basic Support plan + Create a support case 12/09/2025 12/09/2025 cloudjourney.awsstudygroup.com Week 1 Achievements Understood what AWS is and the main service groups:\nCompute: Provides computing resources for applications such as virtual machines and containers. Storage: Used for storing, backing up, and recovering data. Networking: Manages network infrastructure, security, and connectivity between AWS resources. Database: Offers both relational and non-relational database management services. Successfully created and configured an AWS Free Tier account.\nLearned to create and manage Groups and Users in IAM.\nUnderstood how to log in using IAM and that users within the same group share the same permissions.\nBecame familiar with the AWS Management Console and how to navigate, access, and use services from the web interface.\nInstalled and configured AWS CLI on the local machine, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nChecking account and configuration information Listing available regions Creating and deleting S3 Buckets Using Amazon SNS Creating IAM Groups, Users, and adding users to groups Creating and deleting Access Keys Creating and configuring a basic VPS Running and terminating EC2 Instances Learned how to manage and monitor costs in AWS through:\nCreating and configuring Budget types (Cost, Usage, RI, Savings Plan). Cleaning up unused resources for efficient cost management. Understood the AWS Support Plans and learned how to create support requests via the AWS Support Center:\nBasic: Free, provides support for account and billing issues through the Help Center. Developer: $29/month, includes basic architectural guidance and unlimited technical support from the root account. Business: $100/month, commonly used by small and medium businesses with features such as use-case-based guidance, access to AWS Support API, and unlimited support requests from all IAM users. Enterprise: $15,000/month, for large-scale enterprises with strict security and reliability standards, offering advanced architectural, infrastructure, strategic, and cost optimization support, with priority case handling. Became comfortable using both AWS Console and AWS CLI for basic operations.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand the concept of Amazon Virtual Private Cloud (VPC) and its importance in AWS architecture. Learn how to design, deploy, and manage a virtual private network on AWS. Learn how to set up a AWS Site-to-Site VPN connection between On-Premise and AWS Cloud environments. Master network security best practices following the AWS Well-Architected Framework. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Overview of Amazon VPC - Study AWS network architecture and the role of VPC in the cloud environment - Understand the concepts: Subnet, Route Table, Internet Gateway, NAT Gateway, VPC Peering 15/09/2025 15/09/2025 cloudjourney.awsstudygroup.com 3 - Hands-on: + Create a VPC with a standard network structure + Create Public and Private Subnets + Configure Route Table and Internet Gateway for Internet access 16/09/2025 16/09/2025 cloudjourney.awsstudygroup.com 4 - Learn about Network Security on AWS + Security Groups + Network ACLs + Compare and practice configuring access rules - Hands-on: + Create Security Groups and Network ACLs for VPC + Test access control using EC2 instances 17/09/2025 17/09/2025 cloudjourney.awsstudygroup.com 5 - Introduction to AWS Site-to-Site VPN - Study the connection model between On-premise and AWS Cloud - Hands-on: + Create Virtual Private Gateway (VGW) and Customer Gateway (CGW) + Establish VPN connection between both environments + Verify connection status and routing configuration 18/09/2025 18/09/2025 cloudjourney.awsstudygroup.com 6 - Summarize and review Week 2 learning outcomes - Advanced Practice: + Design VPC based on AWS Well-Architected Framework + Automate infrastructure deployment using Infrastructure as Code (IaC) templates (CloudFormation or Terraform) + Clean up resources after completing the workshop 19/09/2025 19/09/2025 cloudjourney.awsstudygroup.com Week 2 Achievements: Clearly understood what Amazon VPC is and the importance of network isolation in AWS Cloud.\nMastered the basic structure of a VPC, including:\nSubnets: Divide network segments for different resources. Route Tables: Define routing paths for network traffic within VPC. Internet Gateway: Enables public subnet Internet access. NAT Gateway: Allows private subnet instances to securely connect to the Internet. VPC Peering: Connects two different VPCs for resource sharing. Successfully deployed a VPC with both public and private subnets, and verified connectivity through EC2 instances.\nUnderstood and configured Security Groups and Network ACLs, differentiating their scopes and use cases.\nLearned the process of setting up a Site-to-Site VPN between on-premise systems and AWS:\nCreated and configured Virtual Private Gateway (VGW) and Customer Gateway (CGW). Established VPN connections using routing and tunnel configuration parameters. Verified VPN connection via AWS CLI and AWS Management Console. Mastered advanced network security practices following the AWS Well-Architected Framework, including:\nNetwork segmentation. Access control at both subnet and instance levels. Data encryption during transmission over VPN. Gained hands-on experience with Infrastructure as Code (IaC) to automate the creation of VPCs, Subnets, Route Tables, and Security Groups.\nCompleted the workshop by:\nDesigning and deploying a complete VPC model. Successfully establishing a Site-to-Site VPN connection. Cleaning up all AWS resources after completion. Summary of Knowledge Gained: Amazon VPC: Gained in-depth understanding of virtual networking in AWS. Network Security: Learned to configure and manage secure access with Security Groups and Network ACLs. VPN Connection: Understood how to set up and maintain secure Site-to-Site VPN connections. AWS CLI \u0026amp; Console: Practiced deploying, testing, and cleaning up resources using both tools. IaC (Infrastructure as Code): Learned how to automate AWS infrastructure deployment efficiently. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand the concept and functionality of Amazon EC2 (Elastic Compute Cloud). Learn how to launch, connect, configure, and manage EC2 instances on both Windows and Linux. Practice deploying a sample web application (AWS User Management) on EC2 instances. Learn how to monitor, secure, and clean up EC2 resources efficiently. Tasks to be carried out this week: No. Task Start Date Completion Date Reference 1 - Overview of Amazon EC2 and its key concepts: + Instance, AMI, Key Pair, Elastic IP, Security Group, Volume + EC2 pricing models (On-Demand, Spot, Reserved, Savings Plan) 22/09/2025 22/09/2025 cloudjourney.awsstudygroup.com 2 - Hands-on: Create and configure a Windows EC2 instance + Choose AMI and instance type + Configure key pair and security group + Connect using Remote Desktop (RDP) + Explore Windows Server 2022 environment 23/09/2025 23/09/2025 cloudjourney.awsstudygroup.com 3 - Hands-on: Create and configure a Linux EC2 instance + Launch Amazon Linux 2 + Connect via SSH using key pair + Explore the Linux environment and essential commands + Update system packages 24/09/2025 24/09/2025 cloudjourney.awsstudygroup.com 4 - Deploy sample app ‚ÄúAWS User Management‚Äù + Install Node.js, npm, and dependencies on both Linux and Windows instances + Deploy CRUD web app (User Management System) + Test Create, Read, Update, Delete, and Search functions + Share app across network using Security Groups 25/09/2025 25/09/2025 cloudjourney.awsstudygroup.com 5 - Learn EC2 monitoring and management tools: + Amazon CloudWatch (for metrics and logs) + AWS Systems Manager + EC2 Instance Connect - Clean up unused instances, Elastic IPs, and security groups 26/09/2025 26/09/2025 cloudjourney.awsstudygroup.com Week 3 Achievements: Understood the core concepts of Amazon EC2, including:\nAMI (Amazon Machine Image): Provides the OS and app template for launching instances. Instance Type: Defines computing capacity (CPU, RAM, storage). Key Pair: Used for secure login (SSH/RDP). Elastic IP: A static IP for accessing instances over the Internet. Security Group: Acts as a virtual firewall controlling inbound/outbound traffic. Successfully created and configured Windows and Linux EC2 instances:\nConnected via RDP (Windows) and SSH (Linux). Managed instances through AWS Console and CLI. Configured network rules to allow web traffic (port 80, 443, 22, 3389). Deployed the AWS User Management web application on both platforms:\nInstalled Node.js, npm, and app dependencies. Deployed a fully functional CRUD application (Add, Edit, Delete, Search users). Shared the application with other users using public IP or Elastic IP. Learned to monitor EC2 instances using:\nAmazon CloudWatch (view CPU, memory, and network usage). AWS Systems Manager for automation and instance control. EC2 Instance Connect for secure browser-based access. Practiced resource management and cost optimization:\nStopped and terminated instances when not in use. Released unused Elastic IPs. Deleted unneeded Security Groups and Key Pairs. Summary of Knowledge Gained: Amazon EC2: Deep understanding of cloud-based compute instances. Windows \u0026amp; Linux Management: Hands-on experience configuring, connecting, and securing both OS environments. Application Deployment: Deployed Node.js CRUD app on EC2 instances. Cloud Monitoring: Used CloudWatch and Systems Manager to observe performance. Cost Efficiency: Learned to clean up and optimize EC2 resources effectively. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives Understand how to grant application permissions to access AWS services. Learn the difference between Access Key/Secret Access Key and IAM Role. Know how to create and attach IAM Roles to EC2 instances for secure access to AWS services. Become familiar with AWS Cloud9 IDE, a cloud-based development environment. Practice writing, running, and debugging code directly in Cloud9 with AWS CLI integration. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Overview of AWS Identity and Access Management (IAM) - Understand how AWS controls access to resources - Review key concepts: User, Group, Policy, Role 2025-09-22 2025-09-22 cloudjourney.awsstudygroup.com 3 - Practice granting permissions via Access Key and Secret Access Key - Test application accessing AWS services with credentials - Analyze the security risks of embedding credentials in source code 2025-09-23 2025-09-23 cloudjourney.awsstudygroup.com 4 - Introduce and create IAM Role for EC2 Instances - Attach IAM Role and test application access to S3/DynamoDB - Practice revoking permissions and re-testing application 2025-09-24 2025-09-24 cloudjourney.awsstudygroup.com 5 - Introduction to AWS Cloud9 IDE - Create a Cloud9 environment and configure workspace - Explore features: terminal, file explorer, debugger, syntax highlighting 2025-09-25 2025-09-25 cloudjourney.awsstudygroup.com 6 - Advanced Practice: + Write AWS CLI scripts within Cloud9 + Deploy Node.js CRUD app (AWS User Management) in Cloud9 + Clean up AWS resources after completion 2025-09-26 2025-09-26 cloudjourney.awsstudygroup.com Week 4 Achievements: Fully understood the difference between Access Key/Secret Key and IAM Role, and why IAM Roles are more secure. Learned how to create and configure IAM Roles with proper permissions (e.g., read/write access to S3). Successfully attached IAM Roles to EC2 instances and accessed AWS services securely. Became familiar with AWS Cloud9 IDE: Created and configured development environments. Connected to EC2 instance. Executed AWS CLI commands and tested sample code directly in the IDE. Built and deployed a simple Node.js application on Cloud9 to interact with AWS S3 and DynamoDB. Practiced resource cleanup after workshops to avoid unnecessary billing. Summary of Knowledge Gained: IAM Role: Secure and scalable permission model for EC2. Access Key/Secret Key: Recognized risks of storing static credentials in code. AWS Cloud9: Browser-based IDE supporting multiple languages and AWS integration. AWS CLI \u0026amp; SDK: Tools for interacting programmatically with AWS services. Practical Experience: Configured, tested, and cleaned up AWS resources effectively. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Gain a deep understanding of Amazon S3 ‚Äî object storage model, key features, and use cases. Practice hosting a static website on S3: enable the feature, configure public access, test, and optimize performance. Learn how to secure buckets (Block Public Access, IAM policy, Bucket Policy) while allowing public access for specific objects when necessary. Perform advanced management operations: Versioning, S3 Transfer Acceleration, S3 Batch / S3 Replication (cross-region replication), and object migration. Understand how to clean up resources to avoid unwanted costs and follow S3 best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 Environment setup \u0026amp; basic theory + Overview of S3: object, bucket, key, region, storage class (STANDARD, IA, GLACIER). + Durability \u0026amp; availability (11 nines). + Use cases (static website, backup, data lake). 2025-10-06 2025-10-06 cloudjourney.awsstudygroup.com 3 Create an S3 bucket \u0026amp; enable Static Website Hosting + Create a bucket (naming rules, region selection). + Upload index.html / error.html files. + Enable static website hosting and test the endpoint. 2025-10-07 2025-10-07 cloudjourney.awsstudygroup.com 4 Configure Block Public Access \u0026amp; Bucket Policy + Understand Block Public Access at account and bucket levels. + Configure Bucket Policy to allow public access only for index.html. + Test browser access and validate security. 2025-10-08 2025-10-08 cloudjourney.awsstudygroup.com 4 Optimize performance \u0026amp; advanced security + Compare S3 + CloudFront vs S3 Transfer Acceleration vs AWS Amplify Hosting. + Enable S3 Transfer Acceleration and test with curl from multiple regions. + (Optional) Create a CloudFront distribution for HTTPS + CDN. + Monitor via CloudWatch and track costs. 2025-10-09 2025-10-09 cloudjourney.awsstudygroup.com 6 Versioning, Lifecycle \u0026amp; Replication + Resource cleanup + Enable bucket versioning, test overwrite and restore old versions. + Set lifecycle rules to transition objects to IA/Glacier. + Configure Cross-Region Replication (CRR), create IAM replication role. + Move/copy/sync objects across buckets or regions. + Clean up resources: delete test objects, disable acceleration, remove CloudFront (if any), delete test bucket. 2025-10-10 2025-10-10 cloudjourney.awsstudygroup.com Week 4 Achievements: Configuration and basic operations:\nSet up and configured AWS CLI to work with Amazon S3 (Access Key, Secret Key, default Region). Verified connection and configuration using: aws configure list aws s3 ls to list existing buckets. aws s3api list-buckets for detailed information. Created and deleted buckets via CLI: aws s3 mb s3://bucket-name aws s3 rb s3://bucket-name --force Static Website Hosting Practice:\nCreated a new bucket, uploaded index.html and error.html. Enabled Static Website Hosting and successfully accessed the website through the public endpoint. Verified 403/404 errors and ensured error.html was correctly displayed. Security and Access Management:\nEnabled and disabled Block Public Access at both account and bucket levels to understand behavior. Configured Bucket Policy to allow only index.html to be publicly accessible. Tested public/private object access for permission validation. Data Management and Performance Optimization:\nEnabled Versioning on the bucket, uploaded multiple object versions, and restored older ones. Configured Lifecycle Rules to automatically move infrequently accessed data to IA or Glacier. Enabled S3 Transfer Acceleration and measured upload speed differences across regions using curl. Advanced Operations and Data Movement:\nConfigured Cross-Region Replication (CRR) to copy objects to a different region, understood IAM Role requirements (Replication Role). Practiced CLI operations: aws s3 cp, aws s3 mv, aws s3 sync for moving and syncing data between buckets. Verified successful replication in the destination bucket. Cleanup \u0026amp; Cost Optimization:\nDeleted test objects, disabled Transfer Acceleration and CloudFront (if used). Deleted unused test buckets to prevent additional charges. Compiled best practices for cost and security: Avoid public buckets entirely. Use Versioning + Lifecycle Rules for recovery and cost reduction. Use S3 Storage Lens and CloudWatch Budgets to monitor and control costs. Notes: Never make the entire bucket public; use Bucket Policy to expose only specific objects. Enable Block Public Access at the account level by default. Use Versioning + Lifecycle Rules to prevent data loss and reduce cost. Use CloudFront or Amplify Hosting for HTTPS, CDN, and modern static site deployment. Monitor costs using CloudWatch and S3 Storage Lens. Delete carefully when Versioning is enabled‚Äîremove all versions and delete markers before deleting a bucket. Summary of Knowledge Gained: Operate Amazon S3 at a production level: hosting, security, cost optimization, versioning, and replication. Choose the right tool for static website hosting: Amplify (recommended), CloudFront + S3, or S3 only for simplicity. Perform large-scale data management: object transfer, cross-region replication, and automation with Lifecycle Rules. Clean up resources properly to avoid unexpected costs. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 6 Objectives: Understand the architecture of Amazon RDS and supported engines (MySQL, PostgreSQL, MariaDB, SQL Server, etc.). Create, configure, and manage an RDS Instance at the infrastructure level. Set up parameter groups, security groups, and subnet groups for RDS. Connect to RDS from EC2, Cloud9, and from a local machine. Practice backup ‚Äì restore ‚Äì snapshot ‚Äì automated backup ‚Äì failover. Become familiar with Performance Insights, Monitoring, and Enhanced Logging. Learn how to optimize cost, scale storage/compute, and clean up unused resources. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Amazon RDS Overview + Supported engines + Multi-AZ \u0026amp; Read Replica architecture + Backup/Snapshot lifecycle 13/10/2025 13/10/2025 cloudjourney.awsstudygroup.com 3 Create an RDS Instance + Create a Subnet Group + Configure Security Group (allow EC2/Cloud9) + Choose engine, instance size, storage 14/10/2025 14/10/2025 cloudjourney.awsstudygroup.com 4 Connect \u0026amp; Operate on RDS + Connect from Cloud9/EC2 + Create database \u0026amp; table + CRUD using MySQL Client or PostgreSQL Client 15/10/2025 15/10/2025 cloudjourney.awsstudygroup.com 5 Backup ‚Äì Restore ‚Äì Snapshot ‚Äì Monitoring + Create manual snapshot + Restore from snapshot + Monitor CPU, connections + Use Performance Insights 16/10/2025 16/10/2025 cloudjourney.awsstudygroup.com 6 Scaling ‚Äì Cost Optimization ‚Äì Resource Cleanup + Modify instance class + Increase storage + Configure proper automated backup retention + Delete snapshots \u0026amp; RDS instances 17/10/2025 17/10/2025 cloudjourney.awsstudygroup.com Achievements in Week 6: 1. Solid understanding of Amazon RDS architecture: Differentiated between Single-AZ, Multi-AZ, and Read Replica. Understood how automated backups work (retention 1‚Äì35 days). Understood that snapshots are manual backups and not auto-deleted. 2. Successfully created and configured a full RDS Instance: Created a DB Subnet Group with 2 subnets across 2 AZs.\nCreated a Security Group allowing connections from EC2/Cloud9 (port 3306 or 5432).\nConfigured the instance with:\nEngine (MySQL/PostgreSQL) Instance class (db.t3.micro) Storage (20GB gp3) Backup retention Public/Private endpoint options 3. Successfully connected from EC2 / Cloud9 / local machine: Installed MySQL/PostgreSQL client.\nConnected using the command:\nmysql -h endpoint.amazonaws.com -u admin -p Created database, table, and performed CRUD operations:\nCREATE DATABASE demo; CREATE TABLE users (...); INSERT, SELECT, UPDATE, DELETE 4. Mastered Backup ‚Äì Restore ‚Äì Snapshot flow: Created manual snapshots. Restored a new RDS instance from snapshots. Observed backup windows. Verified daily automated backups. 5. Monitoring \u0026amp; Performance: Monitored CPU, RAM, and active connections from the Monitoring dashboard. Used Performance Insights to identify heavy queries. Checked Slow Query Log (if enabled). 6. Scaling \u0026amp; Cost Optimization: Modified instance class from t3.micro ‚Üí t3.small. Increased storage from 20GB ‚Üí 30GB. Reduced backup retention to 3 days for cost savings. Removed unnecessary snapshots. 7. Cleaned up resources to avoid extra charges: Deleted the RDS instance. Deleted subnet group. Deleted security group. Deleted old snapshots. Summary of Knowledge Gained: Understood how Amazon RDS works and common database engines. Learned how to create, configure, and connect to RDS from Cloud9 and EC2. Gained knowledge of backup, snapshot, and restore workflows. Used Performance Insights for performance monitoring. Understood how to scale and optimize RDS cost. Cleaned up resources properly to avoid unnecessary fees. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 7 Objectives: Amazon Route 53 ‚Äì DNS \u0026amp; Domain Management\nUnderstand DNS architecture and Route 53‚Äôs role in AWS. Create Public Hosted Zones and manage A/AAAA/CNAME records. Integrate Route 53 with CloudFront and API Gateway. Learn Simple, Weighted, Latency, and Failover routing. AWS WAF ‚Äì Web Application Firewall\nUnderstand Layer 7 protection mechanisms. Create Web ACL and enable AWS Managed Rules. Configure IP blocking and rate limiting. Attach WAF to CloudFront to secure API and website traffic. Amazon CloudFront ‚Äì CDN \u0026amp; Edge Security\nCreate CDN distribution for S3 and API Gateway origins. Configure cache behaviors, TTL settings, and HTTPS. Use Origin Access Control for securing S3 buckets. Enable access logs for monitoring. AWS CloudWatch ‚Äì Monitoring \u0026amp; Observability\nCreate CloudWatch Dashboard for Edge Layer. Analyze WAF \u0026amp; CloudFront logs using Logs Insights. Build alarms for abnormal traffic spikes. Understand metrics such as request count, cache hit ratio, blocked requests. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Route 53 ‚Äì Domain \u0026amp; DNS Setup ‚Ä¢ Create Hosted Zone. ‚Ä¢ Add A/AAAA alias to CloudFront. ‚Ä¢ Verify DNS propagation. 20/10/2025 20/10/2025 cloudjourney.awsstudygroup.com 3 CloudFront ‚Äì Distribution Setup ‚Ä¢ Create distribution for S3/API. ‚Ä¢ Enable HTTPS and OAC. ‚Ä¢ Configure cache behavior. ‚Ä¢ Enable access logs. 21/10/2025 21/10/2025 cloudjourney.awsstudygroup.com 4 AWS WAF ‚Äì Web ACL Configuration ‚Ä¢ Create Web ACL. ‚Ä¢ Enable Managed Rules. ‚Ä¢ Add IP block \u0026amp; rate limit rules. ‚Ä¢ Attach to CloudFront. 22/10/2025 22/10/2025 cloudjourney.awsstudygroup.com 5 CloudWatch ‚Äì Logs \u0026amp; Metrics Analysis ‚Ä¢ Create Log Groups. ‚Ä¢ Query logs via Logs Insights. ‚Ä¢ Create Metric Filter for blocked requests. ‚Ä¢ Build alarms. 23/10/2025 23/10/2025 cloudjourney.awsstudygroup.com 6 Security Hardening ‚Ä¢ Enforce HTTPS redirection. ‚Ä¢ Optimize cache TTL. ‚Ä¢ Analyze attack patterns in logs. ‚Ä¢ Enable AWS Shield Standard. 24/10/2025 24/10/2025 cloudjourney.awsstudygroup.com 7 Resource Cleanup ‚Ä¢ Remove test Web ACL. ‚Ä¢ Delete unused Log Groups. ‚Ä¢ Disable Distribution. ‚Ä¢ Review Billing for WAF/CF logs. 25/10/2025 25/10/2025 cloudjourney.awsstudygroup.com Achievements in Week 7: 1. Strong understanding of Route 53 DNS architecture Configured public DNS and domain routing successfully. Understood traffic flow: DNS ‚Üí CloudFront ‚Üí API. 2. Successfully deployed CloudFront CDN Improved global latency. Configured cache behavior and encryption. Protected S3 using OAC. 3. Built a secure Layer 7 protection system using AWS WAF Blocked malicious traffic (SQLi, XSS, bots). Implemented rate limiting. Analyzed WAF logs for threat detection. 4. Built observability with CloudWatch Created Edge Monitoring Dashboard. Queried logs using Logs Insights. Set alarms for suspicious activity. Summary of Knowledge Gained: Understanding of AWS Edge Services (Route 53, CloudFront, WAF). Ability to secure and accelerate applications using CDN. Hands-on experience in threat detection \u0026amp; log analysis. Ability to build a front-door security system following AWS best practices. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 8 Objectives: Amazon API Gateway ‚Äì API Layer\nUnderstand REST API architecture and components. Build REST API with multiple resources and methods. Configure Integration Requests/Responses with Lambda. Enable CORS following best practices. Configure Custom Domain Name with ACM SSL. Enable execution logging and access logging. Test API using Postman and frontend clients. Amazon Cognito ‚Äì Authentication \u0026amp; Authorization\nLearn Cognito User Pool vs Identity Pool. Create User Pool, App Client, and Hosted UI domain. Configure password policy and email verification. Create Identity Pool for AWS credentials. Integrate Cognito with API Gateway using JWT Authorizer. Test end-to-end login ‚Üí token ‚Üí protected API. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 API Gateway ‚Äì Architecture \u0026amp; REST API Setup ‚Ä¢ Study REST API. ‚Ä¢ Create resources /auth, /users, /items. ‚Ä¢ Configure CRUD methods. 27/10/2025 27/10/2025 cloudjourney.awsstudygroup.com 3 Lambda Integration ‚Ä¢ Create Lambda handlers. ‚Ä¢ Configure IAM roles. ‚Ä¢ Enable Proxy Integration. ‚Ä¢ Test API with Postman. 28/10/2025 28/10/2025 cloudjourney.awsstudygroup.com 4 Cognito User Pool \u0026amp; Identity Pool Setup ‚Ä¢ Create User Pool. ‚Ä¢ Configure password/email policies. ‚Ä¢ Create App Client \u0026amp; Hosted UI. ‚Ä¢ Test signup/login. 29/10/2025 29/10/2025 cloudjourney.awsstudygroup.com 5 JWT Authorization with Cognito ‚Ä¢ Create JWT Authorizer. ‚Ä¢ Attach to /users/* route. ‚Ä¢ Test Access \u0026amp; ID Tokens. ‚Ä¢ Configure 401/403 responses. 30/10/2025 30/10/2025 cloudjourney.awsstudygroup.com 6 Custom Domain + Logging Setup ‚Ä¢ Create custom API domain with ACM SSL. ‚Ä¢ Map domain to API stage. ‚Ä¢ Enable execution \u0026amp; access logs. ‚Ä¢ Analyze logs in CloudWatch Insights. 31/10/2025 31/10/2025 cloudjourney.awsstudygroup.com 7 End-to-End Testing \u0026amp; Cleanup ‚Ä¢ Test full login-to-API flow. ‚Ä¢ Remove unused Lambda/API test resources. ‚Ä¢ Delete test users. ‚Ä¢ Review Billing. 01/11/2025 01/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 8: 1. Built a complete REST API backend with API Gateway Configured multiple routes and methods. Implemented CORS and Lambda Proxy Integration. Successfully tested using Postman. 2. Implemented full authentication system using Cognito Created User Pool and Identity Pool. Enabled secure signup \u0026amp; login flows. Understood JWT token structure and validation. 3. Secured API using JWT Authorizer Protected sensitive routes (/users/*). Tested access with valid, expired, and invalid tokens. 4. Set up monitoring and logging for API Gateway Enabled CloudWatch Logs for debugging. Queried logs using Logs Insights. Investigated 4XX/5XX errors. Summary of Knowledge Gained: Strong understanding of API Gateway operation. Deep knowledge of Cognito authentication flows. Ability to secure APIs using JWT-based authorization. Experience integrating API Gateway + Lambda + Cognito. Familiarity with CloudWatch monitoring and API logging. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 9 Objectives: AWS Lambda ‚Äì Serverless Compute\nUnderstand Lambda execution lifecycle (init, invoke, freeze). Build backend Lambda functions. Integrate Lambda with API Gateway using Lambda Proxy. Configure IAM roles for RDS, S3, and Bedrock access. Use AWS Secrets Manager for secure credential storage. Monitor Lambda via CloudWatch Logs and Metrics. Lambda in VPC \u0026amp; VPC Endpoints\nDeploy Lambda inside private subnets for secure backend operations. Configure Security Groups for Lambda ‚Üí RDS communication. Create VPC Endpoints: S3 Gateway Endpoint for internal S3 access Bedrock Interface Endpoint for private AI API calls Test Lambda connectivity to both RDS and Bedrock. Performance \u0026amp; Cost Optimization\nLearn cold start behaviors and mitigation strategies. Tune memory \u0026amp; timeout settings for best performance. Reduce NAT Gateway traffic using VPC Endpoints. Apply database connection pooling to reduce overhead. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Lambda ‚Äì Overview \u0026amp; Function Creation ‚Ä¢ Study lifecycle. ‚Ä¢ Create first function. ‚Ä¢ Configure IAM execution role. ‚Ä¢ Run test. 03/11/2025 03/11/2025 cloudjourney.awsstudygroup.com 3 Lambda Integration with API Gateway ‚Ä¢ Attach Lambda to REST API. ‚Ä¢ Enable Proxy Integration. ‚Ä¢ Enable CORS. ‚Ä¢ Test with Postman. 04/11/2025 04/11/2025 cloudjourney.awsstudygroup.com 4 Lambda in VPC ‚Äì Private Deployment ‚Ä¢ Attach Lambda to private subnets. ‚Ä¢ Configure SG rules. ‚Ä¢ Fix timeout issues. ‚Ä¢ Validate DB queries. 05/11/2025 05/11/2025 cloudjourney.awsstudygroup.com 5 VPC Endpoints Setup ‚Ä¢ Create S3 Gateway Endpoint. ‚Ä¢ Create Bedrock Interface Endpoint. ‚Ä¢ Test inference from Lambda. 06/11/2025 06/11/2025 cloudjourney.awsstudygroup.com 6 Secrets Manager Integration ‚Ä¢ Store DB credentials. ‚Ä¢ Retrieve via SDK. ‚Ä¢ Apply IAM policies. ‚Ä¢ Test secure DB connection. 07/11/2025 07/11/2025 cloudjourney.awsstudygroup.com 7 Performance \u0026amp; Cost Optimization ‚Ä¢ Tune memory/timeouts. ‚Ä¢ Implement pooling. ‚Ä¢ Analyze NAT vs Endpoint cost. ‚Ä¢ Review Billing. 08/11/2025 08/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 9: 1. Built a functional serverless backend with AWS Lambda Multiple Lambda functions handling API operations. Understood cold starts and how to optimize them. IAM roles configured securely for resource access. 2. Lambda successfully deployed inside VPC Stable connection to RDS from private subnets. Resolved networking issues (SG, routing, NACL). Verified smooth query execution. 3. Implemented VPC Endpoints for secure \u0026amp; cost-efficient networking S3 Gateway Endpoint removed NAT dependency. Bedrock Endpoint enabled secure AI inference. Overall networking cost reduced significantly. 4. Improved security with Secrets Manager No credentials stored in environment variables. Lambda fetches secrets securely using AWS SDK. 5. Performance \u0026amp; cost improvements Better performance through memory tuning. Reduced DB overload via connection pooling. Lower NAT Gateway costs using VPC Endpoint. Summary of Knowledge Gained: Deep understanding of Lambda serverless architecture. Working knowledge of VPC networking for private workloads. Practical usage of VPC Endpoints (S3, Bedrock). Strong experience using Secrets Manager securely. Ability to optimize Lambda performance \u0026amp; cost for production systems. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Understanding and Configuring Amazon VPC\nWeek 3: Deploying and Managing Amazon EC2 Instances on Windows\nWeek 4: Implementing IAM Roles and Working with AWS Cloud9\nWeek 5: Hosting and Managing Static Websites with Amazon S3\nWeek 6: Working with Amazon RDS ‚Äì Creating, Configuring, Connecting \u0026amp; Operating Databases on AWS\nWeek 7: Route 53, AWS WAF \u0026amp; CloudFront ‚Äì Configure edge security and content delivery; monitor the system with CloudWatch\nWeek 8: API Gateway \u0026amp; Cognito ‚Äì Build the API layer and implement user authentication\nWeek 9: AWS Lambda \u0026amp; VPC Endpoints ‚Äì Deploy serverless compute and connect Lambda to RDS inside a private subnet\nWeek 10: Amazon RDS \u0026amp; S3/Bedrock Endpoints ‚Äì Optimize the database and integrate Lambda access to S3 and Bedrock via VPC Endpoints\nWeek 11: Serverless Front-end Deployment ‚Äì Host front-end on S3 + CloudFront and integrate with API Gateway; implement Cognito authentication\nWeek 12: AI Agent with Amazon Bedrock ‚Äì Build AI-powered APIs, orchestrate workflows using Lambda/Step Functions, and send notifications with SES\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.2-call-bedrock-converse/",
	"title": "Add Code to Call the Converse API",
	"tags": [],
	"description": "",
	"content": "Add Code to Call the Converse API In this step, you will add Python code to your Lambda Function to send user questions to Amazon Bedrock using the Converse API, and return the model‚Äôs response to the client.\nThe Lambda function will perform the following tasks:\nReceive a question from the client or a test event Construct a request following the Converse API format Send the prompt to Amazon Bedrock Runtime Receive the model-generated answer Return the response in JSON format üîπ Step 1 ‚Äî Open the Lambda Function source file Open the Lambda function you created Scroll to the Code source section Open the file lambda_function.py üîπ Step 2 ‚Äî Replace the entire file content with the code below import json import boto3 # Create a client to call Bedrock Runtime bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;) # Choose a model that supports the Converse API MODEL_ID = \u0026#34;anthropic.claude-3-sonnet-20240229\u0026#34; def lambda_handler(event, context): # Receive data from API Gateway or test event body = json.loads(event.get(\u0026#34;body\u0026#34;, \u0026#34;{}\u0026#34;)) if isinstance(event.get(\u0026#34;body\u0026#34;), str) else event question = body.get(\u0026#34;question\u0026#34;, \u0026#34;Hello! What would you like to ask?\u0026#34;) # Send request to Bedrock using the Converse API response = bedrock.converse( modelId=MODEL_ID, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;text\u0026#34;: question} ] } ] ) # Extract the model\u0026#39;s response answer = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;][0][\u0026#34;text\u0026#34;] # Return the result to the client return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;answer\u0026#34;: answer}) } üîπ Important note about MODEL_ID The MODEL_ID value inside lambda_function.py can be changed to any model you want to use.\nHowever, the model must support the Converse API.\nTo find the correct Model ID:\nOpen Amazon Bedrock Console ‚Üí Model catalog Select the model you want to use In the model information panel, look for Model ID Example:\nCopy this Model ID and update the variable:\nMODEL_ID = \u0026#34;your-selected-model-id\u0026#34; If the model does not support the Converse API, Lambda will throw an error when calling bedrock.converse().\nüîπ Step 3 ‚Äî Deploy the Lambda Function After updating the source code:\nClick Deploy Your Lambda Function is now ready to call Amazon Bedrock üéØ Expected Outcome At the end of this section, your Lambda Function will be able to:\nReceive questions from a client Send prompts to Bedrock using the Converse API Receive model-generated responses Return the result as JSON You have now completed the core logic of your AI Q\u0026amp;A service.\nContinue to 5.3.3 ‚Äì Test the Lambda Function.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Dynamic Routing with Amazon VPC Route Server AWS provides the managed Amazon VPC Route Server service to help customers establish and operate dynamic routing using the Border Gateway Protocol (BGP) within Amazon Virtual Private Cloud (Amazon VPC).\nPreviously, to enable dynamic routing inside a VPC, you had to deploy a software-based router on Amazon EC2. Now, with Amazon VPC Route Server, you can:\nEstablish BGP peerings between Route Server and virtual network appliances Exchange routing information dynamically based on network state Automatically update VPC route tables without manual configuration Detect failures faster using Bidirectional Forwarding Detection (BFD) Perform deep traffic inspection across firewalls in Multi-AZ environments This blog walks through how Amazon VPC Route Server works, key use cases, and how to begin using dynamic routing in Amazon VPC.\nOverview of Amazon VPC Route Server A customer deploys a software appliance to process voice traffic. This appliance requires dynamic routing as well as updated routing-state information derived from BGP.\nThe appliance requires:\nDynamic routing from Route Server Bidirectional BGP sessions for liveness detection Appropriate route switching Figure 1: Dynamic routing architecture with Amazon VPC Route Server\nDynamic routing flow:\nThe Route Server establishes a BGP session with the network appliance. The appliance advertises its service routes to the Route Server. The Route Server updates the VPC route tables. The Route Server establishes a BFD session the appliance. The appliance establishes its BFD session toward the Route Server. Both sides perform network-detection procedures to maintain BGP session state. How Dynamic Routing Works in Amazon VPC Figure 2: Route exchange from Route Server into VPC route tables\nSequence:\nThe network appliance establishes a BGP session with the Route Server. Routing updates are exchanged. The Route Server updates the VPC route table. Traffic flows using the new route. When Should You Use Dynamic Routing? ‚úî Automated Failover Handling Failover in cloud architectures often requires routes to change immediately when an instance, device, or Availability Zone becomes unhealthy.\nFigure 3: Failover handled through dynamic routing\n‚úî Floating IP Architecture for High Availability Floating IPs maintain a stable IP address even when the backend instance changes ‚Äî especially important for:\nfinancial applications trading systems real-time services Figure 4: Floating IP architecture for high uptime\n‚úî Traffic Inspection Through Firewalls Commonly used for:\nDMZ environments outbound traffic filtering blocking unauthorized access Dynamic routing enables:\nrouting traffic through firewalls on demand automatic adjustment when firewalls go up/down Figure 5: Multi-AZ firewall inspection architecture\n‚úî Faster Failure Detection with BFD Before BFD, failover detection could take 10‚Äì30 seconds.\nWith BFD ‚Üí under 1 second.\nFigure 6: Faster failure detection using BFD\n‚úî Automatic VPC Route Table Updates When routes on the appliance change ‚Üí VPC route tables update automatically.\nFigure 7: Automatic route updates inside VPC\nRoute Consistency and Route Advertisement Figure 8: Route advertisement inside Route Server\nThe Route Server:\nAccepts routes from the appliance Validates allowed routes Publishes routes into the VPC route table Manages lifecycle based on BGP session state BFD and Its Role in Faster Recovery BFD allows:\nmillisecond-level failure detection faster BGP failover improved consistency for latency-sensitive workloads Figure 9: How BFD accelerates failover\nConclusion Amazon VPC Route Server offers a powerful and flexible way to run dynamic routing inside AWS. With support for:\nBGP BFD Firewall inspection Floating IP architectures Automatic route advertisement ‚Üí Route Server enables the design of highly available, resilient, and easy-to-operate cloud network architectures.\nAuthors Rashid Mulji ‚Äì Principal Solutions Architect, AWS\nRyan Umstead ‚Äì Senior Solutions Architect, AWS\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.4-api-gateway-integration/5.4.2-integrate-lambda/",
	"title": "Integrate API with Lambda",
	"tags": [],
	"description": "",
	"content": "Integrate API with Lambda In this step, you will configure the HTTP API so that incoming requests from the client are forwarded to your Lambda function.\nAPI Gateway will serve as the HTTP interface for your Bedrock-powered chatbot.\nüîπ Step 1 ‚Äî Open the API you created Go to API Gateway Console and select the API:\nbedrock-chatbot-api\nüîπ Step 2 ‚Äî Create a route Go to the Routes section Click Create Configure the route: Method: POST Resource path: /chat Click Create to add the route.\nüîπ Step 3 ‚Äî Add Integration with Lambda Click the newly created /chat route In the Integration section, choose Attach integration Select Create and attach an integration, then configure: Integration type: Lambda function Region: The AWS region you are using Lambda function: lambda-bedrock-function (the Lambda you created earlier) Click Create.\nüéØ Expected Result You have now:\nCreated the /chat route Attached the route to your Lambda function Your API now has an endpoint ready for testing "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.2-prerequisite/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Preparation Steps Before Starting Before building your AI API using Lambda + Bedrock, you need to configure several AWS resources and permissions.\n1. Choose an AWS Region that Supports Bedrock Amazon Bedrock is available in several AWS regions.\nFor this workshop, you may choose:\nus-east-1 (N. Virginia) ‚Äì commonly used in AWS documentation ap-southeast-1 (Singapore) ‚Äì also supports Bedrock and works normally Just make sure the model you want to use (Claude, Llama, Mistral‚Ä¶) is available in that region.\n2. Create an IAM Role for Lambda Lambda needs an IAM Role to call Bedrock models and write logs to CloudWatch.\nIn this section, you will first create a custom policy, then create a role and attach the policy.\nüîπ Step 1 ‚Äî Create a New Policy Open IAM Console ‚Üí Policies ‚Üí Create policy In the ‚ÄúSpecify permissions‚Äù page, select the JSON tab.\nDelete all existing content and replace it with the following: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34;, \u0026#34;bedrock:InvokeModelWithResponseStream\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next, give the policy a name (e.g., lambda-bedrock) and click Create policy. üîπ Step 2 ‚Äî Create the IAM Role and Attach the Policy Go to IAM Console ‚Üí Roles ‚Üí Create role Select: Trusted entity type: AWS service Use case: Lambda In ‚ÄúAdd permissions‚Äù, search for the policy you created earlier (lambda-bedrock) and select it. Name your role, for example: lambda-bedrock-role Then click Create role.\nYour Lambda execution role is now ready with the minimum permissions required to call Amazon Bedrock and write CloudWatch logs.\n3. Verify the Model Using Bedrock Playground Before writing Lambda code to call the Converse API, test the model in the AWS Console.\nüîπ Step 1 ‚Äî Open the Model Catalog Open Amazon Bedrock ‚Üí Model catalog üîπ Step 2 ‚Äî Choose a Model and Open Playground Pick a model such as Claude 3.5 Sonnet, Llama 3.1, or Mistral 24.07 Click Open in playground üîπ Step 3 ‚Äî Send a Test Prompt Enter a simple question to confirm the model is responding correctly in your selected region.\nüîπ Step 4 ‚Äî (Optional) Check if the Model Supports the Converse API If you want to confirm whether the selected model supports Converse API, follow these steps:\nReturn to the model page in the catalog Scroll down to Code examples AWS will open an example code page. If the model supports Converse API, the example will include:\nbedrock.converse(...) Like this:\n4. Recommended (Optional) Knowledge You may find it helpful to know the following:\nBasics of AWS Lambda How to create an HTTP API with API Gateway How to view logs in CloudWatch This workshop is beginner-friendly and does not require deep AWS experience.\nIn the next step, you will create a Lambda function and write your first code snippet to call the Converse API in Amazon Bedrock.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "FitAI Challenge An application that helps users lose weight through exercise challenges, integrated with AI for tracking and evaluation 1. Executive Summary FitAI Challenge is a website developed for Vietnamese users, aiming to promote fitness and exercise culture through sports challenges that incorporate gamification and artificial intelligence (AI). The website uses an AI Camera to recognize and count exercise movements such as push-ups, squats, planks, and jumping jacks, while also analyzing posture to provide accurate evaluations. Users can participate in individual challenges to earn FitPoints upon completing tasks, which can be redeemed for vouchers, gifts, or discounts from partner merchants. FitAI Challenge targets students, young adults, and working professionals ‚Äî individuals who need motivation to maintain regular workout habits amid their busy lives.\n2. Problem Statement What‚Äôs the Problem? In Vietnam, most existing fitness applications primarily focus on basic guidance or step counting, and there is currently no platform that combines AI-based motion recognition, gamification, and an online fitness challenge community. Users often lack motivation to exercise consistently and do not have tools that can accurately evaluate their workout performance. In addition, gyms and sports brands also lack creative engagement channels to connect with young and active customer groups.\nThe Solution FitAI Challenge uses an AI Camera to recognize, count, and evaluate the accuracy of workout movements through Computer Vision. All user workout data is stored and processed via AWS Cloud using a serverless architecture: AWS Lambda: processes AI data and backend requests. AWS S3: stores videos, images, and temporary results. The website is developed using React Native with a friendly and intuitive interface. Users can: Participate in individual, group, or nationwide challenges. Earn FitPoints upon completing exercises. Redeem FitPoints for vouchers or gifts from partners (Shopee, Grab, CGV, etc.). Track leaderboards and share achievements on social media.\nBenefits and Return on Investment For users: Create daily workout motivation through challenge and reward mechanisms. Receive transparent performance evaluations supported by AI. Connect with the fitness community through leaderboards and sharing feeds. For partner businesses: A branding channel associated with a healthy lifestyle. Access to a young, dynamic, and health-conscious customer base. For the development team: Establish a unique ‚ÄúFitness + Gamification + E-commerce‚Äù business model in Vietnam. Serverless cloud architecture helps reduce operating costs and allows easy scalability. The MVP can be developed within the first 3 months with low infrastructure costs (estimated at 0.80 USD/month on AWS).\n3. Solution Architecture FitAI Challenge is an intelligent sports training platform that applies an AWS Serverless architecture combined with an AI/ML pipeline. The system‚Äôs goal is to record workout data, analyze performance, and generate AI-powered feedback to provide personalized coaching for users. Data from the web application is sent to Amazon API Gateway, processed by AWS Lambda (Java), and stored in Amazon S3 along with the Docker Database.\nAWS Services Used Service Role Amazon Route 53 Manages domain names and routes traffic to CloudFront. AWS WAF Protects frontend and API layers from DDoS and OWASP attacks. Amazon CloudFront Delivers static content (web app built from Java web, HTML, CSS, JS). Amazon API Gateway Receives requests from the frontend and forwards them to Lambda functions. AWS Lambda (Java) Handles business logic (registration, login, data upload, scoring, AI pipeline). AWS Step Functions \u0026amp; SQS Coordinates workflows between Lambda and SageMaker/Bedrock. Amazon Cognito Authenticates users, manages login sessions, and controls access permissions. Amazon S3 Stores raw data, videos, images, and analysis results. Docker Runs the Java Spring Boot API backend and hosts the database (PostgreSQL or MongoDB). Amazon SageMaker Runs inference for computer vision/pose estimation models. Amazon Bedrock Generates natural language feedback, training suggestions, and summary reports. Amazon SES Sends authentication emails and user result notifications. Amazon CloudWatch Monitors logs, Lambda performance, costs, and system efficiency. IAM Manages access permissions and security across services. AWS CodePipeline / CodeBuild / CodeDeploy CI/CD pipeline for automating Java backend and Lambda deployment. Component Design Frontend Layer: The web app displays the user interface and connects to the API Gateway. The content is built and deployed on S3 + CloudFront. Users access the system through Route 53 ‚Üí WAF ‚Üí CloudFront ‚Üí API Gateway. Application Layer: The API Gateway receives requests from the frontend. Lambda (Java) executes business functions: AuthLambda: handles user login and authentication. UploadLambda: receives workout data, images, or videos. AIPipelineLambda: triggers the AI workflow (SageMaker + Bedrock). SaveResultLambda: stores training results and AI feedback.\n4. Technical Implementation Implementation Phases\nPhase Description Achieved Outcome 1. AWS Infrastructure Setup Deploy VPC, Private Subnets, Route 53, WAF, S3, API Gateway, Lambda, Cognito, RDS MySQL, VPC Endpoints (S3, Bedrock). Secure, isolated, and ready-to-use base infrastructure. 2. CI/CD Pipeline Set up CodeCommit + CodeBuild + CodeDeploy + CodePipeline for Java backend and Lambda; build \u0026amp; deploy web to S3/CloudFront. Automated deployment for backend and frontend. 3. Build Lambda Functions (Java) Create Lambdas for Auth, Upload, AI Pipeline, Save Result; connect to RDS MySQL and S3. Completed serverless backend. 4. AI Pipeline Integrate SQS, Step Functions, and Bedrock; build workflow: receive job ‚Üí analyze data ‚Üí score ‚Üí generate AI feedback. Smooth AI operation with automated user feedback. 5. Web App Deployment Build web app ‚Üí Deploy to S3 + CloudFront ‚Üí configure domain in Route 53. Stable online user interface. 6. Monitoring \u0026amp; Cost Optimization Use CloudWatch + Cost Explorer to track logs, performance, and costs; tune Lambda, RDS, and CloudFront configurations. Stable system with low cost and tight monitoring. 5. Timeline \u0026amp; Milestones Before internship (Month 0):\nDesign detailed architecture based on the new diagram.\nExperiment with a simple AI pipeline using Bedrock (text feedback).\nInternship (Months 1‚Äì3):\nMonth 1:\nSet up infrastructure: VPC, Subnets, RDS MySQL, Cognito, API Gateway, Lambda, S3, CloudFront.\nConfigure CI/CD (CodeCommit, CodeBuild, CodeDeploy, CodePipeline).\nMonth 2:\nDevelop Java backend and Lambda functions.\nBuild the AI pipeline with SQS, Step Functions, and Bedrock.\nMonth 3:\nIntegrate frontend with backend.\nRun performance tests, pilot with 10‚Äì20 users, prepare the final demo.\nPost-deployment:\nContinue optimizing the AI model and add deeper gamification over the next year.\n6. Budget Estimation You can view costs on the AWS Pricing Calculator, or download the attached budget estimate.\nInfrastructure Costs (MVP estimate)\nAWS Services: Amazon API Gateway: 0.38 USD / month (‚âà300 requests/month, 1 KB/request). Amazon Bedrock: 0.32 USD / month (1 req/min, 350 input tokens, 70 output tokens). Amazon CloudFront: 1.20 USD / month (5 GB transfer, 500,000 HTTPS requests). Amazon CloudWatch: 1.85 USD / month (5 metrics, 0.5 GB logs). Amazon Cognito: 0.00 USD / month (‚â§100 MAU). Amazon Route 53: 0.51 USD / month (1 hosted zone). Amazon S3: 0.04 USD / month (1 GB storage, 1,000 PUT/POST/LIST, 20,000 GET). Amazon SES: 0.30 USD / month (3,000 emails from EC2/Lambda). Amazon Simple Queue Service (SQS): ‚âà0.00 USD / month (0.005 million requests/month). AWS Lambda: 0.00 USD / month (‚âà300,000 requests/month, 512 MB ephemeral storage). AWS Step Functions: 0.00 USD / month (500 workflows, 5 state transitions/workflow). AWS Web Application Firewall (WAF): 6.12 USD / month (1 Web ACL, 1 rule). Amazon RDS MySQL (auto-stop mode): 0‚Äì3 USD / month (depending on runtime). Total: around 10.7 ‚Äì 12 USD / month depending on RDS usage; equivalent to 128 ‚Äì 144 USD / 12 months.\n7. Risk Assessment Risk Matrix\nTechnical: AI misidentifies movements or fails on image/video processing; misconfigured VPC/Endpoints cause service disruption. User: Users don‚Äôt maintain workout habits; low return rate. Market \u0026amp; Partners: Hard to expand reward partners and co-branding; partner policy changes. Cost: Unexpected cost increases when user volume spikes (CloudFront, Bedrock). Mitigation Strategies\nOptimize AI models with continuous training; monitor quality via logs; add basic validation before scoring. Deepen gamification (streak chains, friend groups, seasonal challenges, appealing rewards). Prepare clear value propositions; diversify partner types (sports, healthy food, entertainment). Set AWS Budgets + Alarms per service (CloudFront, Bedrock, Lambda, RDS). Contingency Plans\nIf AI fails ‚Üí use fallback logic (simple time/rep-based scoring) and clearly notify users. If user volume drops ‚Üí launch community challenges, pair with social media campaigns. If commercial partners withdraw ‚Üí maintain internal FitPoints with small rewards (in-app badges/vouchers) while finding new partners. 8. Expected Outcomes Technical Improvements:\nComplete the AI motion recognition system with \u0026gt;90% accuracy. Stable app that supports up to 10,000 concurrent active users on serverless architecture. Optimize architecture to keep infra cost around 10‚Äì12 USD/month in the early stage. Long-term Value:\nBuild a sustainable Vietnamese fitness community connected via online challenges. Become the pioneering ‚ÄúAI + Fitness + Gamification‚Äù platform in Vietnam. Establish a workout data foundation to expand into health analytics, personalized training programs, and future AI projects. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Launch of the AWS Asia Pacific (New Zealand) Region Today, we are excited to announce the availability of the AWS Asia Pacific (New Zealand) Region ‚Äì ap-southeast-6.\nThis new region enables customers to deploy workloads and store data within New Zealand, delivering lower latency and supporting organizations with local data residency and regulatory requirements.\nFigure 1: AWS Asia Pacific (New Zealand) Region\nWhy this region matters The new region provides customers an additional option to deploy applications closer to end users in New Zealand. This is especially important for industries such as:\nfinancial services, government, healthcare, media \u0026amp; telecommunications, and organizations requiring strong data sovereignty. The region improves:\nlatency, availability, durability, and user experience across the APAC area. Infrastructure design of the New Zealand Region The AWS Asia Pacific (New Zealand) Region includes:\n3 Availability Zones (AZs) multiple layers of security controls high-performance, low-latency networking Each Availability Zone is located in a separate geographic area and is designed to be:\nphysically isolated, yet interconnected for high availability and fault-tolerant architectures. Economic investment \u0026amp; community impact AWS is committed to long-term investment in New Zealand. Over the coming years, AWS will:\ncontribute to GDP growth, create new jobs, expand the AWS Partner ecosystem, and accelerate innovation across industries. AWS is also collaborating with universities and training programs to:\nupskill the future cloud workforce, and support digital transformation across New Zealand. Accelerating AI \u0026amp; Generative AI adoption This new region empowers organizations to run AI/ML workloads locally, including:\nAmazon Bedrock, Amazon SageMaker, real-time inference with lower latency, secure training and deployment under data residency requirements. AWS emphasizes building AI systems that are sustainable, secure, and responsible.\nCommitment to sustainability AWS aims to:\npower its operations with 100% renewable energy by 2025, move toward carbon neutrality, and design data centers optimized for energy efficiency. Data centers in New Zealand follow AWS‚Äôs sustainability best practices, helping customers reduce the energy footprint of their cloud workloads.\nExpanded options for customers With this new region, customers can:\nbuild multi-region architectures, implement disaster recovery (DR) and business continuity (BCP) plans, use multi-AZ replication strategies, and increase reliability and fault tolerance. AWS continues to support customers in innovating faster, operating securely, and scaling globally.\nConclusion The launch of the AWS Asia Pacific (New Zealand) Region marks an important milestone in AWS‚Äôs long-term commitment to the Asia-Pacific region.\nThis region provides:\nlower latency, improved data sovereignty, enhanced support for AI/ML workloads, and a secure foundation for business growth and innovation. Authors Bill Vass ‚Äì VP of Engineering, AWS\nPrasad Kalyanaraman ‚Äì VP of Infrastructure Services, AWS\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.3-create-lambda-and-bedrock-call/",
	"title": "Create Lambda and Call Bedrock",
	"tags": [],
	"description": "",
	"content": "Create Lambda and Call Bedrock In this section, you will create a Lambda function and configure it to call an Amazon Bedrock model using the Converse API.\nThe overall architecture was introduced earlier, and this step focuses on implementing the components needed for Lambda to send a prompt to Bedrock and receive a response.\nIn the following steps, you will:\nCreate a new Lambda function Assign the Execution Role you prepared in the Prerequisites section Write the initial code to call a model using the Converse API After completing this section, your Lambda function will be able to interact directly with Bedrock and handle AI Question‚ÄìAnswer requests.\nContents Create Lambda Function Assign IAM Role to Lambda Add Code to Call Bedrock via Converse API "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.3-test-lambda/",
	"title": "Test Lambda Function",
	"tags": [],
	"description": "",
	"content": "Test Lambda Function After creating the Lambda Function and adding the code to call Amazon Bedrock using the Converse API, the next step is to test the function to ensure everything works correctly.\nIn this section, you will:\nCreate a test event in the Lambda Console Send a sample question to the Bedrock model Review the response and check logs if any errors occur üîπ Step 1 ‚Äî Open Lambda and Create a Test Event Go to AWS Lambda Console Select the function: bedrock-chatbot-lambda Click the Test button Choose Create new event if this is your first test Enter an event name, for example: Event name: test-bedrock-converse Scroll down to the JSON editor and replace the entire content with:\n{ \u0026#34;question\u0026#34;: \u0026#34;What is Amazon Bedrock?\u0026#34; } Example illustration:\nClick Save to store the test event.\nüîπ Step 2 ‚Äî Run the Test and Review the Output Select the event you just created Click Test If everything is working correctly, Lambda should return a response similar to:\n{ \u0026#34;answer\u0026#34;: \u0026#34;Amazon Bedrock is a fully managed service...\u0026#34; } The \u0026quot;answer\u0026quot; field contains the model‚Äôs generated response.\nüîπ Step 3 ‚Äî Check CloudWatch Logs if Errors Occur If the Lambda function throws an error or behaves unexpectedly:\nOpen the Monitor tab Click View logs in CloudWatch Open the most recent log stream to inspect the execution details Common issues and fixes:\n‚ùå AccessDeniedException Cause: Lambda Execution Role lacks bedrock:InvokeModel Fix: Re-check the IAM Role created in the Prerequisites section ‚ùå Timeout Cause: Default Lambda timeout (3 seconds) is too short Fix: Go to Configuration ‚Üí General and increase timeout to 10‚Äì20 seconds ‚ùå KeyError or missing question field Cause: Payload is missing the \u0026quot;question\u0026quot; key Fix: Ensure the test event JSON is formatted like: { \u0026#34;question\u0026#34;: \u0026#34;Your question here\u0026#34; } üéØ Expected Outcome After completing this step, you will:\nConfirm that Lambda successfully invokes Bedrock Receive responses from the model via the Converse API Verify that your IAM Role and MODEL_ID are correctly configured Be ready to move on to building the API endpoint "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/3-blogstranslated/",
	"title": "Translated AWS Blogs",
	"tags": [],
	"description": "",
	"content": "This section lists and briefly introduces the AWS blogs that the team has translated:\nBlog 1 ‚Äì Building Operational Resilience: Designing Effective Recovery Strategies Against Ransomware This blog focuses on how organizations can strengthen cyber resilience against ransomware using the Cloud Hosted Data Vault (CHDV) architecture. It explains key considerations when designing a vault ‚Äî including identifying critical data and business functions, planning realistic recovery workflows under cyberattack scenarios, segmenting the vault environment, and applying strict access and operational controls. The blog also emphasizes the role of people, processes, and leadership sponsorship in ensuring effective cyber recovery.\nBlog 2 ‚Äì Dynamic Routing Using Amazon VPC Route Server This blog introduces how Amazon VPC Route Server enables dynamic routing (BGP) inside a VPC, allowing more granular traffic control and automated failover. Through scenarios such as using floating IPs for high-availability applications or directing traffic through firewall appliances, the article demonstrates how the Route Server automatically updates VPC route tables, reduces manual configuration, and increases the resiliency of cloud network architectures on AWS.\nBlog 3 ‚Äì Launch of the AWS Asia Pacific (New Zealand) Region This blog announces the official launch of the AWS Asia Pacific (New Zealand) Region (ap-southeast-6). It highlights the benefits of the new region, including local data residency, lower latency for workloads serving New Zealand users, enhanced support for AI and generative AI solutions, and expanded cloud infrastructure across the country. The blog also underscores AWS‚Äôs commitments to sustainability, economic impact, workforce development, and the growth of the AWS partner and developer community in New Zealand.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.4-api-gateway-integration/",
	"title": "Create API Gateway",
	"tags": [],
	"description": "",
	"content": "Create API Gateway In this section, you will create an HTTP API on Amazon API Gateway so that clients can send questions to your Lambda function.\nAPI Gateway will act as the HTTP endpoint intermediary between the client and your AI Q\u0026amp;A service.\nContents 5.4.1 ‚Äì Create HTTP API 5.4.2 ‚Äì Integrate API with Lambda After completing this section, you will have a public (or private, depending on configuration) HTTP endpoint that allows clients such as Postman, cURL, or web frontends to send questions to Lambda and receive responses from Bedrock.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Viet Nam Cloud Day 2025\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nPreview: A flagship day diving into GenAI strategy, unified data platforms, enterprise‚Äëscale migration/modernization, and security‚Äëby‚Äëdesign. Highlights included AI across the SDLC, zero‚Äëtrust practices, AI agents for productivity, and practical stories from banks and startups.\nEvent 2 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nPreview: A practical workshop on AWS AI/ML (SageMaker) and GenAI on Bedrock. Walked through the ML lifecycle, MLOps automation, Prompt Engineering, RAG, Bedrock Agents, Guardrails, and live demos building a context‚Äëaware chatbot.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.5-testing-and-logs/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": "Test the API with Thunder Client (VS Code) After integrating API Gateway with Lambda, you have an HTTP endpoint ready for testing.\nIn this step, you‚Äôll use Thunder Client ‚Äî a popular VS Code extension ‚Äî to send requests and inspect responses.\nüîπ Step 1 ‚Äî Get the Invoke URL from API Gateway In AWS API Gateway:\nOpen the API Gateway service. Select the API you created, e.g., bedrock-chatbot-api. In the left menu, choose Deploy ‚Üí Stages. Click the $default stage. In Stage details, copy the Invoke URL. Copy the Invoke URL, for example:\nhttps://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com Next, append the route path you configured, e.g., /chat.\nüëâ The full endpoint will be:\nhttps://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com/chat üîπ Step 2 ‚Äî Install and open Thunder Client in VS Code Open VS Code. Go to the Extensions tab. Search for Thunder Client and click Install. After installation, the Thunder Client icon appears in the sidebar. Click the icon and choose New Request. Select the POST method. Paste the endpoint into the URL box: https://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com/chat üîπ Step 3 ‚Äî Send JSON body and check the response Choose the Body ‚Üí JSON tab. Enter: { \u0026#34;question\u0026#34;: \u0026#34;Amazon Bedrock l√† g√¨?\u0026#34; } Click Send to issue the request.\nIf everything is wired correctly, you should get a response like:\n{ \u0026#34;answer\u0026#34;: \u0026#34;Amazon Bedrock is a fully managed service...\u0026#34; } This confirms:\nAPI Gateway received the request successfully Lambda ran correctly and called Bedrock The system returned the expected result üîß Troubleshooting 403 / AccessDeniedException ‚Üí Check the Lambda IAM role 500 Internal Error ‚Üí Inspect CloudWatch Logs Missing \u0026lsquo;question\u0026rsquo; field ‚Üí Validate the JSON body Timeout ‚Üí Increase Lambda timeout to 10‚Äì20 seconds ‚úî Conclusion You‚Äôve successfully tested the end‚Äëto‚Äëend pipeline:\nClient ‚Üí API Gateway ‚Üí Lambda ‚Üí Bedrock ‚Üí AI Response\nYou‚Äôve completed the testing portion of the workshop.\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Build a Simple AI API with AWS Lambda + Bedrock + API Gateway Overview Amazon Bedrock is a fully managed service that provides access to leading large language models (LLMs) such as Claude, Llama, Mistral, and Titan.\nYou can integrate AI features into your applications using simple API calls without managing infrastructure or hosting models yourself.\nIn this workshop, you will build a simple AI Q\u0026amp;A API using:\nAWS Lambda ‚Äì handles incoming requests and calls Bedrock Amazon Bedrock Runtime ‚Äì sends prompts and receives model responses Amazon API Gateway ‚Äì exposes an HTTP endpoint for client requests A key aspect of this workshop is the use of the Converse API ‚Äî a unified interface for Bedrock models that support the Converse capability (e.g., Claude 3, Claude 3.5, Llama 3.1, Mistral 24.07‚Ä¶).\nWith the Converse API:\nYou can switch models by simply changing the modelId in Lambda There is no need to rewrite conversation-handling logic You can easily test or compare multiple models with the same API flow Note: Only models that support Converse API can be used with the code in this workshop.\nWorkshop Content Introduction Prerequisites Lambda Calls Bedrock Create API Gateway Testing Cleanup "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/5-workshop/5.6-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Cleanup Resources After completing the workshop, you should delete the AWS resources you no longer need to avoid unnecessary costs.\nBelow is a list of the services you created and how to remove them.\nüîπ 1. Delete the API Gateway Open the API Gateway Console Select the API you created, e.g., bedrock-chatbot-api Choose Actions ‚Üí Delete Confirm deletion This prevents any further requests from reaching Lambda and avoids API Gateway charges.\nüîπ 2. Delete the Lambda Function Open the Lambda Console Select the function bedrock-chatbot-lambda Choose Actions ‚Üí Delete function Confirm deletion üîπ 3. Delete the IAM Role and Policy Delete the Policy: Open IAM Console ‚Üí Policies Search for lambda-bedrock Click Delete Delete the Role: Open IAM Console ‚Üí Roles Search for lambda-bedrock-role Click Delete ‚ö†Ô∏è Note: You can only delete the role after deleting the Lambda function that uses it.\nüîπ 4. Check CloudWatch Log Groups (Optional) Lambda logs remain in CloudWatch and may accumulate storage charges over time.\nOpen the CloudWatch Console Select Logs ‚Üí Log groups Find your Lambda log group (e.g., /aws/lambda/bedrock-chatbot-lambda) Choose Actions ‚Üí Delete log group üîπ 5. Review Other Resources (If Applicable) Depending on how you expanded the workshop, you may have created additional resources such as:\nS3 buckets Step Functions KMS keys VPC / Security Groups If they are no longer needed, delete them to prevent charges.\nüéâ All Done! You have now cleaned up all resources created during this workshop.\nYour AWS account will no longer incur costs from the lab environment.\nThank you for participating in the workshop!\n"
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd. from August 12, 2025 to December 9, 2025, I had the opportunity to learn, practice, and apply the knowledge I had acquired at university in a real working environment.\nI participated in the FitAI Challenge project, through which I developed various skills such as self-directed research, requirement analysis, report writing, team communication, problem-solving, and applying new technologies.\nIn terms of work ethic, I consistently strived to complete assigned tasks, adhere to company regulations, and actively collaborate with colleagues to improve work efficiency.\nTo provide an objective reflection of my internship process, I have evaluated myself based on the criteria below:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, ability to apply knowledge, tool proficiency, work quality ‚úÖ ‚òê ‚òê 2 Learning ability Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Willingness to explore and take on tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adherence to schedules, regulations, and work processes ‚òê ‚úÖ ‚òê 6 Willingness to improve Openness to feedback and commitment to self-improvement ‚úÖ ‚òê ‚òê 7 Communication Ability to present ideas clearly and report work effectively ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and contributing to team activities ‚úÖ ‚òê ‚òê 9 Professional conduct Respect for colleagues, partners, and the working environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Ability to identify issues, propose solutions, and think creatively ‚òê ‚úÖ ‚òê 11 Contribution to the project/team Work efficiency, improvement ideas, and recognition from the team ‚úÖ ‚òê ‚òê 12 Overall performance General evaluation of the entire internship process ‚òê ‚úÖ ‚òê Needs Improvement Although I have followed company regulations and workflows well, there is still room to improve discipline**, especially in time management, task planning, and maintaining consistent progress. Continued development of analytical and problem-solving skills is needed, including exploring multiple solution approaches and identifying the root causes of issues to determine the most suitable resolution. Communication skills can be further improved by presenting problems more clearly, defining goals thoroughly from the beginning, and proactively discussing challenges to avoid delays. Strengthening the ability to organize and systematize knowledge would support more effective learning and faster adaptation to new technologies. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment was new, professional, and provided me with opportunities to observe real-world workflows. Being involved in an actual project helped me better understand how a team operates in a corporate setting. However, as a student without prior cloud background, the initial adaptation phase was quite challenging.\n2. Support from Mentor / Admin Team\nThe mentor was supportive and explained concepts clearly whenever I encountered difficulties. However, since I was completely new to cloud technologies, I would have benefited from additional hands-on guidance, especially in the early stages, to reduce confusion and provide clearer direction.\n3. Alignment Between Tasks and Academic Background\nMy major is Artificial Intelligence, so I had little exposure to cloud computing before joining the program. Some parts of the workload and technical content felt advanced compared to my academic foundation. Adjusting the curriculum to better match each student‚Äôs background would improve the overall learning experience.\n4. Learning Opportunities \u0026amp; Skill Development\nI gained new knowledge and had the opportunity to participate in a real project. However, for interns who are just starting, having more direct instructions or clearer initial orientation would make the learning process smoother and more efficient.\n5. Culture \u0026amp; Team Spirit\nThe team demonstrated strong collaboration and was supportive throughout the program. This positive working culture helped me integrate quickly, even though I was new to cloud technologies.\n6. Policies / Benefits for Interns\nThe program provided adequate learning resources and created favorable conditions for skill development. However, communication channels could be organized more clearly to ensure important updates are not missed.\nAdditional Questions ‚Ä¢ What I was most satisfied with:\nExperiencing a new working environment and being able to contribute to a real project from the beginning.\n‚Ä¢ What the company should improve for future interns:\nProvide more structured and accessible learning content, especially for students unfamiliar with cloud technologies.\n‚Ä¢ Would I recommend this internship to friends:\nYes ‚Äî if the opportunity aligns with their field of study and professional goals.\nSuggestions \u0026amp; Expectations Increase the number of hands-on guidance sessions to help new interns better understand their tasks and reduce confusion during the onboarding phase. Structure communication channels more clearly, such as: One group for general discussions and Q\u0026amp;A One dedicated group for important announcements from mentors Adapt the training content to better fit each academic major, allowing interns to learn and apply knowledge more effectively. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 10 Objectives: Amazon RDS ‚Äì Advanced Operations\nReview RDS architecture: Multi-AZ, Read Replica, backup lifecycle. Understand Parameter Groups and Option Groups. Optimize Lambda ‚Üí RDS connections. Analyze database performance with Performance Insights. Perform backup, restore, manual snapshot management. Amazon S3 ‚Äì Backend Storage Integration\nUnderstand buckets, objects, permissions. Configure secure Bucket Policies \u0026amp; IAM roles. Upload/download files via Lambda. Implement Lifecycle rules for cost optimization. Amazon Bedrock ‚Äì VPC Interface Endpoint\nCreate Interface Endpoint inside private subnets. Configure SG and routing for private access. Invoke Bedrock models (Claude, Titan‚Ä¶) from Lambda. Lambda Integration ‚Äì Full Backend Workflow\nLambda performs database queries on RDS. Lambda stores and reads data from S3. Lambda invokes Bedrock for AI processing. Monitor execution using CloudWatch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 RDS Advanced Operations ‚Ä¢ Review Parameter/Option Groups. ‚Ä¢ Analyze Performance Insights. ‚Ä¢ Create snapshots \u0026amp; restore. 10/11/2025 10/11/2025 cloudjourney.awsstudygroup.com 3 Lambda ‚Üí RDS Integration ‚Ä¢ Configure IAM Role. ‚Ä¢ Connect Lambda to RDS in private subnet. ‚Ä¢ Implement connection pooling. ‚Ä¢ Debug timeout issues. 11/11/2025 11/11/2025 cloudjourney.awsstudygroup.com 4 S3 Integration ‚Ä¢ Create bucket. ‚Ä¢ Configure policies. ‚Ä¢ Upload/download from Lambda. ‚Ä¢ Apply Lifecycle rules. 12/11/2025 12/11/2025 cloudjourney.awsstudygroup.com 5 Create Bedrock Interface Endpoint ‚Ä¢ Deploy endpoint in private subnet. ‚Ä¢ Configure security groups. ‚Ä¢ Test Bedrock inference from Lambda. 13/11/2025 13/11/2025 cloudjourney.awsstudygroup.com 6 AI-Driven Lambda API ‚Ä¢ Query RDS. ‚Ä¢ Store output in S3. ‚Ä¢ Invoke Bedrock model. ‚Ä¢ Monitor logs in CloudWatch. 14/11/2025 14/11/2025 cloudjourney.awsstudygroup.com 7 Cleanup \u0026amp; Cost Optimization ‚Ä¢ Delete snapshots and test DB. ‚Ä¢ Clean S3 bucket. ‚Ä¢ Review Endpoint/RDS/S3 costs. 15/11/2025 15/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 10: 1. Mastered advanced RDS operations Understood Parameter Groups and Option Groups. Applied performance troubleshooting via Performance Insights. Managed backup, restore, and snapshots efficiently. 2. Successfully integrated S3 with backend workflows Secure bucket configuration using IAM best practices. Lambda can upload, read, and manage data objects. Lifecycle rules deployed to reduce storage cost. 3. Built a secure Bedrock Endpoint for private AI inference Lambda can call Bedrock models inside VPC without Internet. Understood use cases for private model inference. 4. Developed AI-enhanced backend API Combined RDS + S3 + Bedrock in a single Lambda workflow. Processed structured + unstructured data. Built CloudWatch monitoring for debugging and optimization. Summary of Knowledge Gained: Strong knowledge of advanced RDS features and optimization. Hands-on skills integrating Lambda with RDS and S3. Experience deploying Bedrock Endpoint for private AI workloads. Ability to build multi-service backend workflows using AWS. Understanding of cost optimization across RDS, S3, Lambda, and VPC networking. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 11 Objectives: Part 1 ‚Äì Front-end Serverless Deployment (S3 + CloudFront)\nUnderstand static website hosting architecture on AWS. Create an S3 bucket to store the frontend build. Configure CloudFront as a global CDN. Enable HTTPS using ACM certificates. Implement Origin Access Control (OAC) for secure S3 access. Block all public access and apply secure bucket policies. Part 2 ‚Äì Integrating Cognito Authentication into the Frontend\nReuse User Pool \u0026amp; App Client configured in Week 8. Build login/signup UI or use Hosted UI. Retrieve ID/Access Tokens after authentication. Store tokens securely (localStorage/sessionStorage). Implement logout and token expiration handling. Part 3 ‚Äì Calling API Gateway from the Frontend\nSend Authorization header: Bearer \u0026lt;JWT\u0026gt;. Test protected API routes using Cognito Authorizer. Handle 401 (Unauthorized) and 403 (Forbidden). Render API responses on the UI. Part 4 ‚Äì Monitoring \u0026amp; Debugging\nEnable and review CloudFront Access Logs. Check API Gateway logs via CloudWatch. Debug Lambda flow end-to-end. Clean up unused resources to reduce costs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Deploy Frontend to S3 ‚Ä¢ Create bucket. ‚Ä¢ Upload build files. ‚Ä¢ Configure index/error pages. 17/11/2025 17/11/2025 cloudjourney.awsstudygroup.com 3 Create CloudFront Distribution ‚Ä¢ Add S3 origin. ‚Ä¢ Configure HTTPS with ACM. ‚Ä¢ Enable OAC + update bucket policy. 18/11/2025 18/11/2025 cloudjourney.awsstudygroup.com 4 Integrate Cognito into Frontend ‚Ä¢ Retrieve JWT tokens from Week 8 User Pool. ‚Ä¢ Store tokens securely. ‚Ä¢ Build login/logout UI. 19/11/2025 19/11/2025 cloudjourney.awsstudygroup.com 5 Frontend ‚Üí API Gateway Integration ‚Ä¢ Send Authorization header. ‚Ä¢ Test protected endpoints. ‚Ä¢ Handle 401/403. 20/11/2025 20/11/2025 cloudjourney.awsstudygroup.com 6 Monitoring \u0026amp; Debugging ‚Ä¢ Review CloudFront logs. ‚Ä¢ Debug API Gateway. ‚Ä¢ Check Lambda logs. 21/11/2025 21/11/2025 cloudjourney.awsstudygroup.com 7 Cleanup Resources ‚Ä¢ Delete test CloudFront. ‚Ä¢ Delete test S3 bucket. ‚Ä¢ Review Billing. 22/11/2025 22/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 11: 1. Fully deployed serverless frontend Frontend hosted on S3 + distributed globally via CloudFront. HTTPS enabled and S3 protected using OAC. Fast and secure static web delivery. 2. Cognito Authentication integrated successfully Reused Week 8 User Pool. JWT retrieval and storage working correctly. Login/logout features implemented with token handling. 3. Secure API Gateway calls from the frontend JWT-based Authorization header validated by API Gateway. Smooth flow from Frontend ‚Üí API ‚Üí Lambda backend. Clear error handling for authentication failures. 4. Effective debugging and monitoring CloudFront logs used to trace user requests. API Gateway + Lambda logs analyzed through CloudWatch. Reduced cost by cleaning unused test resources. Summary of Knowledge Gained: Hosting serverless frontend via S3 + CloudFront. Practical Cognito Authentication integration without Amplify. Making secure API calls with JWT tokens. Debugging across CloudFront, API Gateway, and Lambda. Understanding full serverless stack behavior on AWS. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 12 Objectives: Part 1 ‚Äì Building AI API using Amazon Bedrock\nUnderstand Bedrock architecture, pricing, throttling. Invoke Claude/Titan models from Lambda via Interface Endpoint. Apply prompt engineering for better model performance. Create API Gateway endpoint for external AI calls. Handle errors such as timeouts and throttling. Part 2 ‚Äì Building automated workflows using Step Functions\nDesign multi-step workflows:\nLambda ‚Üí Bedrock ‚Üí Data Storage ‚Üí SES Email. Use Task, Choice, Wait, Retry, and Catch states. Connect Lambda tasks to create an AI pipeline. Log execution history and debug errors. Part 3 ‚Äì Email notifications using Amazon SES\nVerify email identities. Build Lambda function to send emails via SES. Integrate SES into Step Functions workflow. Handle bounce and complaint notifications. Part 4 ‚Äì Workflow Monitoring\nMonitor Bedrock and Lambda logs with CloudWatch. Review Step Functions execution history. Track workflow metric success/failure. Optimize Bedrock cost (max tokens, batching, etc.). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Bedrock model invocation ‚Ä¢ Call Bedrock from Lambda. ‚Ä¢ Improve prompt. ‚Ä¢ Debug endpoint issues. 24/11/2025 24/11/2025 cloudjourney.awsstudygroup.com 3 AI API Development ‚Ä¢ Create /ai/generate endpoint. ‚Ä¢ Return model response. ‚Ä¢ Validate input. 25/11/2025 25/11/2025 cloudjourney.awsstudygroup.com 4 Step Functions Workflow ‚Ä¢ Build State Machine. ‚Ä¢ Lambda ‚Üí Bedrock ‚Üí Store Output. ‚Ä¢ Add error handling. 26/11/2025 26/11/2025 cloudjourney.awsstudygroup.com 5 SES Integration ‚Ä¢ Verify sender identity. ‚Ä¢ Lambda send email. ‚Ä¢ Add SES to workflow. 27/11/2025 27/11/2025 cloudjourney.awsstudygroup.com 6 Monitoring \u0026amp; Optimization ‚Ä¢ Review logs. ‚Ä¢ Analyze Step Functions map. ‚Ä¢ Optimize cost. 28/11/2025 28/11/2025 cloudjourney.awsstudygroup.com 7 Cleanup \u0026amp; Final Review ‚Ä¢ Remove test resources. ‚Ä¢ Review billing. ‚Ä¢ Validate full workflow. 29/11/2025 29/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 12: 1. Successfully built a Bedrock-powered AI API Lambda in private subnet calls Bedrock models. Stable API responses via API Gateway. Effective prompt engineering implemented. 2. Completed AI workflow using Step Functions Multi-step automation pipeline works end-to-end. Error handling \u0026amp; retries implemented properly. Clean execution flow: Input ‚Üí AI ‚Üí Save ‚Üí Notify. 3. SES email notifications integrated AI summary sent automatically after workflow. Email templates working. Improved automation experience. 4. Full monitoring and debugging implemented CloudWatch logs for Lambda, Bedrock, API Gateway. Detailed Step Functions Execution Map. Cost and performance optimization. Summary of Knowledge Gained: Practical usage of Amazon Bedrock for AI inference. Building serverless AI APIs using Lambda + API Gateway. Workflow orchestration using Step Functions. Email automation with Amazon SES. Complete understanding of production-level AI agent architecture. "
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://phanminhtriet283-bit.github.io/Report-AWS/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]